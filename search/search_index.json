{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Tefla: Deep Learning library, a Higher level API for TensorFlow Tefla is built on top of Tensorflow for fast prototyping of deep learning algorithms. It provides high level access to the features of tensorflow. An interface to easily build complex models. Tefla features: . Supports custom optimizers . Supports data-sets, data-augmentation, and others . Supports text datasets . Easy to define complex deep models . Single and multi GPU training . Various prediction functions including ensembling of models . Different metrics for performance measurement . Custom losses . Learning rate schedules, polynomial, step, validation_loss based . Semantic segmentation learning . Semi-supervised learning Installation Prerequisite to install Tefla Before you install Tefla you need to install Tensorflow version r1.8.0 or later. pip install tensorflow-gpu or pip install tensorflow Install Tefla The latest release of Tefla is version 1.9.0. - To install the latest stable version: pip install tefla To install the current version: pip install git+https://github.com/openagi/tefla.git To develop or work with source and modifying source code: git clone https://github.com/openagi/tefla.git cd tefla pip install -r requirements.txt export PYTHONPATH=. Example MNIST example that gives a overview about how to use Tefla def model(is_training, reuse): common_args = common_layer_args(is_training, reuse) conv_args = make_args(batch_norm=True, activation=prelu, **common_args) fc_args = make_args(activation=prelu, **common_args) logit_args = make_args(activation=None, **common_args) x = input((None, height, width, 1), **common_args) x = conv2d(x, 32, name='conv1_1', **conv_args) x = conv2d(x, 32, name='conv1_2', **conv_args) x = max_pool(x, name='pool1', **common_args) x = dropout(x, drop_p=0.25, name='dropout1', **common_args) x = fully_connected(x, n_output=128, name='fc1', **fc_args) x = dropout(x, drop_p=0.5, name='dropout2', **common_args) logits = fully_connected(x, n_output=10, name=\"logits\", **logit_args) predictions = softmax(logits, name='predictions', **common_args) return end_points(is_training) training_cnf = { 'classification': True, 'validation_scores': [('validation accuracy', util.accuracy_wrapper), ('validation kappa', util.kappa_wrapper)], 'num_epochs': 50, 'lr_policy': StepDecayPolicy( schedule={ 0: 0.01, 30: 0.001, } ) } util.init_logging('train.log', file_log_level=logging.INFO, console_log_level=logging.INFO) trainer = SupervisedTrainer(model, training_cnf, classification=training_cnf['classification']) trainer.fit(data_set, weights_from=None, start_epoch=1, verbose=1, summary_every=10)","title":"Index"},{"location":"#tefla-deep-learning-library-a-higher-level-api-for-tensorflow","text":"Tefla is built on top of Tensorflow for fast prototyping of deep learning algorithms. It provides high level access to the features of tensorflow. An interface to easily build complex models. Tefla features: . Supports custom optimizers . Supports data-sets, data-augmentation, and others . Supports text datasets . Easy to define complex deep models . Single and multi GPU training . Various prediction functions including ensembling of models . Different metrics for performance measurement . Custom losses . Learning rate schedules, polynomial, step, validation_loss based . Semantic segmentation learning . Semi-supervised learning","title":"Tefla: Deep Learning library, a Higher level API for TensorFlow"},{"location":"#installation","text":"","title":"Installation"},{"location":"#prerequisite-to-install-tefla","text":"Before you install Tefla you need to install Tensorflow version r1.8.0 or later. pip install tensorflow-gpu or pip install tensorflow","title":"Prerequisite to install Tefla"},{"location":"#install-tefla","text":"The latest release of Tefla is version 1.9.0. - To install the latest stable version: pip install tefla To install the current version: pip install git+https://github.com/openagi/tefla.git To develop or work with source and modifying source code: git clone https://github.com/openagi/tefla.git cd tefla pip install -r requirements.txt export PYTHONPATH=.","title":"Install Tefla"},{"location":"#example","text":"","title":"Example"},{"location":"#mnist-example-that-gives-a-overview-about-how-to-use-tefla","text":"def model(is_training, reuse): common_args = common_layer_args(is_training, reuse) conv_args = make_args(batch_norm=True, activation=prelu, **common_args) fc_args = make_args(activation=prelu, **common_args) logit_args = make_args(activation=None, **common_args) x = input((None, height, width, 1), **common_args) x = conv2d(x, 32, name='conv1_1', **conv_args) x = conv2d(x, 32, name='conv1_2', **conv_args) x = max_pool(x, name='pool1', **common_args) x = dropout(x, drop_p=0.25, name='dropout1', **common_args) x = fully_connected(x, n_output=128, name='fc1', **fc_args) x = dropout(x, drop_p=0.5, name='dropout2', **common_args) logits = fully_connected(x, n_output=10, name=\"logits\", **logit_args) predictions = softmax(logits, name='predictions', **common_args) return end_points(is_training) training_cnf = { 'classification': True, 'validation_scores': [('validation accuracy', util.accuracy_wrapper), ('validation kappa', util.kappa_wrapper)], 'num_epochs': 50, 'lr_policy': StepDecayPolicy( schedule={ 0: 0.01, 30: 0.001, } ) } util.init_logging('train.log', file_log_level=logging.INFO, console_log_level=logging.INFO) trainer = SupervisedTrainer(model, training_cnf, classification=training_cnf['classification']) trainer.fit(data_set, weights_from=None, start_epoch=1, verbose=1, summary_every=10)","title":"MNIST example that gives a overview about how to use Tefla"},{"location":"installation/","text":"Installation Tensorflow Installation Tefla requires Tensorflow (version >= 0.12.0) to be installed. Select the correct binary to install, according to your system: # Ubuntu/Linux/macOS 64-bit, CPU only, Python 3.6 pip install tensorflow # Ubuntu/Linux/macOS 64-bit, GPU enabled, Python 3.6 # Requires CUDA toolkit 8.0/9.0 and CuDNN v5/v7. For other versions, see \"Installing from sources\" below. pip install tensorflow-gpu - For more details: [Tensorflow installation instructions](https://www.tensorflow.org/install). ## Tefla Installation To install Tefla, the easiest way is to run For the bleeding edge version: ```python pip install git+https://github.com/n3011/tefla.git Otherwise, you can also install from source by running (from source folder): pip install tefla Upgrade Tensorflow If you version for Tensorflow is too old (under 0.12.0), you may upgrade Tensorflow to avoid some incompatibilities with Tefla. To upgrade Tensorflow, you first need to uninstall Tensorflow and Protobuf: pip uninstall protobuf pip uninstall tensorflow Then you can re-install Tensorflow: Using Latest Tensorflow Tefla is compatible with master version of Tensorflow, but some warnings may appear.","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#tensorflow-installation","text":"Tefla requires Tensorflow (version >= 0.12.0) to be installed. Select the correct binary to install, according to your system: # Ubuntu/Linux/macOS 64-bit, CPU only, Python 3.6 pip install tensorflow # Ubuntu/Linux/macOS 64-bit, GPU enabled, Python 3.6 # Requires CUDA toolkit 8.0/9.0 and CuDNN v5/v7. For other versions, see \"Installing from sources\" below. pip install tensorflow-gpu - For more details: [Tensorflow installation instructions](https://www.tensorflow.org/install). ## Tefla Installation To install Tefla, the easiest way is to run For the bleeding edge version: ```python pip install git+https://github.com/n3011/tefla.git Otherwise, you can also install from source by running (from source folder): pip install tefla","title":"Tensorflow Installation"},{"location":"installation/#upgrade-tensorflow","text":"If you version for Tensorflow is too old (under 0.12.0), you may upgrade Tensorflow to avoid some incompatibilities with Tefla. To upgrade Tensorflow, you first need to uninstall Tensorflow and Protobuf: pip uninstall protobuf pip uninstall tensorflow Then you can re-install Tensorflow:","title":"Upgrade Tensorflow"},{"location":"installation/#using-latest-tensorflow","text":"Tefla is compatible with master version of Tensorflow, but some warnings may appear.","title":"Using Latest Tensorflow"},{"location":"license/","text":"MIT License Copyright (c) 2016 Tefla contributors Tefla uses a shared copyright model: each contributor holds copyright over their contributions to Tefla. The project versioning records all such contribution and copyright details. By contributing to the Tefla repository through a pull-request, comment, or otherwise, a contributor releases their content to the license and copyright terms herein. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"core/initializers/","text":"He Normal initializer tefla.core.initializers.he_normal (seed=None, scale=1.0, dtype=tf.float32) Kaiming He et al. (2015): Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. arXiv preprint arXiv:1502.01852. Args scale : float Scaling factor for the weights. Set this to 1.0 for linear and sigmoid units, to sqrt(2) for rectified linear units, and to sqrt(2/(1+alpha**2)) for leaky rectified linear units with leakiness alpha . Other transfer functions may need different factors. He Uniform initializer tefla.core.initializers.he_uniform (seed=None, scale=1.0, dtype=tf.float32) Args scale : float Scaling factor for the weights. Set this to 1.0 for linear and sigmoid units, to sqrt(2) for rectified linear units, and to sqrt(2/(1+alpha**2)) for leaky rectified linear units with leakiness alpha . Other transfer functions may need different factors. Random Normal initializer tefla.core.initializers.random_normal (seed=None, mean=0.0, stddev=1.0, dtype=tf.float32, name=None) Args mean : a float stddev : a float Returns an initializer that generates tensors without scaling variance tefla.core.initializers.variance_scaling_initializer_v2 (factor=2.0, mode='FAN_IN', uniform=False, seed=None, dtype=tf.float32, mean=0.0, stddev=1.0, normal_type=None, name=None) When initializing a deep network, it is in principle advantageous to keep the scale of the input variance constant, so it does not explode or diminish by reaching the final layer. This initializer use the following formula: if mode='FAN_IN': # Count only number of input connections. n = fan_in elif mode='FAN_OUT': # Count only number of output connections. n = fan_out elif mode='FAN_AVG': # Average number of inputs and output connections. n = (fan_in + fan_out)/2.0 truncated_normal(shape, 0.0, stddev=sqrt(factor / n)) To get Delving Deep into Rectifiers , use (Default): factor=2.0 mode='FAN_IN' uniform=False To get Convolutional Architecture for Fast Feature Embedding , use: factor=1.0 mode='FAN_IN' uniform=True To get Understanding the difficulty of training deep feedforward neural networks , use: factor=1.0 mode='FAN_AVG' uniform=True. To get xavier_initializer use either: factor=1.0 mode='FAN_AVG' uniform=True , or factor=1.0 mode='FAN_AVG' uniform=False . Args factor: Float. A multiplicative factor. mode: String. 'FAN_IN', 'FAN_OUT', 'FAN_AVG'. uniform: Whether to use uniform or normal distributed random initialization. seed: A Python integer. Used to create random seeds. See - set_random_seed - for behavior. dtype: The data type. Only floating point types are supported. Returns An initializer that generates tensors with unit variance. Raises ValueError: if dtype is not a floating point type. TypeError: if mode is not in ['FAN_IN', 'FAN_OUT', 'FAN_AVG']. Returns An initializer that generates tensors with unit variance. Raises ValueError: if dtype is not a floating point type. TypeError: if mode is not in ['FAN_IN', 'FAN_OUT', 'FAN_AVG']. Bilinear initialization for up sampling operation tefla.core.initializers.bilinear (f_shape) Args f_shape : shape of the variable Returns bilinear initializer Variable initializer that produces a random orthonormal matrix tefla.core.initializers.random_orthonormal_initializer (shape, dtype=tf.float32, partition_info=None) Args shape : shape of the variable Returns random_orthogonal_matrix for initialization.","title":"Initializations"},{"location":"core/initializers/#he-normal-initializer","text":"tefla.core.initializers.he_normal (seed=None, scale=1.0, dtype=tf.float32) Kaiming He et al. (2015): Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. arXiv preprint arXiv:1502.01852.","title":"He Normal initializer"},{"location":"core/initializers/#he-uniform-initializer","text":"tefla.core.initializers.he_uniform (seed=None, scale=1.0, dtype=tf.float32)","title":"He Uniform initializer"},{"location":"core/initializers/#random-normal-initializer","text":"tefla.core.initializers.random_normal (seed=None, mean=0.0, stddev=1.0, dtype=tf.float32, name=None)","title":"Random Normal initializer"},{"location":"core/initializers/#returns-an-initializer-that-generates-tensors-without-scaling-variance","text":"tefla.core.initializers.variance_scaling_initializer_v2 (factor=2.0, mode='FAN_IN', uniform=False, seed=None, dtype=tf.float32, mean=0.0, stddev=1.0, normal_type=None, name=None) When initializing a deep network, it is in principle advantageous to keep the scale of the input variance constant, so it does not explode or diminish by reaching the final layer. This initializer use the following formula: if mode='FAN_IN': # Count only number of input connections. n = fan_in elif mode='FAN_OUT': # Count only number of output connections. n = fan_out elif mode='FAN_AVG': # Average number of inputs and output connections. n = (fan_in + fan_out)/2.0 truncated_normal(shape, 0.0, stddev=sqrt(factor / n)) To get Delving Deep into Rectifiers , use (Default): factor=2.0 mode='FAN_IN' uniform=False To get Convolutional Architecture for Fast Feature Embedding , use: factor=1.0 mode='FAN_IN' uniform=True To get Understanding the difficulty of training deep feedforward neural networks , use: factor=1.0 mode='FAN_AVG' uniform=True. To get xavier_initializer use either: factor=1.0 mode='FAN_AVG' uniform=True , or factor=1.0 mode='FAN_AVG' uniform=False .","title":"Returns an initializer that generates tensors without scaling variance"},{"location":"core/initializers/#bilinear-initialization-for-up-sampling-operation","text":"tefla.core.initializers.bilinear (f_shape)","title":"Bilinear initialization for up sampling operation"},{"location":"core/initializers/#variable-initializer-that-produces-a-random-orthonormal-matrix","text":"tefla.core.initializers.random_orthonormal_initializer (shape, dtype=tf.float32, partition_info=None)","title":"Variable initializer that produces a random orthonormal matrix"},{"location":"core/iter_ops/","text":"Creates training iterator to access and augment the dataset tefla.core.iter_ops.create_training_iters (cnf, data_set, standardizer, crop_size, epoch, parallel=True) Args cnf : configs dict with all training and augmentation params data_set : an instance of the dataset class standardizer : data samples standardization; either samplewise or aggregate crop_size : training time crop_size of the data samples epoch : the current epoch number; used for data balancing parallel : iterator type; either parallel or queued Creates prediction iterator to access and augment the dataset tefla.core.iter_ops.create_prediction_iter (cnf, standardizer, crop_size, preprocessor=None, sync=False) Args cnf : configs dict with all training and augmentation params standardizer : data samples standardization; either samplewise or aggregate crop_size : training time crop_size of the data samples preprocessor : data processing or cropping function sync : a bool, if False, used parallel iterator","title":"Iter Ops"},{"location":"core/iter_ops/#creates-training-iterator-to-access-and-augment-the-dataset","text":"tefla.core.iter_ops.create_training_iters (cnf, data_set, standardizer, crop_size, epoch, parallel=True)","title":"Creates training iterator to access and augment the dataset"},{"location":"core/iter_ops/#creates-prediction-iterator-to-access-and-augment-the-dataset","text":"tefla.core.iter_ops.create_prediction_iter (cnf, standardizer, crop_size, preprocessor=None, sync=False)","title":"Creates prediction iterator to access and augment the dataset"},{"location":"core/layer_arg_ops/","text":"Creates all common parameters tefla.core.layer_arg_ops.common_layer_args (is_training, reuse, **kwargs) Args is_training : a bool, training or prediction resue : resue variables or initializes **kwargs: other common arguments Returns end_points for training or validation tefla.core.layer_arg_ops.end_points (is_training) Args is_training : a bool, training or validation","title":"Layer Args"},{"location":"core/layer_arg_ops/#creates-all-common-parameters","text":"tefla.core.layer_arg_ops.common_layer_args (is_training, reuse, **kwargs)","title":"Creates all common parameters"},{"location":"core/layer_arg_ops/#returns-end_points-for-training-or-validation","text":"tefla.core.layer_arg_ops.end_points (is_training)","title":"Returns end_points for training or validation"},{"location":"core/layers/","text":"Define input layer tefla.core.layers.input (shape, name='inputs', outputs_collections=None, **unused) Args shape : A Tensor , define the input shape e.g. for image input [batch_size, height, width, depth] name : A optional score/name for this op outputs_collections : The collections to which the outputs are added. Returns A placeholder for the input Add item to colelction tefla.core.layers.register_to_collections (inputs, name=None, outputs_collections=None, **unused) Args shape : A Tensor , define the input shape e.g. for image input [batch_size, height, width, depth] name : A optional score/name for this op outputs_collections : The collections to which the outputs are added. Returns A placeholder for the input Adds a fully connected layer tefla.core.layers.fully_connected (x, n_output, is_training, reuse, trainable=True, w_init= , b_init=0.0, w_regularizer= , w_normalized=False, name='fc', batch_norm=None, batch_norm_args=None, activation=None, params=None, outputs_collections=None, use_bias=True) fully_connected creates a variable called weights , representing a fully connected weight matrix, which is multiplied by the x to produce a Tensor of hidden units. If a batch_norm is provided (such as batch_norm ), it is then applied. Otherwise, if batch_norm is None and a b_init and use_bias is provided then a biases variable would be created and added the hidden units. Finally, if activation is not None , it is applied to the hidden units as well. Note: that if x have a rank greater than 2, then x is flattened prior to the initial matrix multiply by weights . Args x : A Tensor of with at least rank 2 and value for the last dimension, i.e. [batch_size, depth] , [None, None, None, channels] . is_training : Bool, training or testing n_output : Integer or long, the number of output units in the layer. reuse : whether or not the layer and its variables should be reused. To be able to reuse the layer scope must be given. activation : activation function, set to None to skip it and maintain a linear activation. batch_norm : normalization function to use. If - batch_norm is True then google original implementation is used and if another function is provided then it is applied. default set to None for no normalizer function batch_norm_args : normalization function parameters. w_init : An initializer for the weights. w_regularizer : Optional regularizer for the weights. b_init : An initializer for the biases. If None skip biases. outputs_collections : The collections to which the outputs are added. trainable : If True also add variables to the graph collection GraphKeys.TRAINABLE_VARIABLES (see tf.Variable). name : Optional name or scope for variable_scope/name_scope. use_bias : Whether to add bias or not Returns The 2-D Tensor variable representing the result of the series of operations. e.g: 2-D Tensor [batch, n_output]. Adds a 2D convolutional layer tefla.core.layers.conv2d (x, n_output_channels, is_training, reuse, trainable=True, filter_size= (3, 3), stride= (1, 1), padding='SAME', w_init= , b_init=0.0, w_regularizer= , untie_biases=False, name='conv2d', batch_norm=None, batch_norm_args=None, activation=None, use_bias=True, outputs_collections=None) convolutional layer creates a variable called weights , representing a conv weight matrix, which is multiplied by the x to produce a Tensor of hidden units. If a batch_norm is provided (such as batch_norm ), it is then applied. Otherwise, if batch_norm is None and a b_init and use_bias is provided then a biases variable would be created and added the hidden units. Finally, if activation is not None , it is applied to the hidden units as well. Note: that if x have a rank 4 Args x : A 4-D Tensor of with at least rank 2 and value for the last dimension, i.e. [batch_size, in_height, in_width, depth] , is_training : Bool, training or testing n_output : Integer or long, the number of output units in the layer. reuse : whether or not the layer and its variables should be reused. To be able to reuse the layer scope must be given. filter_size : a int or list/tuple of 2 positive integers specifying the spatial dimensions of of the filters. stride : a int or tuple/list of 2 positive integers specifying the stride at which to compute output. padding : one of \"VALID\" or \"SAME\" . activation : activation function, set to None to skip it and maintain a linear activation. batch_norm : normalization function to use. If batch_norm is True then google original implementation is used and if another function is provided then it is applied. default set to None for no normalizer function batch_norm_args : normalization function parameters. w_init : An initializer for the weights. w_regularizer : Optional regularizer for the weights. untie_biases : spatial dimensions wise baises b_init : An initializer for the biases. If None skip biases. outputs_collections : The collections to which the outputs are added. trainable : If True also add variables to the graph collection GraphKeys.TRAINABLE_VARIABLES (see tf.Variable). name : Optional name or scope for variable_scope/name_scope. use_bias : Whether to add bias or not Returns The 4-D Tensor variable representing the result of the series of operations. e.g.: 4-D Tensor [batch, new_height, new_width, n_output]. Adds a 2D dilated convolutional layer tefla.core.layers.dilated_conv2d (x, n_output_channels, is_training, reuse, trainable=True, filter_size= (3, 3), dilation=1, stride=1, padding='SAME', w_init= , b_init=0.0, w_regularizer= , untie_biases=False, name='dilated_conv2d', batch_norm=None, batch_norm_args=None, activation=None, use_bias=True, outputs_collections=None) also known as convolution with holes or atrous convolution. If the rate parameter is equal to one, it performs regular 2-D convolution. If the rate parameter is greater than one, it performs convolution with holes, sampling the input values every rate pixels in the height and width dimensions. convolutional layer creates a variable called weights , representing a conv weight matrix, which is multiplied by the x to produce a Tensor of hidden units. If a batch_norm is provided (such as batch_norm ), it is then applied. Otherwise, if batch_norm is None and a b_init and use_bias is provided then a biases variable would be created and added the hidden units. Finally, if activation is not None , it is applied to the hidden units as well. Note: that if x have a rank 4 Args x : A 4-D Tensor of with rank 4 and value for the last dimension, i.e. [batch_size, in_height, in_width, depth] , is_training : Bool, training or testing n_output : Integer or long, the number of output units in the layer. reuse : whether or not the layer and its variables should be reused. To be able to reuse the layer scope must be given. filter_size : a int or list/tuple of 2 positive integers specifying the spatial dimensions of of the filters. dilation : A positive int32. The stride with which we sample input values across the height and width dimensions. Equivalently, the rate by which we upsample the filter values by inserting zeros across the height and width dimensions. In the literature, the same parameter is sometimes called input stride/rate or dilation. padding : one of \"VALID\" or \"SAME\" . activation : activation function, set to None to skip it and maintain a linear activation. batch_norm : normalization function to use. If batch_norm is True then google original implementation is used and if another function is provided then it is applied. default set to None for no normalizer function batch_norm_args : normalization function parameters. w_init : An initializer for the weights. w_regularizer : Optional regularizer for the weights. untie_biases : spatial dimensions wise baises b_init : An initializer for the biases. If None skip biases. outputs_collections : The collections to which the outputs are added. trainable : If True also add variables to the graph collection GraphKeys.TRAINABLE_VARIABLES (see tf.Variable). name : Optional name or scope for variable_scope/name_scope. use_bias : Whether to add bias or not Returns The 4-D Tensor variable representing the result of the series of operations. e.g.: 4-D Tensor [batch, new_height, new_width, n_output]. Adds a 2D seperable convolutional layer tefla.core.layers.separable_conv2d (x, n_output_channels, is_training, reuse, trainable=True, filter_size= (3, 3), stride= (1, 1), depth_multiplier=1, padding='SAME', w_init= , b_init=0.0, w_regularizer= , untie_biases=False, name='separable_conv2d', batch_norm=None, batch_norm_args=None, activation=None, use_bias=True, outputs_collections=None) Performs a depthwise convolution that acts separately on channels followed by a pointwise convolution that mixes channels. Note that this is separability between dimensions [1, 2] and 3, not spatial separability between dimensions 1 and 2. convolutional layer creates two variable called depthwise_W and pointwise_W , depthwise_W is multiplied by x to produce depthwise conolution, which is multiplied by the pointwise_W to produce a output Tensor If a batch_norm is provided (such as batch_norm ), it is then applied. Otherwise, if batch_norm is None and a b_init and use_bias is provided then a biases variable would be created and added the hidden units. Finally, if activation is not None , it is applied to the hidden units as well. Note: that if x have a rank 4 Args x : A 4-D Tensor of with rank 4 and value for the last dimension, i.e. [batch_size, in_height, in_width, depth] , is_training : Bool, training or testing n_output : Integer or long, the number of output units in the layer. reuse : whether or not the layer and its variables should be reused. To be able to reuse the layer scope must be given. filter_size : a int or list/tuple of 2 positive integers specifying the spatial stride : a int or tuple/list of 2 positive integers specifying the stride at which to compute output. dimensions of of the filters. depth_multiplier : A positive int32. the number of depthwise convolution output channels for each input channel. The total number of depthwise convolution output channels will be equal to `num_filters_in * depth_multiplier padding : one of \"VALID\" or \"SAME\" . activation : activation function, set to None to skip it and maintain a linear activation. batch_norm : normalization function to use. If batch_norm is True then google original implementation is used and if another function is provided then it is applied. default set to None for no normalizer function batch_norm_args : normalization function parameters. w_init : An initializer for the weights. w_regularizer : Optional regularizer for the weights. untie_biases : spatial dimensions wise baises b_init : An initializer for the biases. If None skip biases. outputs_collections : The collections to which the outputs are added. trainable : If True also add variables to the graph collection GraphKeys.TRAINABLE_VARIABLES (see tf.Variable). name : Optional name or scope for variable_scope/name_scope. use_bias : Whether to add bias or not Returns The 4-D Tensor variable representing the result of the series of operations. e.g.: 4-D Tensor [batch, new_height, new_width, n_output]. Adds a 2D sdepthwise convolutional layer tefla.core.layers.depthwise_conv2d (x, depth_multiplier, is_training, reuse, trainable=True, filter_size= (3, 3), stride= (1, 1), padding='SAME', w_init= , b_init=0.0, w_regularizer= , untie_biases=False, name='depthwise_conv2d', batch_norm=None, batch_norm_args=None, activation=None, use_bias=True, outputs_collections=None) Given an input tensor of shape [batch, in_height, in_width, in_channels] and a filter tensor of shape [filter_height, filter_width, in_channels, channel_multiplier] containing in_channels convolutional filters of depth 1, depthwise_conv2d applies a different filter to each input channel (expanding from 1 channel to channel_multiplier channels for each), then concatenates the results together. The output has in_channels * channel_multiplier channels. If a batch_norm is provided (such as batch_norm ), it is then applied. Otherwise, if batch_norm is None and a b_init and use_bias is provided then a biases variable would be created and added the hidden units. Finally, if activation is not None , it is applied to the hidden units as well. Note: that if x have a rank 4 Args x : A 4-D Tensor of with rank 4 and value for the last dimension, i.e. [batch_size, in_height, in_width, depth] , is_training : Bool, training or testing reuse : whether or not the layer and its variables should be reused. To be able to reuse the layer scope must be given. filter_size : a int or list/tuple of 2 positive integers specifying the spatial dimensions of of the filters. stride : a int or tuple/list of 2 positive integers specifying the stride at which to compute output. depth_multiplier : A positive int32. the number of depthwise convolution output channels for each input channel. The total number of depthwise convolution output channels will be equal to `num_filters_in * depth_multiplier padding : one of \"VALID\" or \"SAME\" . activation : activation function, set to None to skip it and maintain a linear activation. batch_norm : normalization function to use. If batch_norm is True then google original implementation is used and if another function is provided then it is applied. default set to None for no normalizer function batch_norm_args : normalization function parameters. w_init : An initializer for the weights. w_regularizer : Optional regularizer for the weights. untie_biases : spatial dimensions wise baises b_init : An initializer for the biases. If None skip biases. outputs_collections : The collections to which the outputs are added. trainable : If True also add variables to the graph collection GraphKeys.TRAINABLE_VARIABLES (see tf.Variable). name : Optional name or scope for variable_scope/name_scope. use_bias : Whether to add bias or not Returns The tensor variable representing the result of the series of operations. e.g.: 4-D Tensor [batch, new_height, new_width, n_output]. Adds a 3D convolutional layer tefla.core.layers.conv3d (x, n_output_channels, is_training, reuse, trainable=True, filter_size= (3, 3, 3), stride= (1, 1, 1), padding='SAME', w_init= , b_init=0.0, w_regularizer= , untie_biases=False, name='conv3d', batch_norm=None, batch_norm_args=None, activation=None, use_bias=True, outputs_collections=None) convolutional layer creates a variable called weights , representing a conv weight matrix, which is multiplied by the x to produce a Tensor of hidden units. If a batch_norm is provided (such as batch_norm ), it is then applied. Otherwise, if batch_norm is None and a b_init and use_bias is provided then a biases variable would be created and added the hidden units. Finally, if activation is not None , it is applied to the hidden units as well. Note: that if x have a rank 5 Args x : A 5-D Tensor of with at least rank 2 and value for the last dimension, i.e. [batch_size, in_depth, in_height, in_width, depth] , is_training : Bool, training or testing n_output : Integer or long, the number of output units in the layer. reuse : whether or not the layer and its variables should be reused. To be able to reuse the layer scope must be given. filter_size : a int, or list/tuple of 3 positive integers specifying the spatial dimensions of of the filters. stride : a int, or tuple/list of 3 positive integers specifying the stride at which to compute output. padding : one of \"VALID\" or \"SAME\" . activation : activation function, set to None to skip it and maintain a linear activation. batch_norm : normalization function to use. If batch_norm is True then google original implementation is used and if another function is provided then it is applied. default set to None for no normalizer function batch_norm_args : normalization function parameters. w_init : An initializer for the weights. w_regularizer : Optional regularizer for the weights. untie_biases : spatial dimensions wise baises b_init : An initializer for the biases. If None skip biases. outputs_collections : The collections to which the outputs are added. trainable : If True also add variables to the graph collection GraphKeys.TRAINABLE_VARIABLES (see tf.Variable). name : Optional name or scope for variable_scope/name_scope. use_bias : Whether to add bias or not Returns The 5-D Tensor variable representing the result of the series of operations. e.g.: 5-D Tensor [batch, new_depth, new_height, new_width, n_output]. Adds a 2D upsampling or deconvolutional layer tefla.core.layers.upsample2d (input_, output_shape, is_training, reuse, trainable=True, filter_size= (5, 5), stride= (2, 2), w_init= , b_init=0.0, w_regularizer= , batch_norm=None, batch_norm_args=None, activation=None, name='deconv2d', use_bias=True, with_w=False, outputs_collections=None, **unused) his operation is sometimes called \"deconvolution\" after Deconvolutional Networks, but is actually the transpose (gradient) of conv2d rather than an actual deconvolution. If a batch_norm is provided (such as batch_norm ), it is then applied. Otherwise, if batch_norm is None and a b_init and use_bias is provided then a biases variable would be created and added the hidden units. Finally, if activation is not None , it is applied to the hidden units as well. Note: that if x have a rank 4 Args x : A 4-D Tensor of with at least rank 2 and value for the last dimension, i.e. [batch_size, in_height, in_width, depth] , is_training : Bool, training or testing output_shape : 4D tensor, the output shape reuse : whether or not the layer and its variables should be reused. To be able to reuse the layer scope must be given. filter_size : a int or list/tuple of 2 positive integers specifying the spatial dimensions of of the filters. stride : a int or tuple/list of 2 positive integers specifying the stride at which to compute output. padding : one of \"VALID\" or \"SAME\" . activation : activation function, set to None to skip it and maintain a linear activation. batch_norm : normalization function to use. If batch_norm is True then google original implementation is used and if another function is provided then it is applied. default set to None for no normalizer function batch_norm_args : normalization function parameters. w_init : An initializer for the weights. w_regularizer : Optional regularizer for the weights. b_init : An initializer for the biases. If None skip biases. outputs_collections : The collections to which the outputs are added. trainable : If True also add variables to the graph collection GraphKeys.TRAINABLE_VARIABLES (see tf.Variable). name : Optional name or scope for variable_scope/name_scope. use_bias : Whether to add bias or not Returns The tensor variable representing the result of the series of operations. e.g.: 4-D Tensor [batch, new_height, new_width, n_output]. Adds a 3D upsampling or deconvolutional layer tefla.core.layers.upsample3d (input_, output_shape, is_training, reuse, trainable=True, filter_size= (5, 5, 5), stride= (2, 2, 2), w_init= , b_init=0.0, w_regularizer= , batch_norm=None, batch_norm_args=None, activation=None, name='deconv3d', use_bias=True, with_w=False, outputs_collections=None, **unused) his operation is sometimes called \"deconvolution\" after Deconvolutional Networks, but is actually the transpose (gradient) of conv2d rather than an actual deconvolution. If a batch_norm is provided (such as batch_norm ), it is then applied. Otherwise, if batch_norm is None and a b_init and use_bias is provided then a biases variable would be created and added the hidden units. Finally, if activation is not None , it is applied to the hidden units as well. Note: that if x have a rank 5 Args x : A 5-D Tensor of with at least rank 2 and value for the last dimension, i.e. [batch_size, in_depth, in_height, in_width, depth] , is_training : Bool, training or testing output_shape : 5D tensor, the output shape reuse : whether or not the layer and its variables should be reused. To be able to reuse the layer scope must be given. filter_size : a int or list/tuple of 3 positive integers specifying the spatial dimensions of of the filters. stride : a int or tuple/list of 3 positive integers specifying the stride at which to compute output. padding : one of \"VALID\" or \"SAME\" . activation : activation function, set to None to skip it and maintain a linear activation. batch_norm : normalization function to use. If batch_norm is True then google original implementation is used and if another function is provided then it is applied. default set to None for no normalizer function batch_norm_args : normalization function parameters. w_init : An initializer for the weights. w_regularizer : Optional regularizer for the weights. b_init : An initializer for the biases. If None skip biases. outputs_collections : The collections to which the outputs are added. trainable : If True also add variables to the graph collection GraphKeys.TRAINABLE_VARIABLES (see tf.Variable). name : Optional name or scope for variable_scope/name_scope. use_bias : Whether to add bias or not Returns The tensor variable representing the result of the series of operations. e.g.: 5-D Tensor [batch, new_depth, new_height, new_width, n_output]. Adds a 1D convolutional layer tefla.core.layers.conv1d (x, n_output_channels, is_training, reuse, trainable=True, filter_size=3, stride=1, padding='SAME', w_init= , b_init=0.0, w_regularizer= , untie_biases=False, name='conv1d', batch_norm=None, batch_norm_args=None, activation=None, use_bias=True, outputs_collections=None) convolutional layer creates a variable called weights , representing a conv weight matrix, which is multiplied by the x to produce a Tensor of hidden units. If a batch_norm is provided (such as batch_norm ), it is then applied. Otherwise, if batch_norm is None and a b_init and use_bias is provided then a biases variable would be created and added the hidden units. Finally, if activation is not None , it is applied to the hidden units as well. Note: that if x have a rank 4 Args x : A 3-D Tensor of with at least rank 2 and value for the last dimension, i.e. [batch_size, in_width, depth] , is_training : Bool, training or testing n_output : Integer or long, the number of output units in the layer. reuse : whether or not the layer and its variables should be reused. To be able to reuse the layer scope must be given. filter_size : a `int specifying the spatial dimensions of of the filters. stride : a int specifying the stride at which to compute output. padding : one of \"VALID\" or \"SAME\" . activation : activation function, set to None to skip it and maintain a linear activation. batch_norm : normalization function to use. If batch_norm is True then google original implementation is used and if another function is provided then it is applied. default set to None for no normalizer function batch_norm_args : normalization function parameters. w_init : An initializer for the weights. w_regularizer : Optional regularizer for the weights. untie_biases : spatial dimensions wise baises b_init : An initializer for the biases. If None skip biases. outputs_collections : The collections to which the outputs are added. trainable : If True also add variables to the graph collection GraphKeys.TRAINABLE_VARIABLES (see tf.Variable). name : Optional name or scope for variable_scope/name_scope. use_bias : Whether to add bias or not Returns The 3-D Tensor variable representing the result of the series of operations. e.g.: 3-D Tensor [batch, new_width, n_output]. Max Pooling 1D tefla.core.layers.max_pool_1d (x, filter_size=3, stride=2, padding='SAME', name='maxpool1d', outputs_collections=None, **unused) Args x : a 3-D Tensor [batch_size, steps, in_channels]. kernel_size : int or list of int . Pooling kernel size. strides : int or list of int . Strides of conv operation. Default: same as kernel_size. padding : str from \"same\", \"valid\" . Padding algo to use. Default: 'same'. name : A name for this layer (optional). Default: 'maxpool1d'. Returns 3-D Tensor [batch, pooled steps, in_channels]. Avg Pooling 1D tefla.core.layers.avg_pool_1d (x, filter_size=3, stride=2, padding='SAME', name='avgpool1d', outputs_collections=None, **unused) Args x : a 3-D Tensor [batch_size, steps, in_channels]. kernel_size : int or list of int . Pooling kernel size. strides : int or list of int . Strides of conv operation. Default: same as kernel_size. padding : str from \"same\", \"valid\" . Padding algo to use. Default: 'same'. name : A name for this layer (optional). Default: 'avgpool1d'. Returns 3-D Tensor [batch, pooled steps, in_channels]. Adds a 2D highway convolutional layer tefla.core.layers.highway_conv2d (x, n_output, is_training, reuse, trainable=True, filter_size= (3, 3), stride= (1, 1), padding='SAME', w_init= , b_init=0.0, w_regularizer= , name='highway_conv2d', batch_norm=None, batch_norm_args=None, activation= , use_bias=True, outputs_collections=None) https://arxiv.org/abs/1505.00387 If a batch_norm is provided (such as batch_norm ), it is then applied. Otherwise, if batch_norm is None and a b_init and use_bias is provided then a biases variable would be created and added the hidden units. Finally, if activation is not None , it is applied to the hidden units as well. Note: that if x have a rank 4 Args x : A 4-D Tensor of with at least rank 2 and value for the last dimension, i.e. [batch_size, in_height, in_width, depth] , is_training : Bool, training or testing n_output : Integer or long, the number of output units in the layer. reuse : whether or not the layer and its variables should be reused. To be able to reuse the layer scope must be given. filter_size : a int or list/ tuple of 2 positive integers specifying the spatial dimensions of of the filters. stride : a int or tuple/list of 2 positive integers specifying the stride at which to compute output. padding : one of \"VALID\" or \"SAME\" . activation : activation function, set to None to skip it and maintain a linear activation. batch_norm : normalization function to use. If batch_norm is True then google original implementation is used and if another function is provided then it is applied. default set to None for no normalizer function batch_norm_args : normalization function parameters. w_init : An initializer for the weights. w_regularizer : Optional regularizer for the weights. untie_biases : spatial dimensions wise baises b_init : An initializer for the biases. If None skip biases. outputs_collections : The collections to which the outputs are added. trainable : If True also add variables to the graph collection GraphKeys.TRAINABLE_VARIABLES (see tf.Variable). name : Optional name or scope for variable_scope/name_scope. use_bias : Whether to add bias or not Returns The Tensor variable representing the result of the series of operations. e.g.: 4-D Tensor [batch, new_height, new_width, n_output]. Adds a fully connected highway layer tefla.core.layers.highway_fc2d (x, n_output, is_training, reuse, trainable=True, filter_size= (3, 3), w_init= , b_init=0.0, w_regularizer= , name='highway_fc2d', activation=None, use_bias=True, outputs_collections=None) https://arxiv.org/abs/1505.00387 If a batch_norm is provided (such as batch_norm ), it is then applied. Otherwise, if batch_norm is None and a b_init and use_bias is provided then a biases variable would be created and added the hidden units. Finally, if activation is not None , it is applied to the hidden units as well. Note: that if x have a rank greater than 2, then x is flattened prior to the initial matrix multiply by weights . Args x : A 2-D/4-D Tensor of with at least rank 2 and value for the last dimension, i.e. [batch_size, depth] , [None, None, None, channels] . is_training : Bool, training or testing n_output : Integer or long, the number of output units in the layer. reuse : whether or not the layer and its variables should be reused. To be able to reuse the layer scope must be given. activation : activation function, set to None to skip it and maintain a linear activation. batch_norm : normalization function to use. If batch_norm is True then google original implementation is used and if another function is provided then it is applied. default set to None for no normalizer function batch_norm_args : normalization function parameters. w_init : An initializer for the weights. w_regularizer : Optional regularizer for the weights. b_init : An initializer for the biases. If None skip biases. outputs_collections : The collections to which the outputs are added. trainable : If True also add variables to the graph collection GraphKeys.TRAINABLE_VARIABLES (see tf.Variable). name : Optional name or scope for variable_scope/name_scope. use_bias : Whether to add bias or not Returns The 2-D Tensor variable representing the result of the series of operations. e.g.: 2-D Tensor [batch_size, n_output] Max pooling layer tefla.core.layers.max_pool (x, filter_size= (3, 3), stride= (2, 2), padding='SAME', name='pool', outputs_collections=None, **unused) Args x : A 4-D 'Tensor of shape [batch_size, height, width, channels]` filter_size : A int or list/tuple of length 2: [kernel_height, kernel_width] of the pooling kernel over which the op is computed. Can be an int if both values are the same. stride : A int or list/tuple of length 2: [stride_height, stride_width]. padding : The padding method, either 'VALID' or 'SAME'. outputs_collections : The collections to which the outputs are added. name : Optional scope/name for name_scope. Returns A Tensor representing the results of the pooling operation. e.g.: 4-D Tensor [batch, new_height, new_width, channels]. Max pooling layer tefla.core.layers.max_pool_3d (x, filter_size= (3, 3, 3), stride= (2, 2, 2), padding='SAME', name='pool', outputs_collections=None, **unused) Args x : A 5-D 'Tensor of shape [batch_size, depth, height, width, channels]` filter_size : A int or list/tuple of length 3: [kernel_depth, kernel_height, kernel_width] of the pooling kernel over which the op is computed. Can be an int if both values are the same. stride : A int or list/tuple of length 3: [stride_depth, stride_height, stride_width]. padding : The padding method, either 'VALID' or 'SAME'. outputs_collections : The collections to which the outputs are added. name : Optional scope/name for name_scope. Returns A Tensor representing the results of the pooling operation. e.g.: 5-D Tensor [batch, new_depth, new_height, new_width, channels]. Fractional pooling layer tefla.core.layers.fractional_pool (x, pooling_ratio=[1.0, 1.44, 1.73, 1.0], pseudo_random=None, determinastic=None, overlapping=None, name='fractional_pool', seed=None, seed2=None, type='avg', outputs_collections=None, **unused) Args x : A 4-D Tensor of shape [batch_size, height, width, channels] pooling_ratio : A list of floats that has length >= 4. Pooling ratio for each dimension of value, currently only supports row and col dimension and should be >= 1.0. For example, a valid pooling ratio looks like [1.0, 1.44, 1.73, 1.0]. The first and last elements must be 1.0 because we don't allow pooling on batch and channels dimensions. 1.44 and 1.73 are pooling ratio on height and width dimensions respectively. pseudo_random : An optional bool. Defaults to False. When set to True, generates the pooling sequence in a pseudorandom fashion, otherwise, in a random fashion. Check paper Benjamin Graham, Fractional Max-Pooling for difference between pseudorandom and random. overlapping : An optional bool. Defaults to False. When set to True, it means when pooling, the values at the boundary of adjacent pooling cells are used by both cells. For example: index 0 1 2 3 4 value 20 5 16 3 7; If the pooling sequence is [0, 2, 4], then 16, at index 2 will be used twice. The result would be [41/3, 26/3] for fractional avg pooling. deterministic : An optional bool. Defaults to False. When set to True, a fixed pooling region will be used when iterating over a FractionalAvgPool node in the computation graph. Mainly used in unit test to make FractionalAvgPool deterministic. seed : An optional int. Defaults to 0. If either seed or seed2 are set to be non-zero, the random number generator is seeded by the given seed. Otherwise, it is seeded by a random seed. seed2 : An optional int. Defaults to 0. An second seed to avoid seed collision. outputs_collections : The collections to which the outputs are added. type : avg or max pool name : Optional scope/name for name_scope. Returns A 4-D Tensor representing the results of the pooling operation. e.g.: 4-D Tensor [batch, new_height, new_width, channels]. RMS pooling layer tefla.core.layers.rms_pool_2d (x, filter_size= (3, 3), stride= (2, 2), padding='SAME', name='pool', epsilon=1e-12, outputs_collections=None, **unused) Args x : A 4-D Tensor of shape [batch_size, height, width, channels] filter_size : A int or list/tuple of length 2: [kernel_height, kernel_width] of the pooling kernel over which the op is computed. Can be an int if both values are the same. stride : A int or list/tuple of length 2: [stride_height, stride_width]. padding : The padding method, either 'VALID' or 'SAME'. outputs_collections : The collections to which the outputs are added. name : Optional scope/name for name_scope. epsilon : prevents divide by zero Returns A 4-D Tensor representing the results of the pooling operation. e.g.: 4-D Tensor [batch, new_height, new_width, channels]. RMS pooling layer tefla.core.layers.rms_pool_3d (x, filter_size= (3, 3, 3), stride= (2, 2, 2), padding='SAME', name='pool', epsilon=1e-12, outputs_collections=None, **unused) Args x : A 5-D Tensor of shape [batch_size, depth, height, width, channels] filter_size : A int or list/tuple of length 3: [kernel_depth, kernel_height, kernel_width] of the pooling kernel over which the op is computed. Can be an int if both values are the same. stride : A int or list/tuple of length 3: [stride_depth, stride_height, stride_width]. padding : The padding method, either 'VALID' or 'SAME'. outputs_collections : The collections to which the outputs are added. name : Optional scope/name for name_scope. epsilon : prevents divide by zero Returns A 5-D Tensor representing the results of the pooling operation. e.g.: 5-D Tensor [batch, new_height, new_width, channels]. Avg pooling layer tefla.core.layers.avg_pool_3d (x, filter_size= (3, 3, 3), stride= (2, 2, 2), padding='SAME', name=None, outputs_collections=None, **unused) Args x : A 4-D Tensor of shape [batch_size, depth, height, width, channels] filter_size : A int or list/tuple of length 3: [kernel_depth, kernel_height, kernel_width] of the pooling kernel over which the op is computed. Can be an int if both values are the same. stride : A int or list/tuple of length 3: [stride_depth, stride_height, stride_width]. padding : The padding method, either 'VALID' or 'SAME'. outputs_collections : The collections to which the outputs are added. name : Optional scope/name for name_scope. Returns A 5-D Tensor representing the results of the pooling operation. e.g.: 5-D Tensor [batch, new_depth, new_height, new_width, channels]. Avg pooling layer tefla.core.layers.avg_pool_2d (x, filter_size= (3, 3), stride= (2, 2), padding='SAME', name=None, outputs_collections=None, **unused) Args x : A 4-D Tensor of shape [batch_size, height, width, channels] filter_size : A int or list/tuple of length 2: [kernel_height, kernel_width] of the pooling kernel over which the op is computed. Can be an int if both values are the same. stride : A int or list/tuple of length 2: [stride_height, stride_width]. padding : The padding method, either 'VALID' or 'SAME'. outputs_collections : The collections to which the outputs are added. name : Optional scope/name for name_scope. Returns A 4-D Tensor representing the results of the pooling operation. e.g.: 4-D Tensor [batch, new_height, new_width, channels]. Gloabl pooling layer tefla.core.layers.global_avg_pool (x, name='global_avg_pool', outputs_collections=None, **unused) Args x : A 4-D Tensor of shape [batch_size, height, width, channels] outputs_collections : The collections to which the outputs are added. name : Optional scope/name for name_scope. Returns A 4-D Tensor representing the results of the pooling operation. e.g.: 4-D Tensor [batch, 1, 1, channels]. Gloabl max pooling layer tefla.core.layers.global_max_pool (x, name='global_max_pool', outputs_collections=None, **unused) Args x : A 4-D Tensor of shape [batch_size, height, width, channels] outputs_collections : The collections to which the outputs are added. name : Optional scope/name for name_scope. Returns A 4-D Tensor representing the results of the pooling operation. e.g.: 4-D Tensor [batch, 1, 1, channels]. Feature max pooling layer tefla.core.layers.feature_max_pool_1d (x, stride=2, name='feature_max_pool_1d', outputs_collections=None, **unused) Args x : A 2-D tensor of shape [batch_size, channels] stride : A int. outputs_collections : The collections to which the outputs are added. name : Optional scope/name for name_scope. Returns A 2-D Tensor representing the results of the pooling operation. e.g.: 2-D Tensor [batch_size, new_channels] Feature max pooling layer tefla.core.layers.feature_max_pool_2d (x, stride=2, name='feature_max_pool_2d', outputs_collections=None, **unused) Args x : A 4-D tensor of shape [batch_size, height, width, channels] stride : A int. outputs_collections : The collections to which the outputs are added. name : Optional scope/name for name_scope. Returns A 4-D Tensor representing the results of the pooling operation. e.g.: 4-D Tensor [batch_size, height, width, new_channels] Adds a Batch Normalization layer from http://arxiv.org/abs/1502.03167 tefla.core.layers.batch_norm_tf (x, name='bn', scale=False, updates_collections=None, **kwargs) \"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\", Sergey Ioffe, Christian Szegedy Can be used as a normalizer function for conv2d and fully_connected. Note: When is_training is True the moving_mean and moving_variance need to be updated, by default the update_ops are placed in tf.GraphKeys.UPDATE_OPS so they need to be added as a dependency to the train_op , example: update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) if update_ops: updates = tf.group(*update_ops) total_loss = control_flow_ops.with_dependencies([updates], total_loss) One can set updates_collections=None to force the updates in place, but that can have speed penalty, specially in distributed settings. Args x : a Tensor with 2 or more dimensions, where the first dimension has batch_size . The normalization is over all but the last dimension if data_format is NHWC and the second dimension if data_format is NCHW . decay : decay for the moving average. Reasonable values for decay are close to 1.0, typically in the multiple-nines range: 0.999, 0.99, 0.9, etc. Lower decay value (recommend trying decay =0.9) if model experiences reasonably good training performance but poor validation and/or test performance. Try zero_debias_moving_mean=True for improved stability. center : If True, subtract beta . If False, beta is ignored. scale : If True, multiply by gamma . If False, gamma is not used. When the next layer is linear (also e.g. nn.relu ), this can be disabled since the scaling can be done by the next layer. epsilon : small float added to variance to avoid dividing by zero. activation_fn : activation function, default set to None to skip it and maintain a linear activation. param_initializers : optional initializers for beta, gamma, moving mean and moving variance. updates_collections : collections to collect the update ops for computation. The updates_ops need to be executed with the train_op. If None, a control dependency would be added to make sure the updates are computed in place. is_training : whether or not the layer is in training mode. In training mode it would accumulate the statistics of the moments into moving_mean and moving_variance using an exponential moving average with the given decay . When it is not in training mode then it would use the values of the moving_mean and the moving_variance . reuse : whether or not the layer and its variables should be reused. To be able to reuse the layer scope must be given. outputs_collections: collections to add the outputs. trainable : If True also add variables to the graph collection GraphKeys.TRAINABLE_VARIABLES (see tf.Variable ). batch_weights : An optional tensor of shape [batch_size] , containing a frequency weight for each batch item. If present, then the batch normalization uses weighted mean and variance. (This can be used to correct for bias in training example selection.) fused : Use nn.fused_batch_norm if True, nn.batch_normalization otherwise. name : Optional scope/name for variable_scope . Returns A Tensor representing the output of the operation. Adds a Batch Normalization layer from http://arxiv.org/abs/1502.03167 tefla.core.layers.batch_norm_lasagne (x, is_training, reuse, trainable=True, decay=0.9, epsilon=0.0001, name='bn', updates_collections='update_ops', outputs_collections=None) Instead of storing and updating moving variance, this layer store and update moving inverse standard deviation \"Batch Normalization: Accelerating Deep Network Training by Reducin Internal Covariate Shift\" Sergey Ioffe, Christian Szegedy Can be used as a normalizer function for conv2d and fully_connected. Note: When is_training is True the moving_mean and moving_variance need to be updated, by default the update_ops are placed in tf.GraphKeys.UPDATE_OPS so they need to be added as a dependency to the train_op , example: update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) if update_ops: updates = tf.group(*update_ops) total_loss = control_flow_ops.with_dependencies([updates], total_loss) One can set updates_collections=None to force the updates in place, but that can have speed penalty, specially in distributed settings. Args x : a tensor with 2 or more dimensions, where the first dimension has batch_size . The normalization is over all but the last dimension if data_format is NHWC and the second dimension if data_format is NCHW . decay : decay for the moving average. Reasonable values for decay are close to 1.0, typically in the multiple-nines range: 0.999, 0.99, 0.9, etc. Lower decay value (recommend trying decay =0.9) if model experiences reasonably good training performance but poor validation and/or test performance. Try zero_debias_moving_mean=True for improved stability. epsilon : small float added to variance to avoid dividing by zero. updates_collections : collections to collect the update ops for computation. The updates_ops need to be executed with the train_op. If None, a control dependency would be added to make sure the updates are computed in place. is_training : whether or not the layer is in training mode. In training mode it would accumulate the statistics of the moments into moving_mean and moving_variance using an exponential moving average with the given decay . When it is not in training mode then it would use the values of the moving_mean and the moving_variance . reuse : whether or not the layer and its variables should be reused. To be able to reuse the layer scope must be given. outputs_collections : collections to add the outputs. trainable : If True also add variables to the graph collection GraphKeys.TRAINABLE_VARIABLES (see tf.Variable ). name : Optional scope/name for variable_scope . Returns A Tensor representing the output of the operation. Layer normalize the tensor x, averaging over the last dimension tefla.core.layers.layer_norm (x, reuse, filters=None, trainable=True, epsilon=1e-06, name='layer_norm', allow_defun=False, outputs_collections=None) Args x : a Tensor with type float , double , int32 , int64 , uint8 , int16 , or int8`. reuse : whether or not the layer and its variables should be reused. To be able to reuse the layer scope must be given. trainable : a bool, training or fixed value name : a optional scope/name of the layer outputs_collections : The collections to which the outputs are added. Returns A Tensor representing the results of the layer norm operation. Prametric rectifier linear layer tefla.core.layers.prelu (x, reuse, alpha_init=0.2, trainable=True, name='prelu', outputs_collections=None) Args x : a Tensor with type float , double , int32 , int64 , uint8 , int16 , or int8`. reuse : whether or not the layer and its variables should be reused. To be able to reuse the layer scope must be given. alpha_init : initalization value for alpha trainable : a bool, training or fixed value name : a optional scope/name of the layer outputs_collections : The collections to which the outputs are added. Returns A Tensor representing the results of the prelu activation operation. Computes relu tefla.core.layers.relu (x, name='relu', outputs_collections=None, **unused) Args x : a Tensor with type float , double , int32 , int64 , uint8 , int16 , or int8`. name : a optional scope/name of the layer outputs_collections : The collections to which the outputs are added. Returns A Tensor representing the results of the activation operation. Rectifier linear relu6 layer tefla.core.layers.relu6 (x, name='relu6', outputs_collections=None, **unused) Args x : a Tensor with type float , double , int32 , int64 , uint8 , int16 , or int8`. name : a optional scope/name of the layer outputs_collections : The collections to which the outputs are added. Returns A Tensor representing the results of the relu6 activation operation. Softplus layer tefla.core.layers.softplus (x, name='softplus', outputs_collections=None, **unused) Computes softplus: log(exp(x) + 1). Args x : a Tensor with type float , double , int32 , int64 , uint8 , int16 , or int8`. name : a optional scope/name of the layer outputs_collections : The collections to which the outputs are added. Returns A Tensor representing the results of the activation operation. Softsign layer tefla.core.layers.softsign (x, name='softsign', outputs_collections=None, **unused) Computes softsign: x / (abs(x) + 1). Args x : a Tensor with type float , double , int32 , int64 , uint8 , int16 , or int8`. name : a optional scope/name of the layer outputs_collections : The collections to which the outputs are added. Returns A Tensor representing the results of the activation operation. Computes Concatenated ReLU tefla.core.layers.crelu (x, name='crelu', outputs_collections=None, **unused) Concatenates a ReLU which selects only the positive part of the activation with a ReLU which selects only the negative part of the activation. Note that at as a result this non-linearity doubles the depth of the activations. Source: https://arxiv.org/abs/1603.05201 Args x : a Tensor with type float , double , int32 , int64 , uint8 , int16 , or int8`. name : a optional scope/name of the layer outputs_collections : The collections to which the outputs are added. Returns A Tensor representing the results of the activation operation. Computes exponential linear: exp(features) - 1 if < 0, features otherwise tefla.core.layers.elu (x, name='elu', outputs_collections=None, **unused) See \"Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)\" Args x : a Tensor with type float , double , int32 , int64 , uint8 , int16 , or int8`. name : a optional scope/name of the layer outputs_collections : The collections to which the outputs are added. Returns A Tensor representing the results of the activation operation. like concatenated ReLU (http://arxiv.org/abs/1603.05201), but then with ELU tefla.core.layers.concat_elu (x, name='concat_elu', outputs_collections=None, **unused) Args x : a Tensor with type float , double , int32 , int64 , uint8 , int16 , or int8`. name : a optional scope/name of the layer outputs_collections : The collections to which the outputs are added. Returns A Tensor representing the results of the activation operation. Computes leaky relu tefla.core.layers.leaky_relu (x, alpha=0.01, name='leaky_relu', outputs_collections=None, **unused) Args x : a Tensor with type float , double , int32 , int64 , uint8 , int16 , or int8`. aplha : the conatant fro scalling the activation name : a optional scope/name of the layer outputs_collections : The collections to which the outputs are added. Returns A Tensor representing the results of the activation operation. Computes reaky relu lasagne style tefla.core.layers.lrelu (x, leak=0.2, name='lrelu', outputs_collections=None, **unused) Args x : a Tensor with type float , double , int32 , int64 , uint8 , int16 , or int8`. leak : the conatant fro scalling the activation name : a optional scope/name of the layer outputs_collections : The collections to which the outputs are added. Returns A Tensor representing the results of the activation operation. Computes maxout activation tefla.core.layers.maxout (x, k=2, name='maxout', outputs_collections=None, **unused) Args x : a Tensor with type float , double , int32 , int64 , uint8 , int16 , or int8`. k : output channel splitting factor name : a optional scope/name of the layer outputs_collections : The collections to which the outputs are added. Returns A Tensor representing the results of the activation operation. Computes maxout activation tefla.core.layers.offset_maxout (x, k=2, name='maxout', outputs_collections=None, **unused) Args x : a Tensor with type float , double , int32 , int64 , uint8 , int16 , or int8`. k : output channel splitting factor name : a optional scope/name of the layer outputs_collections : The collections to which the outputs are added. Returns A Tensor representing the results of the activation operation. Computes softmax activation tefla.core.layers.softmax (x, name='softmax', outputs_collections=None, **unused) Args x : a Tensor with type float , double , int32 , int64 , uint8 , int16 , or int8`. name : a optional scope/name of the layer outputs_collections : The collections to which the outputs are added. Returns A Tensor representing the results of the activation operation. Computes the spatial softmax of a convolutional feature map tefla.core.layers.spatial_softmax (features, reuse, temperature=None, name='spatial_softmax', trainable=True, outputs_collections=None, **unused) First computes the softmax over the spatial extent of each channel of a convolutional feature map. Then computes the expected 2D position of the points of maximal activation for each channel, resulting in a set of feature keypoints [x1, y1, ... xN, yN] for all N channels. Read more here: \"Learning visual feature spaces for robotic manipulation with deep spatial autoencoders.\" Finn et. al, http://arxiv.org/abs/1509.06113. Args features : A Tensor of size [batch_size, W, H, num_channels]; the convolutional feature map. reuse : whether or not the layer and its variables should be reused. To be able to reuse the layer scope must be given. outputs_collections : The collections to which the outputs are added. temperature : Softmax temperature (optional). If None, a learnable temperature is created. name : A name for this operation (optional). trainable : If True also add variables to the graph collection GraphKeys.TRAINABLE_VARIABLES (see tf.Variable ). Returns feature_keypoints: A Tensor with size [batch_size, num_channels * 2]; the expected 2D locations of each channel's feature keypoint (normalized to the range (-1,1)). The inner dimension is arranged as [x1, y1, ... xN, yN]. Computes selu tefla.core.layers.selu (x, alpha=None, scale=None, name='selu', outputs_collections=None, **unused) Args x : a Tensor with type float , double , int32 , int64 , uint8 , int16 , or int8`. alpha : float, selu parameters calculated from fixed points scale : float, selu parameters calculated from fixed points name : a optional scope/name of the layer outputs_collections : The collections to which the outputs are added. Returns A Tensor representing the results of the selu activation operation. Dropout layer for self normalizing networks tefla.core.layers.dropout_selu (x, is_training, drop_p=0.2, alpha=-1.7580993408473766, fixedPointMean=0.0, fixedPointVar=1.0, noise_shape=None, seed=None, name='dropout_selu', outputs_collections=None, **unused) Args x : a Tensor . is_training : a bool, training or validation drop_p : probability of droping unit fixedPointsMean : float, the mean used to calculate the selu parameters fixedPointsVar : float, the Variance used to calculate the selu parameters alpha : float, product of the two selu parameters name : a optional scope/name of the layer outputs_collections : The collections to which the outputs are added. Returns A Tensor representing the results of the dropout operation. Computes Gumbel Softmax tefla.core.layers.gumbel_softmax (logits, temperature, hard=False) Sample from the Gumbel-Softmax distribution and optionally discretize. http://blog.evjang.com/2016/11/tutorial-categorical-variational.html https://arxiv.org/abs/1611.01144 Args logits : [batch_size, n_class] unnormalized log-probs temperature : non-negative scalar hard : if True, take argmax, but differentiate w.r.t. soft sample y Returns [batch_size, n_class] sample from the Gumbel-Softmax distribution. If hard=True, then the returned sample will be one-hot, otherwise it will be a probabilitiy distribution that sums to 1 across classes Computes pixel wise softmax activation tefla.core.layers.pixel_wise_softmax (inputs) Args x : a Tensor with type float , double , int32 , int64 , uint8 , int16 , or int8`. name : a optional scope/name of the layer outputs_collections : The collections to which the outputs are added. Returns A Tensor representing the results of the activation operation. Dropout layer tefla.core.layers.dropout (x, is_training, drop_p=0.5, seed=None, name='dropout', outputs_collections=None, **unused) Args x : a Tensor . is_training : a bool, training or validation drop_p : probability of droping unit name : a optional scope/name of the layer outputs_collections : The collections to which the outputs are added. Returns A Tensor representing the results of the dropout operation. Repeat op tefla.core.layers.repeat (x, repetitions, layer, num_outputs=None, name='Repeat', outputs_collections=None, args, *kwargs) Args x : a Tensor . repetitions : a int, number of times to apply the same operation layer : the layer function with arguments to repeat name : a optional scope/name of the layer outputs_collections : The collections to which the outputs are added. Returns A Tensor representing the results of the repetition operation. Merge op tefla.core.layers.merge (tensors_list, mode, axis=1, name='merge', outputs_collections=None, **kwargs) Args tensor_list : A list Tensors to merge mode : str, available modes are ['concat', 'elemwise_sum', 'elemwise_mul', 'sum','mean', 'prod', 'max', 'min', 'and', 'or'] name : a optional scope/name of the layer outputs_collections : The collections to which the outputs are added. Returns A Tensor representing the results of the repetition operation. Builds a stack of layers by applying layer repeatedly using stack_args tefla.core.layers.stack (inputs, layer, stack_args, is_training, reuse, outputs_collections=None, **kwargs) stack allows you to repeatedly apply the same operation with different arguments stack_args[i] . For each application of the layer, stack creates a new scope appended with an increasing number. For example: y = stack(x, fully_connected, [32, 64, 128], scope='fc') # It is equivalent to: x = fully_connected(x, 32, scope='fc/fc_1') x = fully_connected(x, 64, scope='fc/fc_2') y = fully_connected(x, 128, scope='fc/fc_3') If the scope argument is not given in kwargs , it is set to layer.__name__ , or layer.func.__name__ (for functools.partial objects). If neither __name__ nor func.__name__ is available, the layers are called with scope='stack' . Args inputs : A Tensor suitable for layer. layer : A layer with arguments (inputs, *args, **kwargs) stack_args : A list/tuple of parameters for each call of layer. outputs_collections : The collections to which the outputs are added. **kwargs: Extra kwargs for the layer. Returns a Tensor result of applying the stacked layers. Normalizes the given input across the specified dimension to unit length tefla.core.layers.unit_norm (inputs, dim, epsilon=1e-07, scope=None) Note that the rank of input must be known. Args inputs : A Tensor of arbitrary size. dim : The dimension along which the input is normalized. epsilon : A small value to add to the inputs to avoid dividing by zero. scope : Optional scope for variable_scope. Returns The normalized Tensor . Concates two features maps tefla.core.layers.crop_and_concat (inputs1, inputs2, name='crop_concat') concates different sizes feature maps cropping the larger map concatenation across output channels Args inputs1 : A Tensor inputs2 : A Tensor Returns concated output tensor","title":"Layers"},{"location":"core/layers/#define-input-layer","text":"tefla.core.layers.input (shape, name='inputs', outputs_collections=None, **unused)","title":"Define input layer"},{"location":"core/layers/#add-item-to-colelction","text":"tefla.core.layers.register_to_collections (inputs, name=None, outputs_collections=None, **unused)","title":"Add item to colelction"},{"location":"core/layers/#adds-a-fully-connected-layer","text":"tefla.core.layers.fully_connected (x, n_output, is_training, reuse, trainable=True, w_init= , b_init=0.0, w_regularizer= , w_normalized=False, name='fc', batch_norm=None, batch_norm_args=None, activation=None, params=None, outputs_collections=None, use_bias=True) fully_connected creates a variable called weights , representing a fully connected weight matrix, which is multiplied by the x to produce a Tensor of hidden units. If a batch_norm is provided (such as batch_norm ), it is then applied. Otherwise, if batch_norm is None and a b_init and use_bias is provided then a biases variable would be created and added the hidden units. Finally, if activation is not None , it is applied to the hidden units as well. Note: that if x have a rank greater than 2, then x is flattened prior to the initial matrix multiply by weights .","title":"Adds a fully connected layer"},{"location":"core/layers/#adds-a-2d-convolutional-layer","text":"tefla.core.layers.conv2d (x, n_output_channels, is_training, reuse, trainable=True, filter_size= (3, 3), stride= (1, 1), padding='SAME', w_init= , b_init=0.0, w_regularizer= , untie_biases=False, name='conv2d', batch_norm=None, batch_norm_args=None, activation=None, use_bias=True, outputs_collections=None) convolutional layer creates a variable called weights , representing a conv weight matrix, which is multiplied by the x to produce a Tensor of hidden units. If a batch_norm is provided (such as batch_norm ), it is then applied. Otherwise, if batch_norm is None and a b_init and use_bias is provided then a biases variable would be created and added the hidden units. Finally, if activation is not None , it is applied to the hidden units as well. Note: that if x have a rank 4","title":"Adds a 2D convolutional layer"},{"location":"core/layers/#adds-a-2d-dilated-convolutional-layer","text":"tefla.core.layers.dilated_conv2d (x, n_output_channels, is_training, reuse, trainable=True, filter_size= (3, 3), dilation=1, stride=1, padding='SAME', w_init= , b_init=0.0, w_regularizer= , untie_biases=False, name='dilated_conv2d', batch_norm=None, batch_norm_args=None, activation=None, use_bias=True, outputs_collections=None) also known as convolution with holes or atrous convolution. If the rate parameter is equal to one, it performs regular 2-D convolution. If the rate parameter is greater than one, it performs convolution with holes, sampling the input values every rate pixels in the height and width dimensions. convolutional layer creates a variable called weights , representing a conv weight matrix, which is multiplied by the x to produce a Tensor of hidden units. If a batch_norm is provided (such as batch_norm ), it is then applied. Otherwise, if batch_norm is None and a b_init and use_bias is provided then a biases variable would be created and added the hidden units. Finally, if activation is not None , it is applied to the hidden units as well. Note: that if x have a rank 4","title":"Adds a 2D dilated convolutional layer"},{"location":"core/layers/#adds-a-2d-seperable-convolutional-layer","text":"tefla.core.layers.separable_conv2d (x, n_output_channels, is_training, reuse, trainable=True, filter_size= (3, 3), stride= (1, 1), depth_multiplier=1, padding='SAME', w_init= , b_init=0.0, w_regularizer= , untie_biases=False, name='separable_conv2d', batch_norm=None, batch_norm_args=None, activation=None, use_bias=True, outputs_collections=None) Performs a depthwise convolution that acts separately on channels followed by a pointwise convolution that mixes channels. Note that this is separability between dimensions [1, 2] and 3, not spatial separability between dimensions 1 and 2. convolutional layer creates two variable called depthwise_W and pointwise_W , depthwise_W is multiplied by x to produce depthwise conolution, which is multiplied by the pointwise_W to produce a output Tensor If a batch_norm is provided (such as batch_norm ), it is then applied. Otherwise, if batch_norm is None and a b_init and use_bias is provided then a biases variable would be created and added the hidden units. Finally, if activation is not None , it is applied to the hidden units as well. Note: that if x have a rank 4","title":"Adds a 2D seperable convolutional layer"},{"location":"core/layers/#adds-a-2d-sdepthwise-convolutional-layer","text":"tefla.core.layers.depthwise_conv2d (x, depth_multiplier, is_training, reuse, trainable=True, filter_size= (3, 3), stride= (1, 1), padding='SAME', w_init= , b_init=0.0, w_regularizer= , untie_biases=False, name='depthwise_conv2d', batch_norm=None, batch_norm_args=None, activation=None, use_bias=True, outputs_collections=None) Given an input tensor of shape [batch, in_height, in_width, in_channels] and a filter tensor of shape [filter_height, filter_width, in_channels, channel_multiplier] containing in_channels convolutional filters of depth 1, depthwise_conv2d applies a different filter to each input channel (expanding from 1 channel to channel_multiplier channels for each), then concatenates the results together. The output has in_channels * channel_multiplier channels. If a batch_norm is provided (such as batch_norm ), it is then applied. Otherwise, if batch_norm is None and a b_init and use_bias is provided then a biases variable would be created and added the hidden units. Finally, if activation is not None , it is applied to the hidden units as well. Note: that if x have a rank 4","title":"Adds a 2D sdepthwise convolutional layer"},{"location":"core/layers/#adds-a-3d-convolutional-layer","text":"tefla.core.layers.conv3d (x, n_output_channels, is_training, reuse, trainable=True, filter_size= (3, 3, 3), stride= (1, 1, 1), padding='SAME', w_init= , b_init=0.0, w_regularizer= , untie_biases=False, name='conv3d', batch_norm=None, batch_norm_args=None, activation=None, use_bias=True, outputs_collections=None) convolutional layer creates a variable called weights , representing a conv weight matrix, which is multiplied by the x to produce a Tensor of hidden units. If a batch_norm is provided (such as batch_norm ), it is then applied. Otherwise, if batch_norm is None and a b_init and use_bias is provided then a biases variable would be created and added the hidden units. Finally, if activation is not None , it is applied to the hidden units as well. Note: that if x have a rank 5","title":"Adds a 3D convolutional layer"},{"location":"core/layers/#adds-a-2d-upsampling-or-deconvolutional-layer","text":"tefla.core.layers.upsample2d (input_, output_shape, is_training, reuse, trainable=True, filter_size= (5, 5), stride= (2, 2), w_init= , b_init=0.0, w_regularizer= , batch_norm=None, batch_norm_args=None, activation=None, name='deconv2d', use_bias=True, with_w=False, outputs_collections=None, **unused) his operation is sometimes called \"deconvolution\" after Deconvolutional Networks, but is actually the transpose (gradient) of conv2d rather than an actual deconvolution. If a batch_norm is provided (such as batch_norm ), it is then applied. Otherwise, if batch_norm is None and a b_init and use_bias is provided then a biases variable would be created and added the hidden units. Finally, if activation is not None , it is applied to the hidden units as well. Note: that if x have a rank 4","title":"Adds a 2D upsampling or deconvolutional layer"},{"location":"core/layers/#adds-a-3d-upsampling-or-deconvolutional-layer","text":"tefla.core.layers.upsample3d (input_, output_shape, is_training, reuse, trainable=True, filter_size= (5, 5, 5), stride= (2, 2, 2), w_init= , b_init=0.0, w_regularizer= , batch_norm=None, batch_norm_args=None, activation=None, name='deconv3d', use_bias=True, with_w=False, outputs_collections=None, **unused) his operation is sometimes called \"deconvolution\" after Deconvolutional Networks, but is actually the transpose (gradient) of conv2d rather than an actual deconvolution. If a batch_norm is provided (such as batch_norm ), it is then applied. Otherwise, if batch_norm is None and a b_init and use_bias is provided then a biases variable would be created and added the hidden units. Finally, if activation is not None , it is applied to the hidden units as well. Note: that if x have a rank 5","title":"Adds a 3D upsampling or deconvolutional layer"},{"location":"core/layers/#adds-a-1d-convolutional-layer","text":"tefla.core.layers.conv1d (x, n_output_channels, is_training, reuse, trainable=True, filter_size=3, stride=1, padding='SAME', w_init= , b_init=0.0, w_regularizer= , untie_biases=False, name='conv1d', batch_norm=None, batch_norm_args=None, activation=None, use_bias=True, outputs_collections=None) convolutional layer creates a variable called weights , representing a conv weight matrix, which is multiplied by the x to produce a Tensor of hidden units. If a batch_norm is provided (such as batch_norm ), it is then applied. Otherwise, if batch_norm is None and a b_init and use_bias is provided then a biases variable would be created and added the hidden units. Finally, if activation is not None , it is applied to the hidden units as well. Note: that if x have a rank 4","title":"Adds a 1D convolutional layer"},{"location":"core/layers/#max-pooling-1d","text":"tefla.core.layers.max_pool_1d (x, filter_size=3, stride=2, padding='SAME', name='maxpool1d', outputs_collections=None, **unused)","title":"Max Pooling 1D"},{"location":"core/layers/#avg-pooling-1d","text":"tefla.core.layers.avg_pool_1d (x, filter_size=3, stride=2, padding='SAME', name='avgpool1d', outputs_collections=None, **unused)","title":"Avg Pooling 1D"},{"location":"core/layers/#adds-a-2d-highway-convolutional-layer","text":"tefla.core.layers.highway_conv2d (x, n_output, is_training, reuse, trainable=True, filter_size= (3, 3), stride= (1, 1), padding='SAME', w_init= , b_init=0.0, w_regularizer= , name='highway_conv2d', batch_norm=None, batch_norm_args=None, activation= , use_bias=True, outputs_collections=None) https://arxiv.org/abs/1505.00387 If a batch_norm is provided (such as batch_norm ), it is then applied. Otherwise, if batch_norm is None and a b_init and use_bias is provided then a biases variable would be created and added the hidden units. Finally, if activation is not None , it is applied to the hidden units as well. Note: that if x have a rank 4","title":"Adds a 2D highway convolutional layer"},{"location":"core/layers/#adds-a-fully-connected-highway-layer","text":"tefla.core.layers.highway_fc2d (x, n_output, is_training, reuse, trainable=True, filter_size= (3, 3), w_init= , b_init=0.0, w_regularizer= , name='highway_fc2d', activation=None, use_bias=True, outputs_collections=None) https://arxiv.org/abs/1505.00387 If a batch_norm is provided (such as batch_norm ), it is then applied. Otherwise, if batch_norm is None and a b_init and use_bias is provided then a biases variable would be created and added the hidden units. Finally, if activation is not None , it is applied to the hidden units as well. Note: that if x have a rank greater than 2, then x is flattened prior to the initial matrix multiply by weights .","title":"Adds a fully connected highway layer"},{"location":"core/layers/#max-pooling-layer","text":"tefla.core.layers.max_pool (x, filter_size= (3, 3), stride= (2, 2), padding='SAME', name='pool', outputs_collections=None, **unused)","title":"Max pooling layer"},{"location":"core/layers/#max-pooling-layer_1","text":"tefla.core.layers.max_pool_3d (x, filter_size= (3, 3, 3), stride= (2, 2, 2), padding='SAME', name='pool', outputs_collections=None, **unused)","title":"Max pooling layer"},{"location":"core/layers/#fractional-pooling-layer","text":"tefla.core.layers.fractional_pool (x, pooling_ratio=[1.0, 1.44, 1.73, 1.0], pseudo_random=None, determinastic=None, overlapping=None, name='fractional_pool', seed=None, seed2=None, type='avg', outputs_collections=None, **unused)","title":"Fractional pooling layer"},{"location":"core/layers/#rms-pooling-layer","text":"tefla.core.layers.rms_pool_2d (x, filter_size= (3, 3), stride= (2, 2), padding='SAME', name='pool', epsilon=1e-12, outputs_collections=None, **unused)","title":"RMS pooling layer"},{"location":"core/layers/#rms-pooling-layer_1","text":"tefla.core.layers.rms_pool_3d (x, filter_size= (3, 3, 3), stride= (2, 2, 2), padding='SAME', name='pool', epsilon=1e-12, outputs_collections=None, **unused)","title":"RMS pooling layer"},{"location":"core/layers/#avg-pooling-layer","text":"tefla.core.layers.avg_pool_3d (x, filter_size= (3, 3, 3), stride= (2, 2, 2), padding='SAME', name=None, outputs_collections=None, **unused)","title":"Avg pooling layer"},{"location":"core/layers/#avg-pooling-layer_1","text":"tefla.core.layers.avg_pool_2d (x, filter_size= (3, 3), stride= (2, 2), padding='SAME', name=None, outputs_collections=None, **unused)","title":"Avg pooling layer"},{"location":"core/layers/#gloabl-pooling-layer","text":"tefla.core.layers.global_avg_pool (x, name='global_avg_pool', outputs_collections=None, **unused)","title":"Gloabl pooling layer"},{"location":"core/layers/#gloabl-max-pooling-layer","text":"tefla.core.layers.global_max_pool (x, name='global_max_pool', outputs_collections=None, **unused)","title":"Gloabl max pooling layer"},{"location":"core/layers/#feature-max-pooling-layer","text":"tefla.core.layers.feature_max_pool_1d (x, stride=2, name='feature_max_pool_1d', outputs_collections=None, **unused)","title":"Feature max pooling layer"},{"location":"core/layers/#feature-max-pooling-layer_1","text":"tefla.core.layers.feature_max_pool_2d (x, stride=2, name='feature_max_pool_2d', outputs_collections=None, **unused)","title":"Feature max pooling layer"},{"location":"core/layers/#adds-a-batch-normalization-layer-from-httparxivorgabs150203167","text":"tefla.core.layers.batch_norm_tf (x, name='bn', scale=False, updates_collections=None, **kwargs) \"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\", Sergey Ioffe, Christian Szegedy Can be used as a normalizer function for conv2d and fully_connected. Note: When is_training is True the moving_mean and moving_variance need to be updated, by default the update_ops are placed in tf.GraphKeys.UPDATE_OPS so they need to be added as a dependency to the train_op , example: update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) if update_ops: updates = tf.group(*update_ops) total_loss = control_flow_ops.with_dependencies([updates], total_loss) One can set updates_collections=None to force the updates in place, but that can have speed penalty, specially in distributed settings.","title":"Adds a Batch Normalization layer from http://arxiv.org/abs/1502.03167"},{"location":"core/layers/#adds-a-batch-normalization-layer-from-httparxivorgabs150203167_1","text":"tefla.core.layers.batch_norm_lasagne (x, is_training, reuse, trainable=True, decay=0.9, epsilon=0.0001, name='bn', updates_collections='update_ops', outputs_collections=None) Instead of storing and updating moving variance, this layer store and update moving inverse standard deviation \"Batch Normalization: Accelerating Deep Network Training by Reducin Internal Covariate Shift\" Sergey Ioffe, Christian Szegedy Can be used as a normalizer function for conv2d and fully_connected. Note: When is_training is True the moving_mean and moving_variance need to be updated, by default the update_ops are placed in tf.GraphKeys.UPDATE_OPS so they need to be added as a dependency to the train_op , example: update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) if update_ops: updates = tf.group(*update_ops) total_loss = control_flow_ops.with_dependencies([updates], total_loss) One can set updates_collections=None to force the updates in place, but that can have speed penalty, specially in distributed settings.","title":"Adds a Batch Normalization layer from http://arxiv.org/abs/1502.03167"},{"location":"core/layers/#layer-normalize-the-tensor-x-averaging-over-the-last-dimension","text":"tefla.core.layers.layer_norm (x, reuse, filters=None, trainable=True, epsilon=1e-06, name='layer_norm', allow_defun=False, outputs_collections=None)","title":"Layer normalize the tensor x, averaging over the last dimension"},{"location":"core/layers/#prametric-rectifier-linear-layer","text":"tefla.core.layers.prelu (x, reuse, alpha_init=0.2, trainable=True, name='prelu', outputs_collections=None)","title":"Prametric rectifier linear layer"},{"location":"core/layers/#computes-relu","text":"tefla.core.layers.relu (x, name='relu', outputs_collections=None, **unused)","title":"Computes relu"},{"location":"core/layers/#rectifier-linear-relu6-layer","text":"tefla.core.layers.relu6 (x, name='relu6', outputs_collections=None, **unused)","title":"Rectifier linear relu6 layer"},{"location":"core/layers/#softplus-layer","text":"tefla.core.layers.softplus (x, name='softplus', outputs_collections=None, **unused) Computes softplus: log(exp(x) + 1).","title":"Softplus layer"},{"location":"core/layers/#softsign-layer","text":"tefla.core.layers.softsign (x, name='softsign', outputs_collections=None, **unused) Computes softsign: x / (abs(x) + 1).","title":"Softsign layer"},{"location":"core/layers/#computes-concatenated-relu","text":"tefla.core.layers.crelu (x, name='crelu', outputs_collections=None, **unused) Concatenates a ReLU which selects only the positive part of the activation with a ReLU which selects only the negative part of the activation. Note that at as a result this non-linearity doubles the depth of the activations. Source: https://arxiv.org/abs/1603.05201","title":"Computes Concatenated ReLU"},{"location":"core/layers/#computes-exponential-linear-expfeatures-1-if-0-features-otherwise","text":"tefla.core.layers.elu (x, name='elu', outputs_collections=None, **unused) See \"Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)\"","title":"Computes exponential linear: exp(features) - 1 if &lt; 0, features otherwise"},{"location":"core/layers/#like-concatenated-relu-httparxivorgabs160305201-but-then-with-elu","text":"tefla.core.layers.concat_elu (x, name='concat_elu', outputs_collections=None, **unused)","title":"like concatenated ReLU (http://arxiv.org/abs/1603.05201), but then with ELU"},{"location":"core/layers/#computes-leaky-relu","text":"tefla.core.layers.leaky_relu (x, alpha=0.01, name='leaky_relu', outputs_collections=None, **unused)","title":"Computes leaky relu"},{"location":"core/layers/#computes-reaky-relu-lasagne-style","text":"tefla.core.layers.lrelu (x, leak=0.2, name='lrelu', outputs_collections=None, **unused)","title":"Computes reaky relu lasagne style"},{"location":"core/layers/#computes-maxout-activation","text":"tefla.core.layers.maxout (x, k=2, name='maxout', outputs_collections=None, **unused)","title":"Computes maxout activation"},{"location":"core/layers/#computes-maxout-activation_1","text":"tefla.core.layers.offset_maxout (x, k=2, name='maxout', outputs_collections=None, **unused)","title":"Computes maxout activation"},{"location":"core/layers/#computes-softmax-activation","text":"tefla.core.layers.softmax (x, name='softmax', outputs_collections=None, **unused)","title":"Computes softmax activation"},{"location":"core/layers/#computes-the-spatial-softmax-of-a-convolutional-feature-map","text":"tefla.core.layers.spatial_softmax (features, reuse, temperature=None, name='spatial_softmax', trainable=True, outputs_collections=None, **unused) First computes the softmax over the spatial extent of each channel of a convolutional feature map. Then computes the expected 2D position of the points of maximal activation for each channel, resulting in a set of feature keypoints [x1, y1, ... xN, yN] for all N channels. Read more here: \"Learning visual feature spaces for robotic manipulation with deep spatial autoencoders.\" Finn et. al, http://arxiv.org/abs/1509.06113.","title":"Computes the spatial softmax of a convolutional feature map"},{"location":"core/layers/#computes-selu","text":"tefla.core.layers.selu (x, alpha=None, scale=None, name='selu', outputs_collections=None, **unused)","title":"Computes selu"},{"location":"core/layers/#dropout-layer-for-self-normalizing-networks","text":"tefla.core.layers.dropout_selu (x, is_training, drop_p=0.2, alpha=-1.7580993408473766, fixedPointMean=0.0, fixedPointVar=1.0, noise_shape=None, seed=None, name='dropout_selu', outputs_collections=None, **unused)","title":"Dropout layer for self normalizing networks"},{"location":"core/layers/#computes-gumbel-softmax","text":"tefla.core.layers.gumbel_softmax (logits, temperature, hard=False) Sample from the Gumbel-Softmax distribution and optionally discretize. http://blog.evjang.com/2016/11/tutorial-categorical-variational.html https://arxiv.org/abs/1611.01144","title":"Computes Gumbel Softmax"},{"location":"core/layers/#computes-pixel-wise-softmax-activation","text":"tefla.core.layers.pixel_wise_softmax (inputs)","title":"Computes pixel wise softmax activation"},{"location":"core/layers/#dropout-layer","text":"tefla.core.layers.dropout (x, is_training, drop_p=0.5, seed=None, name='dropout', outputs_collections=None, **unused)","title":"Dropout layer"},{"location":"core/layers/#repeat-op","text":"tefla.core.layers.repeat (x, repetitions, layer, num_outputs=None, name='Repeat', outputs_collections=None, args, *kwargs)","title":"Repeat op"},{"location":"core/layers/#merge-op","text":"tefla.core.layers.merge (tensors_list, mode, axis=1, name='merge', outputs_collections=None, **kwargs)","title":"Merge op"},{"location":"core/layers/#builds-a-stack-of-layers-by-applying-layer-repeatedly-using-stack_args","text":"tefla.core.layers.stack (inputs, layer, stack_args, is_training, reuse, outputs_collections=None, **kwargs) stack allows you to repeatedly apply the same operation with different arguments stack_args[i] . For each application of the layer, stack creates a new scope appended with an increasing number. For example: y = stack(x, fully_connected, [32, 64, 128], scope='fc') # It is equivalent to: x = fully_connected(x, 32, scope='fc/fc_1') x = fully_connected(x, 64, scope='fc/fc_2') y = fully_connected(x, 128, scope='fc/fc_3') If the scope argument is not given in kwargs , it is set to layer.__name__ , or layer.func.__name__ (for functools.partial objects). If neither __name__ nor func.__name__ is available, the layers are called with scope='stack' .","title":"Builds a stack of layers by applying layer repeatedly using stack_args"},{"location":"core/layers/#normalizes-the-given-input-across-the-specified-dimension-to-unit-length","text":"tefla.core.layers.unit_norm (inputs, dim, epsilon=1e-07, scope=None) Note that the rank of input must be known.","title":"Normalizes the given input across the specified dimension to unit length"},{"location":"core/layers/#concates-two-features-maps","text":"tefla.core.layers.crop_and_concat (inputs1, inputs2, name='crop_concat') concates different sizes feature maps cropping the larger map concatenation across output channels","title":"Concates two features maps"},{"location":"core/learning/","text":"Supervised Trainer, support data parallelism, multi GPU tefla.core.learning.SupervisedLearner (model, cnf, clip_by_global_norm=False, **kwargs) Args model : model definition cnf : dict, training configs training_iterator : iterator to use for training data access, processing and augmentations validation_iterator : iterator to use for validation data access, processing and augmentations start_epoch : int, training start epoch; for resuming training provide the last epoch number to resume training from, its a required parameter for training data balancing resume_lr : float, learning rate to use for new training classification : bool, classificattion or regression clip_norm : bool, to clip gradient using gradient norm, stabilizes the training n_iters_per_epoch : int, number of iteratiosn for each epoch; e.g: total_training_samples/batch_size gpu_memory_fraction : amount of gpu memory to use is_summary : bool, to write summary or not Methods fit (data_set, weights_from=None, weights_dir='weights', start_epoch=1, summary_every=10, keep_moving_averages=False, **kwargs) Args data_set : dataset instance to use to access data for training/validation weights_from : str, if not None, initializes model from exisiting weights start_epoch : int, epoch number to start training from e.g. for retarining set the epoch number you want to resume training from summary_every : int, epoch interval to write summary; higher value means lower frequency of summary writing keep_moving_averages : a bool, keep moving averages of trainable variables","title":"Learner Multi GPU"},{"location":"core/learning/#supervised-trainer-support-data-parallelism-multi-gpu","text":"tefla.core.learning.SupervisedLearner (model, cnf, clip_by_global_norm=False, **kwargs)","title":"Supervised Trainer, support data parallelism, multi GPU"},{"location":"core/learning_ss/","text":"Semi Supervised Trainer tefla.core.learning_ss.SemiSupervisedTrainer (model, cnf, clip_by_global_norm=False, **kwargs) Args model : model definition cnf : dict, training configs training_iterator : iterator to use for training data access, processing and augmentations validation_iterator : iterator to use for validation data access, processing and augmentations start_epoch : int, training start epoch; for resuming training provide the last epoch number to resume training from, its a required parameter for training data balancing resume_lr : float, learning rate to use for new training classification : bool, classificattion or regression clip_norm : bool, to clip gradient using gradient norm, stabilizes the training n_iters_per_epoch : int, number of iteratiosn for each epoch; e.g: total_training_samples/batch_size gpu_memory_fraction : amount of gpu memory to use is_summary : bool, to write summary or not Methods fit (data_set, num_classes=6, weights_from=None, start_epoch=1, summary_every=199, model_name='multiclass_ss', weights_dir='weights') Args data_set : dataset instance to use to access data for training/validation weights_from : str, if not None, initializes model from exisiting weights start_epoch : int, epoch number to start training from e.g. for retarining set the epoch number you want to resume training from summary_every : int, epoch interval to write summary; higher value means lower frequency of summary writing sigmoid_kl_with_logits (logits, targets) Args logits : logits targets : smooth targets Returns cross entropy loss","title":"Learner Semi Supervised"},{"location":"core/learning_ss/#semi-supervised-trainer","text":"tefla.core.learning_ss.SemiSupervisedTrainer (model, cnf, clip_by_global_norm=False, **kwargs)","title":"Semi Supervised Trainer"},{"location":"core/learning_v2/","text":"Supervised Learner, support data parallelism, multi GPU, accept TFRecords data as input tefla.core.learning_v2.SupervisedLearner (model, cnf, clip_by_global_norm=False, data_balancing=1, **kwargs) Args model : model definition cnf : dict, training configs training_iterator : iterator to use for training data access, processing and augmentations validation_iterator : iterator to use for validation data access, processing and augmentations start_epoch : int, training start epoch; for resuming training provide the last epoch number to resume training from, its a required parameter for training data balancing resume_lr : float, learning rate to use for new training classification : bool, classificattion or regression clip_norm : bool, to clip gradient using gradient norm, stabilizes the training n_iters_per_epoch : int, number of iteratiosn for each epoch; e.g: total_training_samples/batch_size gpu_memory_fraction : amount of gpu memory to use is_summary : bool, to write summary or not Methods fit (data_dir, data_dir_val=None, features_keys=None, weights_from=None, weights_dir='weights', max_to_keep=None, start_epoch=1, summary_every=10, training_set_size=None, val_set_size=None, dataset_name='cifar10', keep_moving_averages=False) Args data_dir : str, training dataset directory (where tfrecords are staored for training) data_dir_val : str optional, validation dataset directory (where tfrecords are stored for validation) features_keys : a dict, tfrecords keys to datum features e.g.: features_keys = { 'image/encoded/image': tf.FixedLenFeature((), tf.string, default_value=''), 'image/format': tf.FixedLenFeature((), tf.string, default_value='jpg'), 'image/class/label': tf.FixedLenFeature([], tf.int64, default_value=tf.zeros([], dtype=tf.int64)), } weights_from : str, if not None, initializes model from exisiting weights training_set_size : int, number of training examples val_set_size : int, set if data_dir_val not None, number of validation examples dataset_name : a optional, Name of the dataset start_epoch : int, epoch number to start training from e.g. for retarining set the epoch number you want to resume training from summary_every : int, epoch interval to write summary; higher value means lower frequency of summary writing keep_moving_averages : a bool, keep moving averages of trainable variables","title":"Learner Multi GPU V2"},{"location":"core/learning_v2/#supervised-learner-support-data-parallelism-multi-gpu-accept-tfrecords-data-as-input","text":"tefla.core.learning_v2.SupervisedLearner (model, cnf, clip_by_global_norm=False, data_balancing=1, **kwargs)","title":"Supervised Learner, support data parallelism, multi GPU, accept TFRecords data as input"},{"location":"core/logger/","text":"Set the log file name, using append mode tefla.core.logger.setFileHandler (filename, mode='a') Args filename : log file name e.g. tefla.log mode : file writing mode, append/over write if exists else start new file e.g. a string, 'a' or 'w' set the verbosity level of logging tefla.core.logger.setVerbosity (verbosity=0) Args verbosity : set the verbosity level using an integer {0, 1, 2, 3, 4} e.g. verbosity=0, imply DEBUG logging, it logs all level of logs verbosity=1, imply INFO logging verbosity=2, imply WARN logging verbosity=3, imply ERROR logging verbosity=4, imply FATAL logging, it logs only the lowest FATAL level Logs the Highest level DEBUG logging, it logs all level tefla.core.logger.debug (msg, args, *kwargs) Args msg : the message to log Logs the level INFO logging, it logs all LEVEL BELOW INFO tefla.core.logger.info (msg, args, *kwargs) Args msg : the message to log Logs the WARN logging, it logs all level BELOW WARN tefla.core.logger.warn (msg, args, *kwargs) Args msg : the message to log Logs the level ERROR logging, it logs level ERROR and FATAL tefla.core.logger.error (msg, args, *kwargs) Args msg : the message to log Logs thE level FATAL logging, it logs only FATAL tefla.core.logger.fatal (msg, args, *kwargs) Args msg : the message to log","title":"Logger"},{"location":"core/logger/#set-the-log-file-name-using-append-mode","text":"tefla.core.logger.setFileHandler (filename, mode='a')","title":"Set the log file name, using append mode"},{"location":"core/logger/#set-the-verbosity-level-of-logging","text":"tefla.core.logger.setVerbosity (verbosity=0)","title":"set the verbosity level of logging"},{"location":"core/logger/#logs-the-highest-level-debug-logging-it-logs-all-level","text":"tefla.core.logger.debug (msg, args, *kwargs)","title":"Logs the Highest level DEBUG logging, it logs all level"},{"location":"core/logger/#logs-the-level-info-logging-it-logs-all-level-below-info","text":"tefla.core.logger.info (msg, args, *kwargs)","title":"Logs the level INFO logging, it logs all LEVEL BELOW INFO"},{"location":"core/logger/#logs-the-warn-logging-it-logs-all-level-below-warn","text":"tefla.core.logger.warn (msg, args, *kwargs)","title":"Logs the WARN logging, it logs all level BELOW WARN"},{"location":"core/logger/#logs-the-level-error-logging-it-logs-level-error-and-fatal","text":"tefla.core.logger.error (msg, args, *kwargs)","title":"Logs the level ERROR logging, it logs level ERROR  and FATAL"},{"location":"core/logger/#logs-the-level-fatal-logging-it-logs-only-fatal","text":"tefla.core.logger.fatal (msg, args, *kwargs)","title":"Logs thE level FATAL logging, it logs only FATAL"},{"location":"core/losses/","text":"Define a log loss tefla.core.losses.log_loss_custom (predictions, labels, eps=1e-07, name='log') Args predictions : 2D tensor or array, [batch_size, num_classes] predictions of the network . labels : 2D or array tensor, [batch_size, num_classes] ground truth labels or target labels. eps : a constant to set upper or lower limit for labels, smoothening factor name : Optional scope/name for op_scope. Returns A tensor with the log loss. Define a log loss tefla.core.losses.log_loss_tf (predictions, labels, eps=1e-07, weights=1.0, name='log_loss') Args predictions : 2D tensor or array, [batch_size, num_classes] predictions of the network . labels : 2D or array tensor, [batch_size, num_classes] ground truth labels or target labels. eps : a constant to set upper or lower limit for labels, smoothening factor name : Optional scope/name for op_scope. Returns A tensor with the log loss. Define a kappa loss, Its a continuous differentiable approximation of discrete kappa loss tefla.core.losses.kappa_loss (predictions, labels, y_pow=1, eps=1e-15, num_ratings=5, batch_size=32, name='kappa') Args predictions : 2D tensor or array, [batch_size, num_classes] predictions of the network . labels : 2D tensor or array,[batch_size, num_classes] ground truth labels or target labels. y_pow : int, to whcih the labels should be raised; useful if model diverge. e.g. y_pow=2 num_ratings : numbers of rater to used, typically num_classes of the model batch_size : batch_size of the training or validation ops eps : a float, prevents divide by zero name : Optional scope/name for op_scope. Returns A tensor with the kappa loss. Define a joint kappa and log loss, Kappa is a continuous differentiable approximation of discrete kappa loss tefla.core.losses.kappa_log_loss (predictions, labels, label_smoothing=0.0, y_pow=1, batch_size=32, log_scale=0.5, num_classes=5, log_offset=0.5, name='kappa_log') Args predictions : 2D tensor or array, [batch_size, num_classes] predictions of the network . labels : 2D tensor or array,[batch_size, num_classes] ground truth labels or target labels. label_smoothing : a float, used to smooth the labels for better generalization if greater than 0 then smooth the labels. y_pow : int, to whcih the labels should be raised; useful if model diverge. e.g. y_pow=2 num_ratings : numbers of rater to used, typically num_classes of the model batch_size : batch_size of the training or validation ops log_scale : a float, used to multiply the clipped log loss, e.g: 0.5 log_offset :a float minimum log loss offset to substract from original log loss; e.g. 0.50 name : Optional scope/name for op_scope. Returns A tensor with the kappa log loss. Define a joint kappa and log loss; log loss is clipped by a defined min value; Kappa is a continuous differentiable approximation of discrete kappa loss tefla.core.losses.kappa_log_loss_clipped (predictions, labels, label_smoothing=0.0, y_pow=1, batch_size=32, log_scale=0.5, log_cutoff=0.8, num_classes=5, name='kappa_log_clipped') Args predictions : 2D tensor or array, [batch_size, num_classes] predictions of the network . labels : 2D tensor or array,[batch_size, num_classes] ground truth labels or target labels. label_smoothing : a float, used to smooth the labels for better generalization if greater than 0 then smooth the labels. y_pow : int, to whcih the labels should be raised; useful if model diverge. e.g. y_pow=2 num_ratings : numbers of rater to used, typically num_classes of the model batch_size : batch_size of the training or validation ops log_scale : a float, used to multiply the clipped log loss, e.g: 0.5 log_cutoff :a float, minimum log loss value; e.g. 0.50 name : Optional scope/name for op_scope. Returns A tensor with the clipped kappa log loss. Define a cross entropy loss with label smoothing tefla.core.losses.cross_entropy_loss (logits, labels, label_smoothing=0.0, weight=1.0, name='cross_entropy_loss') Args predictions : 2D tensor or array, [batch_size, num_classes] predictions of the network . labels : 2D tensor or array,[batch_size, num_classes] ground truth labels or target labels. label_smoothing : a float, used to smooth the labels for better generalizationif greater than 0 then smooth the labels. weight : scale the loss by this factor. name : Optional scope/name for op_scope. Returns A tensor with the cross entropy loss. Define a L2Loss, useful for regularize, i.e. weight decay tefla.core.losses.l1_l2_regularizer (var, weight_l1=1.0, weight_l2=1.0, name='l1_l2_regularizer') Args var : tensor to regularize. weight_l1 : an optional weight to modulate the l1 loss. weight_l2 : an optional weight to modulate the l2 loss. name : Optional scope/name for op_scope. Returns the l1+L2 loss op. Returns a function that can be used to apply L1 regularization to weights tefla.core.losses.l1_regularizer (scale, name='l1_regularizer') L1 regularization encourages sparsity. Args scale: A scalar multiplier Tensor . 0.0 disables the regularizer. name: An optional name/scope name. Returns A function with signature l1(weights) that apply L1 regularization. Returns a function that can be used to apply L2 regularization to weights tefla.core.losses.l2_regularizer (scale, name='l2_regularizer') Small values of L2 can help prevent overfitting the training data. Args scale: A scalar multiplier Tensor . 0.0 disables the regularizer. name: An optional name/scope name. Returns A function with signature l2(weights) that applies L2 regularization. log-likelihood for mixture of discretized logistics, assumes the data has been rescaled to [-1,1] interval tefla.core.losses.discretized_mix_logistic_loss (inputs, predictions, sum_all=True, name='disretized_mix_logistic_loss') Args predictions : 4D tensor or array, [batch_size, width, height, out_channels] predictions of the network . inputs : 4D tensor or array, [batch_size, width, height, num_classes] ground truth labels or target labels. name : Optional scope/name for op_scope. Returns A tensor with the discretized mix logistic loss. Pull Away loss calculation tefla.core.losses.pullaway_loss (embeddings, name='pullaway_loss') Args embeddings : The embeddings to be orthogonalized for varied faces. Shape [batch_size, embeddings_dim] Calculate the loss from the logits and the labels tefla.core.losses.segment_loss (logits, labels, num_classes, head=None) Args logits: tensor, float - [batch_size * width * height, num_classes]. - Use vgg_fcn.up as logits. labels: Labels tensor, int32 - [batch_size * width * height, num_classes]. - The ground truth of your data. head: numpy array - [num_classes] - Weighting the loss of each class - Optional: Prioritize some classes Returns loss: Loss tensor of type float. Calculate the triplet loss according to the FaceNet paper tefla.core.losses.triplet_loss (anchor, positive, negative, alpha=0.2, name='triplet_loss') Args anchor: 2-D tensor [batch_size, embedding_size], the embeddings for the anchor images. positive: 2-D tensor [batch_size, embedding_size], the embeddings for the positive images. negative: 2-D tensor [batch_size, embedding_size], the embeddings for the negative images. alpha: positive to negative triplet distance margin Returns the triplet loss. Decov loss as described in https://arxiv.org/pdf/1511.06068.pdf tefla.core.losses.decov_loss (xs, name='decov_loss') 'Reducing Overfitting In Deep Networks by Decorrelating Representation' Args xs : 4-D tensor [batch_size, height, width, channels], input Returns a float decov loss Center loss based on the paper \"A Discriminative Feature Learning Approach for Deep Face Recognition\" tefla.core.losses.center_loss (features, label, alpha, num_classes, name='center_loss') (http://ydwen.github.io/papers/WenECCV16.pdf) Args features : 2-D tensor [batch_size, feature_length], input features label : 1-D tensor [batch_size], input label alpha : center loss parameter num_classes : a int numof classes for training Returns a float , center loss Adds a similarity loss term, the correlation between two representations tefla.core.losses.correlation_loss (source_samples, target_samples, weight, name='corr_loss') Args source_samples : a tensor of shape [num_samples, num_features] target_samples : a tensor of shape [num_samples, num_features] weight : a scalar weight for the loss. scope : optional name scope for summary tags. Returns a scalar tensor representing the correlation loss value. Computes the Maximum Mean Discrepancy (MMD) of two samples: x and y tefla.core.losses.maximum_mean_discrepancy (x, y, kernel= , name='maximum_mean_discrepancy') Maximum Mean Discrepancy (MMD) is a distance-measure between the samples of the distributions of x and y. Here we use the kernel two sample estimate using the empirical mean of the two distributions. MMD^2(P, Q) = || \\E{\\phi(x)} - \\E{\\phi(y)} ||^2= \\E{ K(x, x) } + \\E{ K(y, y) } - 2 \\E{ K(x, y) }, where K = <\\phi(x), \\phi(y)>, is the desired kernel function, in this case a radial basis kernel. Args x : a tensor of shape [num_samples, num_features] y : a tensor of shape [num_samples, num_features] kernel : a function which computes the kernel in MMD. Defaults to theGaussianKernelMatrix. Returns a scalar denoting the squared maximum mean discrepancy loss. Adds a similarity loss term, the MMD between two representations tefla.core.losses.mmd_loss (source_samples, target_samples, weight, name='mmd_loss') This Maximum Mean Discrepancy (MMD) loss is calculated with a number of different Gaussian kernels. Args source_samples: a tensor of shape [num_samples, num_features]. target_samples: a tensor of shape [num_samples, num_features]. weight: the weight of the MMD loss. scope: optional name scope for summary tags. Returns a scalar tensor representing the MMD loss value. Adds the domain adversarial (DANN) loss tefla.core.losses.dann_loss (source_samples, target_samples, weight, name='dann_loss') Args source_samples: a tensor of shape [num_samples, num_features]. target_samples: a tensor of shape [num_samples, num_features]. weight: the weight of the loss. scope: optional name scope for summary tags. Returns a scalar tensor representing the correlation loss value. Adds the difference loss between the private and shared representations tefla.core.losses.difference_loss (private_samples, shared_samples, weight=1.0, name='difference_loss') Args private_samples: a tensor of shape [num_samples, num_features]. shared_samples: a tensor of shape [num_samples, num_features]. weight: the weight of the incoherence loss. name: the name of the tf summary. A helper function to compute the error between quaternions tefla.core.losses.log_quaternion_loss_batch (predictions, labels, name='log_quaternion_batch_loss') Args predictions: A Tensor of size [batch_size, 4]. labels: A Tensor of size [batch_size, 4]. params: A dictionary of parameters. Expecting 'use_logging', 'batch_size'. Returns A Tensor of size [batch_size], denoting the error between the quaternions. A helper function to compute the mean error between batches of quaternions tefla.core.losses.log_quaternion_loss (predictions, labels, batch_size, name='log_quaternion_loss') The caller is expected to add the loss to the graph. Args predictions: A Tensor of size [batch_size, 4]. labels: A Tensor of size [batch_size, 4]. params: A dictionary of parameters. Expecting 'use_logging', 'batch_size'. Returns A Tensor of size 1, denoting the mean error between batches of quaternions. Adds noise to embeddings and recomputes classification loss tefla.core.losses.random_perturbation_loss (embedded, length, loss_fn, perturb_norm_length=0.1) Args embedded : 3-D float Tensor , [batch_size, num_timesteps, embedding_dim] length : a int , length of the mask loss_fn : a callable, that returns loss perturb_norm_length : a float , Norm length of adversarial perturbation to be optimized with validatio Returns perturbation loss Adds gradient to embedding and recomputes classification loss tefla.core.losses.adversarial_loss (embedded, loss, loss_fn, perturb_norm_length=0.1) Args embedded : 3-D float Tensor , [batch_size, num_timesteps, embedding_dim] loss : float , loss loss_fn : a callable, that returns loss perturb_norm_length : a float , Norm length of adversarial perturbation to be optimized with validatio Returns adversial loss Virtual adversarial loss tefla.core.losses.virtual_adversarial_loss (logits, embedded, labels, length, logits_from_embedding_fn, num_classes, num_power_iteration=1, small_constant_for_finite_diff=0.001, perturb_norm_length=0.1) Computes virtual adversarial perturbation by finite difference method and power iteration, adds it to the embedding, and computes the KL divergence between the new logits and the original logits. Args logits : 2-D float Tensor , [num_timesteps*batch_size, m], where m=1 if num_classes=2, otherwise m=num_classes. embedded : 3-D float Tensor , [batch_size, num_timesteps, embedding_dim]. labels : 1-D Tensor , input labels length : a int , input length logits_from_embedding_fn : callable that takes embeddings and returns classifier logits. num_classes : num_classes for training vocab_size : a int , vocabular size of the problem num_power_iteration : a int , the number of power iteration small_constant_for_finite_diff : a float , Small constant for finite difference method perturb_norm_length : a float , Norm length of adversarial perturbation to be optimized with validatio Returns a float scalar , KL divergence. Adds noise to embeddings and recomputes classification loss fir bidirectional rnn models tefla.core.losses.random_perturbation_loss_brnn (embedded, length, loss_fn, perturb_norm_length=0.1) Args embedded : 3-D float Tensor , [batch_size, num_timesteps, embedding_dim] length : a int , length of the mask loss_fn : a callable, that returns loss perturb_norm_length : a float , Norm length of adversarial perturbation to be optimized with validatio Returns perturbation loss Adds gradient to embeddings and recomputes classification loss for bidirectional rnn models tefla.core.losses.adversarial_loss_brnn (embedded, loss, loss_fn, perurb_norm_length=0.1) Args embedded : 3-D float Tensor , [batch_size, num_timesteps, embedding_dim] loss : float , loss loss_fn : a callable, that returns loss perturb_norm_length : a float , Norm length of adversarial perturbation to be optimized with validatio Returns adversial loss Virtual adversarial loss for bidirectional models tefla.core.losses.virtual_adversarial_loss_brnn (logits, embedded, labels, length, logits_from_embedding_fn, vocab_size, num_classes, num_power_iteration=1, small_constant_for_finite_diff=0.001, perturb_norm_length=0.1) Computes virtual adversarial perturbation by finite difference method and power iteration, adds it to the embedding, and computes the KL divergence between the new logits and the original logits. Args logits : 2-D float Tensor , [num_timesteps*batch_size, m], where m=1 if num_classes=2, otherwise m=num_classes. embedded : 3-D float Tensor , [batch_size, num_timesteps, embedding_dim]. labels : 1-D Tensor , input labels length : a int , input length logits_from_embedding_fn : callable that takes embeddings and returns classifier logits. num_classes : num_classes for training vocab_size : a int , vocabular size of the problem num_power_iteration : a int , the number of power iteration small_constant_for_finite_diff : a float , Small constant for finite difference method perturb_norm_length : a float , Norm length of adversarial perturbation to be optimized with validatio Returns a float scalar , KL divergence. Generate a mask for the EOS token (1.0 on EOS, 0.0 otherwise) tefla.core.losses._end_of_seq_mask (tokens, vocab_size) Args tokens : 1-D integer Tensor [num_timesteps*batch_size]. Each element is an id from the vocab. vocab_size : a int , vocabular size of the problem Returns Float 1-D Tensor same shape as tokens, whose values are 1.0 on the end of sequence and 0.0 on the others. Returns weighted KL divergence between distributions q and p tefla.core.losses._kl_divergence_with_logits (q_logits, p_logits, weights, num_classes) Args q_logits : logits for 1st argument of KL divergence shape [num_timesteps * batch_size, num_classes] if num_classes > 2, and [num_timesteps * batch_size] if num_classes == 2. p_logits : logits for 2nd argument of KL divergence with same shape q_logits. weights : 1-D float tensor with shape [num_timesteps * batch_size]. Elements should be 1.0 only on end of sequences num_classes : a int , number of training classes Returns a float scalar , KL divergence. Calculates the per-example cross-entropy loss for a sequence of logits and tefla.core.losses.cross_entropy_sequence_loss (logits, targets, sequence_length) masks out all losses passed the sequence length. Args logits : Logits of shape [T, B, vocab_size] targets : Target classes of shape [T, B] sequence_length : An int32 tensor of shape [B] corresponding -to the length of each input Returns A tensor of shape [T, B] that contains the loss per example, per time step.","title":"Losses"},{"location":"core/losses/#define-a-log-loss","text":"tefla.core.losses.log_loss_custom (predictions, labels, eps=1e-07, name='log')","title":"Define a log loss"},{"location":"core/losses/#define-a-log-loss_1","text":"tefla.core.losses.log_loss_tf (predictions, labels, eps=1e-07, weights=1.0, name='log_loss')","title":"Define a log loss"},{"location":"core/losses/#define-a-kappa-loss-its-a-continuous-differentiable-approximation-of-discrete-kappa-loss","text":"tefla.core.losses.kappa_loss (predictions, labels, y_pow=1, eps=1e-15, num_ratings=5, batch_size=32, name='kappa')","title":"Define a kappa loss, Its a continuous differentiable approximation of discrete kappa loss"},{"location":"core/losses/#define-a-joint-kappa-and-log-loss-kappa-is-a-continuous-differentiable-approximation-of-discrete-kappa-loss","text":"tefla.core.losses.kappa_log_loss (predictions, labels, label_smoothing=0.0, y_pow=1, batch_size=32, log_scale=0.5, num_classes=5, log_offset=0.5, name='kappa_log')","title":"Define a joint kappa and log loss, Kappa is a continuous differentiable approximation of discrete kappa loss"},{"location":"core/losses/#define-a-joint-kappa-and-log-loss-log-loss-is-clipped-by-a-defined-min-value-kappa-is-a-continuous-differentiable-approximation-of-discrete-kappa-loss","text":"tefla.core.losses.kappa_log_loss_clipped (predictions, labels, label_smoothing=0.0, y_pow=1, batch_size=32, log_scale=0.5, log_cutoff=0.8, num_classes=5, name='kappa_log_clipped')","title":"Define a joint kappa and log loss; log loss is clipped by a defined min value; Kappa is a continuous differentiable approximation of discrete kappa loss"},{"location":"core/losses/#define-a-cross-entropy-loss-with-label-smoothing","text":"tefla.core.losses.cross_entropy_loss (logits, labels, label_smoothing=0.0, weight=1.0, name='cross_entropy_loss')","title":"Define a cross entropy loss with label smoothing"},{"location":"core/losses/#define-a-l2loss-useful-for-regularize-ie-weight-decay","text":"tefla.core.losses.l1_l2_regularizer (var, weight_l1=1.0, weight_l2=1.0, name='l1_l2_regularizer')","title":"Define a L2Loss, useful for regularize, i.e. weight decay"},{"location":"core/losses/#returns-a-function-that-can-be-used-to-apply-l1-regularization-to-weights","text":"tefla.core.losses.l1_regularizer (scale, name='l1_regularizer') L1 regularization encourages sparsity.","title":"Returns a function that can be used to apply L1 regularization to weights"},{"location":"core/losses/#returns-a-function-that-can-be-used-to-apply-l2-regularization-to-weights","text":"tefla.core.losses.l2_regularizer (scale, name='l2_regularizer') Small values of L2 can help prevent overfitting the training data.","title":"Returns a function that can be used to apply L2 regularization to weights"},{"location":"core/losses/#log-likelihood-for-mixture-of-discretized-logistics-assumes-the-data-has-been-rescaled-to-11-interval","text":"tefla.core.losses.discretized_mix_logistic_loss (inputs, predictions, sum_all=True, name='disretized_mix_logistic_loss')","title":"log-likelihood for mixture of discretized logistics, assumes the data has been rescaled to [-1,1] interval"},{"location":"core/losses/#pull-away-loss-calculation","text":"tefla.core.losses.pullaway_loss (embeddings, name='pullaway_loss')","title":"Pull Away loss calculation"},{"location":"core/losses/#calculate-the-loss-from-the-logits-and-the-labels","text":"tefla.core.losses.segment_loss (logits, labels, num_classes, head=None)","title":"Calculate the loss from the logits and the labels"},{"location":"core/losses/#calculate-the-triplet-loss-according-to-the-facenet-paper","text":"tefla.core.losses.triplet_loss (anchor, positive, negative, alpha=0.2, name='triplet_loss')","title":"Calculate the triplet loss according to the FaceNet paper"},{"location":"core/losses/#decov-loss-as-described-in-httpsarxivorgpdf151106068pdf","text":"tefla.core.losses.decov_loss (xs, name='decov_loss') 'Reducing Overfitting In Deep Networks by Decorrelating Representation'","title":"Decov loss as described in https://arxiv.org/pdf/1511.06068.pdf"},{"location":"core/losses/#center-loss-based-on-the-paper-a-discriminative-feature-learning-approach-for-deep-face-recognition","text":"tefla.core.losses.center_loss (features, label, alpha, num_classes, name='center_loss') (http://ydwen.github.io/papers/WenECCV16.pdf)","title":"Center loss based on the paper \"A Discriminative Feature Learning Approach for Deep Face Recognition\""},{"location":"core/losses/#adds-a-similarity-loss-term-the-correlation-between-two-representations","text":"tefla.core.losses.correlation_loss (source_samples, target_samples, weight, name='corr_loss')","title":"Adds a similarity loss term, the correlation between two representations"},{"location":"core/losses/#computes-the-maximum-mean-discrepancy-mmd-of-two-samples-x-and-y","text":"tefla.core.losses.maximum_mean_discrepancy (x, y, kernel= , name='maximum_mean_discrepancy') Maximum Mean Discrepancy (MMD) is a distance-measure between the samples of the distributions of x and y. Here we use the kernel two sample estimate using the empirical mean of the two distributions. MMD^2(P, Q) = || \\E{\\phi(x)} - \\E{\\phi(y)} ||^2= \\E{ K(x, x) } + \\E{ K(y, y) } - 2 \\E{ K(x, y) }, where K = <\\phi(x), \\phi(y)>, is the desired kernel function, in this case a radial basis kernel.","title":"Computes the Maximum Mean Discrepancy (MMD) of two samples: x and y"},{"location":"core/losses/#adds-a-similarity-loss-term-the-mmd-between-two-representations","text":"tefla.core.losses.mmd_loss (source_samples, target_samples, weight, name='mmd_loss') This Maximum Mean Discrepancy (MMD) loss is calculated with a number of different Gaussian kernels.","title":"Adds a similarity loss term, the MMD between two representations"},{"location":"core/losses/#adds-the-domain-adversarial-dann-loss","text":"tefla.core.losses.dann_loss (source_samples, target_samples, weight, name='dann_loss')","title":"Adds the domain adversarial (DANN) loss"},{"location":"core/losses/#adds-the-difference-loss-between-the-private-and-shared-representations","text":"tefla.core.losses.difference_loss (private_samples, shared_samples, weight=1.0, name='difference_loss')","title":"Adds the difference loss between the private and shared representations"},{"location":"core/losses/#a-helper-function-to-compute-the-error-between-quaternions","text":"tefla.core.losses.log_quaternion_loss_batch (predictions, labels, name='log_quaternion_batch_loss')","title":"A helper function to compute the error between quaternions"},{"location":"core/losses/#a-helper-function-to-compute-the-mean-error-between-batches-of-quaternions","text":"tefla.core.losses.log_quaternion_loss (predictions, labels, batch_size, name='log_quaternion_loss') The caller is expected to add the loss to the graph.","title":"A helper function to compute the mean error between batches of quaternions"},{"location":"core/losses/#adds-noise-to-embeddings-and-recomputes-classification-loss","text":"tefla.core.losses.random_perturbation_loss (embedded, length, loss_fn, perturb_norm_length=0.1)","title":"Adds noise to embeddings and recomputes classification loss"},{"location":"core/losses/#adds-gradient-to-embedding-and-recomputes-classification-loss","text":"tefla.core.losses.adversarial_loss (embedded, loss, loss_fn, perturb_norm_length=0.1)","title":"Adds gradient to embedding and recomputes classification loss"},{"location":"core/losses/#virtual-adversarial-loss","text":"tefla.core.losses.virtual_adversarial_loss (logits, embedded, labels, length, logits_from_embedding_fn, num_classes, num_power_iteration=1, small_constant_for_finite_diff=0.001, perturb_norm_length=0.1) Computes virtual adversarial perturbation by finite difference method and power iteration, adds it to the embedding, and computes the KL divergence between the new logits and the original logits.","title":"Virtual adversarial loss"},{"location":"core/losses/#adds-noise-to-embeddings-and-recomputes-classification-loss-fir-bidirectional-rnn-models","text":"tefla.core.losses.random_perturbation_loss_brnn (embedded, length, loss_fn, perturb_norm_length=0.1)","title":"Adds noise to embeddings and recomputes classification loss fir bidirectional rnn models"},{"location":"core/losses/#adds-gradient-to-embeddings-and-recomputes-classification-loss-for-bidirectional-rnn-models","text":"tefla.core.losses.adversarial_loss_brnn (embedded, loss, loss_fn, perurb_norm_length=0.1)","title":"Adds gradient to embeddings and recomputes classification loss for bidirectional rnn models"},{"location":"core/losses/#virtual-adversarial-loss-for-bidirectional-models","text":"tefla.core.losses.virtual_adversarial_loss_brnn (logits, embedded, labels, length, logits_from_embedding_fn, vocab_size, num_classes, num_power_iteration=1, small_constant_for_finite_diff=0.001, perturb_norm_length=0.1) Computes virtual adversarial perturbation by finite difference method and power iteration, adds it to the embedding, and computes the KL divergence between the new logits and the original logits.","title":"Virtual adversarial loss for bidirectional models"},{"location":"core/losses/#generate-a-mask-for-the-eos-token-10-on-eos-00-otherwise","text":"tefla.core.losses._end_of_seq_mask (tokens, vocab_size)","title":"Generate a mask for the EOS token (1.0 on EOS, 0.0 otherwise)"},{"location":"core/losses/#returns-weighted-kl-divergence-between-distributions-q-and-p","text":"tefla.core.losses._kl_divergence_with_logits (q_logits, p_logits, weights, num_classes)","title":"Returns weighted KL divergence between distributions q and p"},{"location":"core/losses/#calculates-the-per-example-cross-entropy-loss-for-a-sequence-of-logits-and","text":"tefla.core.losses.cross_entropy_sequence_loss (logits, targets, sequence_length) masks out all losses passed the sequence length.","title":"Calculates the per-example cross-entropy loss for a sequence of logits and"},{"location":"core/lr_policy/","text":"Training learning rate schedule based on inputs dict with epoch number as keys tefla.core.lr_policy.StepDecayPolicy (schedule, start_epoch=1) Args schedule : a dict, epoch number as keys and learning rate as values start_epoch : training start epoch number Methods epoch_update (learning_rate, training_history) Args learning_rate : previous epoch learning rate training_histoty : a dict with epoch, training loss, validation loss as keys Returns updated learning rate Polynomial learning rate decay policy tefla.core.lr_policy.PolyDecayPolicy (base_lr, power=10.0, max_epoch=500, n_iters_per_epoch=1094) the effective learning rate follows a polynomial decay, to be zero by the max_iter. return base_lr (1 - iter/max_iter) ^ (power) Args base_lr : a float, starting learning rate power : a float, decay factor max_epoch : a int, max training epoch n_iters_per_epoch : number of interations per epoch, e.g. total_training_samples/batch_size Methods batch_update (learning_rate, iter_idx) it follows a polynomial decay policy Args learning_rate : current batch learning rate iter_idx : iteration number, e.g. number_of_iterations_per_batch*epoch+current_batch_iteration_number Returns updated_lr Cosine learning rate decay policy tefla.core.lr_policy.CosineDecayPolicy (base_lr, cycle_steps=100000, **kwargs) the effective learning rate follows a polynomial decay, to be zero by the max_iter. return base_lr (1 - iter/max_iter) ^ (power) Args base_lr : a float, starting learning rate cycle_steps : a float, decay steps one cycle, learning rate decayed by inv(cycle_steps) Methods batch_update (learning_rate, iter_idx) it follows a polynomial decay policy Args learning_rate : current batch learning rate iter_idx : iteration number, e.g. number_of_iterations_per_batch*epoch+current_batch_iteration_number Returns updated_lr Cyclic learning rate decay policy tefla.core.lr_policy.CyclicDecayPolicy (base_lr, cycle_steps=100000, **kwargs) the effective learning rate follows a polynomial decay, to be zero by the max_iter. return base_lr (1 - iter/max_iter) ^ (power) Args base_lr : a float, starting learning rate cycle_steps : a float, decay steps one cycle, learning rate decayed by inv(cycle_steps) Methods batch_update (learning_rate, iter_idx) it follows a polynomial decay policy Args learning_rate : current batch learning rate iter_idx : iteration number, e.g. number_of_iterations_per_batch*epoch+current_batch_iteration_number Returns updated_lr","title":"Lr_Policy"},{"location":"core/lr_policy/#training-learning-rate-schedule-based-on-inputs-dict-with-epoch-number-as-keys","text":"tefla.core.lr_policy.StepDecayPolicy (schedule, start_epoch=1)","title":"Training learning rate schedule based on  inputs dict with epoch number as keys"},{"location":"core/lr_policy/#polynomial-learning-rate-decay-policy","text":"tefla.core.lr_policy.PolyDecayPolicy (base_lr, power=10.0, max_epoch=500, n_iters_per_epoch=1094) the effective learning rate follows a polynomial decay, to be zero by the max_iter. return base_lr (1 - iter/max_iter) ^ (power)","title":"Polynomial learning rate decay policy"},{"location":"core/lr_policy/#cosine-learning-rate-decay-policy","text":"tefla.core.lr_policy.CosineDecayPolicy (base_lr, cycle_steps=100000, **kwargs) the effective learning rate follows a polynomial decay, to be zero by the max_iter. return base_lr (1 - iter/max_iter) ^ (power)","title":"Cosine learning rate decay policy"},{"location":"core/lr_policy/#cyclic-learning-rate-decay-policy","text":"tefla.core.lr_policy.CyclicDecayPolicy (base_lr, cycle_steps=100000, **kwargs) the effective learning rate follows a polynomial decay, to be zero by the max_iter. return base_lr (1 - iter/max_iter) ^ (power)","title":"Cyclic learning rate decay policy"},{"location":"core/metrics/","text":"","title":"Metrics"},{"location":"core/prediction/","text":"base mixin class for prediction tefla.core.prediction.PredictSession (weights_from, gpu_memory_fraction=None) Args weights_from : path to the weights file gpu_memory_fraction : fraction of gpu memory to use, if not cpu prediction One crop Predictor, it predict network out put from a single crop of an input image tefla.core.prediction.OneCropPredictor (model, cnf, weights_from, prediction_iterator) Args model : model definition file cnf : prediction configs weights_from : location of the model weights file prediction_iterator : iterator to access and augment the data for prediction gpu_memory_fraction : fraction of gpu memory to use, if not cpu prediction Quasi transform predictor tefla.core.prediction.QuasiPredictor (model, cnf, weights_from, prediction_iterator, number_of_transforms) Args model : model definition file cnf : prediction configs weights_from : location of the model weights file prediction_iterator : iterator to access and augment the data for prediction number_of_transform : number of determinastic augmentaions to be performed on the input data resulted predictions are averaged over the augmentated transformation prediction outputs gpu_memory_fraction : fraction of gpu memory to use, if not cpu prediction Multiples non Data augmented crops predictor tefla.core.prediction.CropPredictor (model, cnf, weights_from, prediction_iterator, im_size, crop_size) Args model : model definition file cnf : prediction configs weights_from : location of the model weights file prediction_iterator : iterator to access and augment the data for prediction crop_size : crop size for network input im_size : original image size number_of_crops : total number of crops to extract from the input image gpu_memory_fraction : fraction of gpu memory to use, if not cpu prediction Returns predcitions from multiples models tefla.core.prediction.EnsemblePredictor (predictors) Ensembled predictions from multiples models using ensemble type Args predictors : predictor instances Methods predict (X, ensemble_type='mean') Args X : 4D tensor, inputs ensemble_type : operation to combine models probabilitiesavailable type: ['mean', 'gmean', 'log_mean']","title":"Predictor"},{"location":"core/prediction/#base-mixin-class-for-prediction","text":"tefla.core.prediction.PredictSession (weights_from, gpu_memory_fraction=None)","title":"base mixin class for prediction"},{"location":"core/prediction/#one-crop-predictor-it-predict-network-out-put-from-a-single-crop-of-an-input-image","text":"tefla.core.prediction.OneCropPredictor (model, cnf, weights_from, prediction_iterator)","title":"One crop Predictor, it predict network out put from a single crop of an input image"},{"location":"core/prediction/#quasi-transform-predictor","text":"tefla.core.prediction.QuasiPredictor (model, cnf, weights_from, prediction_iterator, number_of_transforms)","title":"Quasi transform predictor"},{"location":"core/prediction/#multiples-non-data-augmented-crops-predictor","text":"tefla.core.prediction.CropPredictor (model, cnf, weights_from, prediction_iterator, im_size, crop_size)","title":"Multiples non Data augmented crops predictor"},{"location":"core/prediction/#returns-predcitions-from-multiples-models","text":"tefla.core.prediction.EnsemblePredictor (predictors) Ensembled predictions from multiples models using ensemble type","title":"Returns predcitions from multiples models"},{"location":"core/rnn_cell/","text":"The most basic RNN cell tefla.core.rnn_cell.BasicRNNCell (num_units, reuse, trainable=True, w_init= , use_bias=False, input_size=None, activation= , layer_norm=None, layer_norm_args=None) Args num_units : int, The number of units in the LSTM cell. reuse : whether or not the layer and its variables should be reused. To be able to reuse the layer scope must be given. input_size : Deprecated and unused. activation : Activation function of the states. layer_norm : If True , layer normalization will be applied. layer_norm_args : optional dict, layer_norm arguments trainable : If True also add variables to the graph collection GraphKeys.TRAINABLE_VARIABLES (see tf.Variable). Methods add_variable (name, shape, dtype=None, initializer=None, regularizer=None, trainable=True) Arguments: name: variable name. shape: variable shape. dtype: The type of the variable. Defaults to self.dtype . initializer: initializer instance (callable). regularizer: regularizer instance (callable). trainable: whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean, stddev). Returns The created variable. apply (inputs, args, *kwargs) This simply wraps self.__call__ . Arguments: inputs: Input tensor(s). args: additional positional arguments to be passed to self.call . *kwargs: additional keyword arguments to be passed to self.call . Returns Output tensor(s). call (inputs, **kwargs) Arguments: inputs: input tensor(s). **kwargs: additional keyword arguments. Returns Output tensor(s). get_losses_for (inputs) Arguments: inputs: Input tensor or list/tuple of input tensors. Must match the inputs argument passed to the __call__ method at the time the losses were created. If you pass inputs=None , unconditional losses are returned, such as weight regularization losses. Returns List of loss tensors of the layer that depend on inputs . get_updates_for (inputs) Arguments: inputs: Input tensor or list/tuple of input tensors. Must match the inputs argument passed to the __call__ method at the time the updates were created. If you pass inputs=None , unconditional updates are returned. Returns List of update ops of the layer that depend on inputs . zero_state (batch_size, dtype) Args batch_size: int, float, or unit Tensor representing the batch size. dtype: the data type to use for the state. Returns If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. LSTM unit tefla.core.rnn_cell.LSTMCell (num_units, reuse, trainable=True, w_init= , forget_bias=1.0, use_bias=False, input_size=None, activation= , inner_activation= , keep_prob=1.0, dropout_seed=None, cell_clip=None, layer_norm=None, layer_norm_args=None) This class adds layer normalization and recurrent dropout to a basic LSTM unit. Layer normalization implementation is based on: https://arxiv.org/abs/1607.06450. \"Layer Normalization\" Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton and is applied before the internal nonlinearities. Recurrent dropout is base on: https://arxiv.org/abs/1603.05118 \"Recurrent Dropout without Memory Loss\" Stanislau Semeniuta, Aliaksei Severyn, Erhardt Barth. Args num_units : int, The number of units in the LSTM cell. reuse : whether or not the layer and its variables should be reused. To be able to reuse the layer scope must be given. forget_bias : float, The bias added to forget gates (see above). input_size : Deprecated and unused. activation : Activation function of the states. inner_activation : Activation function of the inner states. layer_norm : If True , layer normalization will be applied. layer_norm_args : optional dict, layer_norm arguments cell_clip : (optional) A float value, if provided the cell state is clipped by this value prior to the cell output activation. keep_prob : unit Tensor or float between 0 and 1 representing the recurrent dropout probability value. If float and 1.0, no dropout will be applied. dropout_seed : (optional) integer, the randomness seed. trainable : If True also add variables to the graph collection GraphKeys.TRAINABLE_VARIABLES (see tf.Variable). Basic attention cell tefla.core.rnn_cell.AttentionCell (cell, attn_length, reuse, w_init= , use_bias=False, trainable=True, attn_size=None, attn_vec_size=None, input_size=None, layer_norm=None, layer_norm_args=None) Implementation based on https://arxiv.org/abs/1409.0473. Create a cell with attention. Args cell : an RNNCell, an attention is added to it. e.g.: a LSTMCell attn_length : integer, the size of an attention window. reuse : whether or not the layer and its variables should be reused. To be able to reuse the layer scope must be given. attn_size : integer, the size of an attention vector. Equal to cell.output_size by default. attn_vec_size : integer, the number of convolutional features calculated on attention state and a size of the hidden layer built from base cell state. Equal attn_size to by default. input_size : integer, the size of a hidden linear layer, layer_norm : If True , layer normalization will be applied. layer_norm_args : optional dict, layer_norm arguments built from inputs and attention. Derived from the input tensor by default. trainable : If True also add variables to the graph collection GraphKeys.TRAINABLE_VARIABLES (see tf.Variable). Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078) tefla.core.rnn_cell.GRUCell (num_units, reuse, w_init= , use_bias=False, trainable=True, input_size=None, activation= , inner_activation= , b_init=1.0, keep_prob=1.0, layer_norm=None, layer_norm_args=None) Args num_units : int, The number of units in the LSTM cell. reuse : whether or not the layer and its variables should be reused. To be able to reuse the layer scope must be given. input_size : Deprecated and unused. activation : Activation function of the states. inner_activation : Activation function of the inner states. layer_norm : If True , layer normalization will be applied. layer_norm_args : optional dict, layer_norm arguments trainable : If True also add variables to the graph collection GraphKeys.TRAINABLE_VARIABLES (see tf.Variable). RNN cell composed sequentially of multiple simple cells tefla.core.rnn_cell.MultiRNNCell (cells, state_is_tuple=True) Create a RNN cell composed sequentially of a number of RNNCells. Args cells : list of RNNCells that will be composed in this order. Operator adding dropout to inputs and outputs of the given cell tefla.core.rnn_cell.DropoutWrapper (cell, is_training, input_keep_prob=1.0, output_keep_prob=1.0, seed=None) Create a cell with added input and/or output dropout. Dropout is never used on the state. Args cell : an RNNCell, a projection to output_size is added to it. is_training : a bool, training if true else validation/testing input_keep_prob : unit Tensor or float between 0 and 1, input keep probability; if it is float and 1, no input dropout will be added. output_keep_prob : unit Tensor or float between 0 and 1, output keep probability; if it is float and 1, no output dropout will be added. seed : (optional) integer, the randomness seed. convolution: tefla.core.rnn_cell._conv (args, filter_size, num_features, bias, reuse, w_init=None, b_init=0.0, scope='_conv') Args args : a Tensor or a list of Tensors of dimension 3D, 4D or 5D batch x n, Tensors. filter_size : int tuple of filter height and width. reuse : None/True, whether to reuse variables w_init : weights initializer object b_init : a int , bias initializer value num_features : int, number of features. bias_start : starting value to initialize the bias; 0 by default. Returns A 3D, 4D, or 5D Tensor with shape [batch ... num_features] Adds a fully connected layer tefla.core.rnn_cell._linear (x, n_output, reuse, trainable=True, w_init= , b_init=0.0, w_regularizer= , name='fc', layer_norm=None, layer_norm_args=None, activation=None, outputs_collections=None, use_bias=True) fully_connected creates a variable called weights , representing a fully connected weight matrix, which is multiplied by the x to produce a Tensor of hidden units. If a layer_norm is provided (such as layer_norm ), it is then applied. Otherwise, if layer_norm is None and a b_init and use_bias is provided then a biases variable would be created and added the hidden units. Finally, if activation is not None , it is applied to the hidden units as well. Note: that if x have a rank greater than 2, then x is flattened prior to the initial matrix multiply by weights . Args x : A Tensor of with at least rank 2 and value for the last dimension, i.e. [batch_size, depth] , [None, None, None, channels] . n_output : Integer or long, the number of output units in the layer. reuse : whether or not the layer and its variables should be reused. To be able to reuse the layer scope must be given. activation : activation function, set to None to skip it and maintain a linear activation. layer_norm : normalization function to use. If - batch_norm is True then google original implementation is used and if another function is provided then it is applied. default set to None for no normalizer function layer_norm_args : normalization function parameters. w_init : An initializer for the weights. w_regularizer : Optional regularizer for the weights. b_init : An initializer for the biases. If None skip biases. outputs_collections : The collections to which the outputs are added. trainable : If True also add variables to the graph collection GraphKeys.TRAINABLE_VARIABLES (see tf.Variable). name : Optional name or scope for variable_scope/name_scope. use_bias : Whether to add bias or not Returns The 2-D Tensor variable representing the result of the series of operations. e.g: 2-D Tensor [batch, n_output]. Adds a Layer Normalization layer from https://arxiv.org/abs/1607.06450 tefla.core.rnn_cell.layer_norm (x, reuse, center=True, scale=True, trainable=True, epsilon=1e-12, name='bn', outputs_collections=None) \"Layer Normalization\" Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton Can be used as a normalizer function for conv2d and fully_connected. Args x : a tensor with 2 or more dimensions, where the first dimension has batch_size . The normalization is over all but the last dimension if data_format is NHWC and the second dimension if data_format is NCHW . center : If True, subtract beta . If False, beta is ignored. scale : If True, multiply by gamma . If False, gamma is not used. When the next layer is linear (also e.g. nn.relu ), this can be disabled since the scaling can be done by the next layer. epsilon : small float added to variance to avoid dividing by zero. reuse : whether or not the layer and its variables should be reused. To be able to reuse the layer scope must be given. outputs_collections : collections to add the outputs. trainable : If True also add variables to the graph collection GraphKeys.TRAINABLE_VARIABLES (see tf.Variable ). name : Optional scope/name for variable_scope . Returns A Tensor representing the output of the operation. LSTM tefla.core.rnn_cell.lstm (inputs, n_units, reuse, is_training, activation= , inner_activation= , dropout=None, use_bias=True, w_init= , forget_bias=1.0, return_seq=False, return_state=False, initial_state=None, dynamic=True, trainable=True, scope='lstm') Long Short Term Memory Recurrent Layer. Args inputs : Tensor . Inputs 3-D Tensor [samples, timesteps, input_dim]. n_units : int , number of units for this layer. reuse : bool . If True and 'scope' is provided, this layer variables will be reused (shared). is_training : bool , training if True activation : function (returning a Tensor ). inner_activation : function (returning a Tensor ). dropout : tuple of float : (input_keep_prob, output_keep_prob). The input and output keep probability. use_bias : bool . If True, a bias is used. w_init : function (returning a Tensor ). Weights initialization. forget_bias : float . Bias of the forget gate. Default: 1.0. return_seq : bool . If True, returns the full sequence instead of last sequence output only. return_state : bool . If True, returns a tuple with output and states: (output, states). initial_state : Tensor . An initial state for the RNN. This must be a tensor of appropriate type and shape [batch_size x cell.state_size]. dynamic : bool . If True, dynamic computation is performed. It will not compute RNN steps above the sequence length. Note that because TF requires to feed sequences of same length, 0 is used as a mask. So a sequence padded with 0 at the end must be provided. When computation is performed, it will stop when it meets a step with a value of 0. trainable : bool . If True, weights will be trainable. scope : str . Define this layer scope (optional). A scope can be used to share variables between layers. Note that scope will override name. Returns if return_seq : 3-D Tensor [samples, timesteps, output dim]. else: 2-D Tensor [samples, output dim]. GRU tefla.core.rnn_cell.gru (inputs, n_units, reuse, is_training, activation= , inner_activation= , dropout=None, use_bias=True, w_init= , forget_bias=1.0, return_seq=False, return_state=False, initial_state=None, dynamic=True, trainable=True, scope='gru') Gated Recurrent Layer. Args inputs : Tensor . Inputs 3-D Tensor [samples, timesteps, input_dim]. n_units : int , number of units for this layer. reuse : bool . If True and 'scope' is provided, this layer variables will be reused (shared). is_training : bool , training if True activation : function (returning a Tensor ). inner_activation : function (returning a Tensor ). dropout : tuple of float : (input_keep_prob, output_keep_prob). The input and output keep probability. use_bias : bool . If True, a bias is used. w_init : function (returning a Tensor ). Weights initialization. forget_bias : float . Bias of the forget gate. Default: 1.0. return_seq : bool . If True, returns the full sequence instead of last sequence output only. return_state : bool . If True, returns a tuple with output and states: (output, states). initial_state : Tensor . An initial state for the RNN. This must be a tensor of appropriate type and shape [batch_size x cell.state_size]. dynamic : bool . If True, dynamic computation is performed. It will not compute RNN steps above the sequence length. Note that because TF requires to feed sequences of same length, 0 is used as a mask. So a sequence padded with 0 at the end must be provided. When computation is performed, it will stop when it meets a step with a value of 0. trainable : bool . If True, weights will be trainable. scope : str . Define this layer scope (optional). A scope can be used to share variables between layers. Note that scope will override name. Returns if return_seq : 3-D Tensor [samples, timesteps, output dim]. else: 2-D Tensor [samples, output dim]. Bidirectional RNN tefla.core.rnn_cell.bidirectional_rnn (inputs, rnncell_fw, rnncell_bw, reuse, is_training, dropout_fw=None, dropout_bw=None, return_seq=False, return_states=False, initial_state_fw=None, initial_state_bw=None, dynamic=False, scope='BiRNN', outputs_collections=None) Build a bidirectional recurrent neural network, it requires 2 RNN Cells to process sequence in forward and backward order. Any RNN Cell can be used i.e. SimpleRNN, LSTM, GRU... with its own parameters. But the two cells number of units must match. Args inputs : Tensor . The 3D inputs Tensor [samples, timesteps, input_dim]. rnncell_fw : RNNCell . The RNN Cell to use for foward computation. rnncell_bw : RNNCell . The RNN Cell to use for backward computation. reuse : bool . If True and 'scope' is provided, this layer variables will be reused (shared). is_training : bool , training if True dropout_fw : tuple of float : (input_keep_prob, output_keep_prob). the input and output keep probability. dropout_bw : tuple of float : (input_keep_prob, output_keep_prob). the input and output keep probability. return_seq : bool . If True, returns the full sequence instead of last sequence output only. return_states : bool . If True, returns a tuple with output and states: (output, states). initial_state_fw : Tensor . An initial state for the forward RNN. This must be a tensor of appropriate type and shape [batch_size x cell.state_size]. initial_state_bw : Tensor . An initial state for the backward RNN. This must be a tensor of appropriate type and shape [batch_size x cell.state_size]. dynamic : bool . If True, dynamic computation is performed. It will not compute RNN steps above the sequence length. Note that because TF requires to feed sequences of same length, 0 is used as a mask. So a sequence padded with 0 at the end must be provided. When computation is performed, it will stop when it meets a step with a value of 0. scope : str . Define this layer scope (optional). A scope can be used to share variables between layers. Note that scope will override name. Returns if return_seq : 3-D Tensor [samples, timesteps, output dim]. else: 2-D Tensor Layer [samples, output dim].","title":"RNN"},{"location":"core/rnn_cell/#the-most-basic-rnn-cell","text":"tefla.core.rnn_cell.BasicRNNCell (num_units, reuse, trainable=True, w_init= , use_bias=False, input_size=None, activation= , layer_norm=None, layer_norm_args=None)","title":"The most basic RNN cell"},{"location":"core/rnn_cell/#lstm-unit","text":"tefla.core.rnn_cell.LSTMCell (num_units, reuse, trainable=True, w_init= , forget_bias=1.0, use_bias=False, input_size=None, activation= , inner_activation= , keep_prob=1.0, dropout_seed=None, cell_clip=None, layer_norm=None, layer_norm_args=None) This class adds layer normalization and recurrent dropout to a basic LSTM unit. Layer normalization implementation is based on: https://arxiv.org/abs/1607.06450. \"Layer Normalization\" Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton and is applied before the internal nonlinearities. Recurrent dropout is base on: https://arxiv.org/abs/1603.05118 \"Recurrent Dropout without Memory Loss\" Stanislau Semeniuta, Aliaksei Severyn, Erhardt Barth.","title":"LSTM unit"},{"location":"core/rnn_cell/#basic-attention-cell","text":"tefla.core.rnn_cell.AttentionCell (cell, attn_length, reuse, w_init= , use_bias=False, trainable=True, attn_size=None, attn_vec_size=None, input_size=None, layer_norm=None, layer_norm_args=None) Implementation based on https://arxiv.org/abs/1409.0473. Create a cell with attention.","title":"Basic attention cell"},{"location":"core/rnn_cell/#gated-recurrent-unit-cell-cf-httparxivorgabs14061078","text":"tefla.core.rnn_cell.GRUCell (num_units, reuse, w_init= , use_bias=False, trainable=True, input_size=None, activation= , inner_activation= , b_init=1.0, keep_prob=1.0, layer_norm=None, layer_norm_args=None)","title":"Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078)"},{"location":"core/rnn_cell/#rnn-cell-composed-sequentially-of-multiple-simple-cells","text":"tefla.core.rnn_cell.MultiRNNCell (cells, state_is_tuple=True) Create a RNN cell composed sequentially of a number of RNNCells.","title":"RNN cell composed sequentially of multiple simple cells"},{"location":"core/rnn_cell/#operator-adding-dropout-to-inputs-and-outputs-of-the-given-cell","text":"tefla.core.rnn_cell.DropoutWrapper (cell, is_training, input_keep_prob=1.0, output_keep_prob=1.0, seed=None) Create a cell with added input and/or output dropout. Dropout is never used on the state.","title":"Operator adding dropout to inputs and outputs of the given cell"},{"location":"core/rnn_cell/#convolution","text":"tefla.core.rnn_cell._conv (args, filter_size, num_features, bias, reuse, w_init=None, b_init=0.0, scope='_conv')","title":"convolution:"},{"location":"core/rnn_cell/#adds-a-fully-connected-layer","text":"tefla.core.rnn_cell._linear (x, n_output, reuse, trainable=True, w_init= , b_init=0.0, w_regularizer= , name='fc', layer_norm=None, layer_norm_args=None, activation=None, outputs_collections=None, use_bias=True) fully_connected creates a variable called weights , representing a fully connected weight matrix, which is multiplied by the x to produce a Tensor of hidden units. If a layer_norm is provided (such as layer_norm ), it is then applied. Otherwise, if layer_norm is None and a b_init and use_bias is provided then a biases variable would be created and added the hidden units. Finally, if activation is not None , it is applied to the hidden units as well. Note: that if x have a rank greater than 2, then x is flattened prior to the initial matrix multiply by weights .","title":"Adds a fully connected layer"},{"location":"core/rnn_cell/#adds-a-layer-normalization-layer-from-httpsarxivorgabs160706450","text":"tefla.core.rnn_cell.layer_norm (x, reuse, center=True, scale=True, trainable=True, epsilon=1e-12, name='bn', outputs_collections=None) \"Layer Normalization\" Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton Can be used as a normalizer function for conv2d and fully_connected.","title":"Adds a Layer Normalization layer from https://arxiv.org/abs/1607.06450"},{"location":"core/rnn_cell/#lstm","text":"tefla.core.rnn_cell.lstm (inputs, n_units, reuse, is_training, activation= , inner_activation= , dropout=None, use_bias=True, w_init= , forget_bias=1.0, return_seq=False, return_state=False, initial_state=None, dynamic=True, trainable=True, scope='lstm') Long Short Term Memory Recurrent Layer.","title":"LSTM"},{"location":"core/rnn_cell/#gru","text":"tefla.core.rnn_cell.gru (inputs, n_units, reuse, is_training, activation= , inner_activation= , dropout=None, use_bias=True, w_init= , forget_bias=1.0, return_seq=False, return_state=False, initial_state=None, dynamic=True, trainable=True, scope='gru') Gated Recurrent Layer.","title":"GRU"},{"location":"core/rnn_cell/#bidirectional-rnn","text":"tefla.core.rnn_cell.bidirectional_rnn (inputs, rnncell_fw, rnncell_bw, reuse, is_training, dropout_fw=None, dropout_bw=None, return_seq=False, return_states=False, initial_state_fw=None, initial_state_bw=None, dynamic=False, scope='BiRNN', outputs_collections=None) Build a bidirectional recurrent neural network, it requires 2 RNN Cells to process sequence in forward and backward order. Any RNN Cell can be used i.e. SimpleRNN, LSTM, GRU... with its own parameters. But the two cells number of units must match.","title":"Bidirectional RNN"},{"location":"core/special_layers/","text":"Spatial Transformer Layer tefla.core.special_layers.spatialtransformer (U, theta, batch_size=64, downsample_factor=1.0, num_transform=1, name='SpatialTransformer', **kwargs) Implements a spatial transformer layer as described in [1] . It's based on lasagne implementation in [2] , modified by Mrinal Haloi Args U : float The output of a convolutional net should have the shape [batch_size, height, width, num_channels]. theta : float The output of the localisation network should be [batch_size, num_transform, 6] or [batch_size, 6] if num_transform=1 python`theta`` to : - identity = np.array([[1., 0., 0.], - [0., 1., 0.]]) - identity = identity.flatten() - theta = tf.Variable(initial_value=identity) downsample_factor : a float, determines output shape, downsample input shape by downsample_factor Returns spatial transformed output of the network Subsamples the input along the spatial dimensions tefla.core.special_layers.subsample (inputs, factor, name=None) Args inputs : A Tensor of size [batch, height_in, width_in, channels]. factor : The subsampling factor. name : Optional variable_scope. Returns output: A Tensor of size [batch, height_out, width_out, channels] with the input, either intact (if factor == 1) or subsampled (if factor > 1). Strided 2-D convolution with 'SAME' padding tefla.core.special_layers.conv2d_same (inputs, num_outputs, kernel_size, stride, rate=1, name=None, **kwargs) When stride > 1, then we do explicit zero-padding, followed by conv2d with 'VALID' padding. Note that net = conv2d_same(inputs, num_outputs, 3, stride=stride) is equivalent to net = conv2d(inputs, num_outputs, 3, stride=1, padding='SAME') net = subsample(net, factor=stride) whereas net = conv2d(inputs, num_outputs, 3, stride=stride, padding='SAME') is different when the input's height or width is even, which is why we add the current function. For more details, see ResnetUtilsTest.testConv2DSameEven(). Args inputs : A 4-D tensor of size [batch, height_in, width_in, channels]. num_outputs : An integer, the number of output filters. kernel_size : An int with the kernel_size of the filters. stride : An integer, the output stride. rate : An integer, rate for atrous convolution. name : name. Returns output: A 4-D tensor of size [batch, height_out, width_out, channels] with the convolution output. Bottleneck residual unit variant with BN before convolutions tefla.core.special_layers.bottleneck_v1 (inputs, depth, depth_bottleneck, stride, rate=1, name=None, **kwargs) This is the full preactivation residual unit variant proposed in [2]. See Fig. 1(b) of [2] for its definition. Note that we use here the bottleneck variant which has an extra bottleneck layer. When putting together two consecutive ResNet blocks that use this unit, one should use stride = 2 in the last unit of the first block. Args inputs : A tensor of size [batch, height, width, channels]. depth : The depth of the ResNet unit output. depth_bottleneck : The depth of the bottleneck layers. stride : The ResNet unit's stride. Determines the amount of downsampling of the units output compared to its input. rate : An integer, rate for atrous convolution. outputs_collections : Collection to add the ResNet unit output. name : Optional variable_scope. Returns The ResNet unit's output. Bottleneck residual unit variant with BN before convolutions tefla.core.special_layers.bottleneck_v2 (inputs, depth, depth_bottleneck, stride, rate=1, name=None, **kwargs) This is the full preactivation residual unit variant proposed in [2]. See Fig. 1(b) of [2] for its definition. Note that we use here the bottleneck variant which has an extra bottleneck layer. When putting together two consecutive ResNet blocks that use this unit, one should use stride = 2 in the last unit of the first block. Args inputs : A tensor of size [batch, height, width, channels]. depth : The depth of the ResNet unit output. depth_bottleneck : The depth of the bottleneck layers. stride : The ResNet unit's stride. Determines the amount of downsampling of the units output compared to its input. rate : An integer, rate for atrous convolution. outputs_collections : Collection to add the ResNet unit output. name : Optional variable_scope. Returns The ResNet unit's output. DenseCRF over unnormalised predictions tefla.core.special_layers.dense_crf (probs, img=None, n_classes=21, n_iters=10, sxy_gaussian= (1, 1), compat_gaussian=4, kernel_gaussian= , normalisation_gaussian= , sxy_bilateral= (49, 49), compat_bilateral=2, srgb_bilateral= (13, 13, 13), kernel_bilateral= , normalisation_bilateral= ) More details on the arguments at https://github.com/lucasb-eyer/pydensecrf. Args probs : class probabilities per pixel. img : if given, the pairwise bilateral potential on raw RGB values will be computed. n_iters : number of iterations of MAP inference. sxy_gaussian : standard deviations for the location component of the colour-independent term. compat_gaussian : label compatibilities for the colour-independent term (can be a number, a 1D array, or a 2D array). kernel_gaussian : kernel precision matrix for the colour-independent term (can take values CONST_KERNEL, DIAG_KERNEL, or FULL_KERNEL). normalisation_gaussian : normalisation for the colour-independent term (possible values are NO_NORMALIZATION, NORMALIZE_BEFORE, NORMALIZE_AFTER, NORMALIZE_SYMMETRIC). sxy_bilateral : standard deviations for the location component of the colour-dependent term. compat_bilateral : label compatibilities for the colour-dependent term (can be a number, a 1D array, or a 2D array). srgb_bilateral : standard deviations for the colour component of the colour-dependent term. kernel_bilateral : kernel precision matrix for the colour-dependent term (can take values CONST_KERNEL, DIAG_KERNEL, or FULL_KERNEL). normalisation_bilateral : normalisation for the colour-dependent term (possible values are NO_NORMALIZATION, NORMALIZE_BEFORE, NORMALIZE_AFTER, NORMALIZE_SYMMETRIC). Returns Refined predictions after MAP inference. ResNeXt Block tefla.core.special_layers.resnext_block (inputs, nb_blocks, out_channels, is_training, reuse, cardinality, downsample=False, downsample_strides=2, activation= , batch_norm=None, batch_norm_args=None, name='ResNeXtBlock', **kwargs) resnext paper https://arxiv.org/pdf/1611.05431.pdf Args inputs : Tensor . Inputs 4-D Layer. nb_blocks : int . Number of layer blocks. out_channels : int . The number of convolutional filters of the layers surrounding the bottleneck layer. cardinality : int . Number of aggregated residual transformations. downsample : bool . If True, apply downsampling using 'downsample_strides' for strides. downsample_strides : int . The strides to use when downsampling. activation : function (returning a Tensor ). batch_norm : bool . If True, apply batch normalization. use_ bias: bool . If True, a bias is used. w_init : function , Weights initialization. b_init : tf.Tensor . Bias initialization. w_regularizer : function . Add a regularizer to this weight_decay : float . Regularizer decay parameter. Default: 0.001. trainable : bool . If True, weights will be trainable. reuse : bool . If True and 'scope' is provided, this layer variables will be reused (shared). override name. name : A name for this layer (optional). Default: 'ResNeXtBlock'. Returns 4-D Tensor [batch, new height, new width, out_channels]. Embedding tefla.core.special_layers.embedding (inputs, vocab_dim, embedding_dim, reuse, validate_indices=False, w_init= , trainable=True, normalize=False, vocab_freqs=None, name='Embedding') Embedding layer for a sequence of integer ids or floats. Args inputs : a 2-D Tensor [samples, ids]. vocab_dim : list of int . Vocabulary size (number of ids). embedding_dim : list of int . Embedding size. validate_indices : bool . Whether or not to validate gather indices. w_init : Weights initialization. trainable : bool . If True, weights will be trainable. reuse : bool . If True and 'scope' is provided, this layer variables will be reused (shared). name : A name for this layer (optional). Default: 'Embedding'. Returns 3-D Tensor [samples, embedded_ids, features]. Gated unit for language modelling tefla.core.special_layers.gated_layer (inputs, layer, num_units, is_training, reuse, name='gated_layer', **kwargs) Args inputs : a 3-D/4-D Tensor , input [samples, timesteps, input_dim] layer : a layer , layer to pass the inputs e.g. tefla.core.layers num_units : a int , number of units for each layer is_training : a boolean , Training if its true reuse : bool . If True and 'scope' is provided, this layer variables will be reused (shared). name : A name for this layer (optional). Default: 'gated_layer'. Returns a 3-D/4-D Tensor , output of the gated unit Returns glimpses at the locations tefla.core.special_layers.glimpseSensor (img, normLoc, minRadius=4, depth=1, sensorBandwidth=12) Args img : a 4-D Tensor , [batch_size, width, height, channels] normloc : a float , [0, 1] normalized location minRadius : a int , min radius for zooming depth : a int , number of zooms sensorbandwidth : a int , output glimpse size, width/height Returns a 5-D tensor of glimpses Adds a PVA block layer tefla.core.special_layers.pva_block_v1 (x, num_units, name='pva_block_v1', **kwargs) convolution followed by crelu and scaling Args x : A 4-D Tensor of with at least rank 2 and value for the last dimension, i.e. [batch_size, in_height, in_width, depth] , is_training : Bool, training or testing num_units : Integer or long, the number of output units in the layer. reuse : whether or not the layer and its variables should be reused. To be able to reuse the layer scope must be given. filter_size : a int or list/tuple of 2 positive integers specifying the spatial dimensions of of the filters. stride : a int or tuple/list of 2 positive integers specifying the stride at which to compute output. padding : one of \"VALID\" or \"SAME\" . activation : activation function, set to None to skip it and maintain a linear activation. batch_norm : normalization function to use. If batch_norm is True then google original implementation is used and if another function is provided then it is applied. default set to None for no normalizer function batch_norm_args : normalization function parameters. w_init : An initializer for the weights. w_regularizer : Optional regularizer for the weights. untie_biases : spatial dimensions wise baises b_init : An initializer for the biases. If None skip biases. trainable : If True also add variables to the graph collection GraphKeys.TRAINABLE_VARIABLES (see tf.Variable). name : Optional name or scope for variable_scope/name_scope. use_bias : Whether to add bias or not Returns The 4-D Tensor variable representing the result of the series of operations. e.g.: 4-D Tensor [batch, new_height, new_width, n_output]. Adds a PVA block v2 layer tefla.core.special_layers.pva_block_v2 (x, num_units, name='pva_block_v2', **kwargs) first batch normalization followed by crelu and scaling, convolution is applied after scalling Args x : A 4-D Tensor of with at least rank 2 and value for the last dimension, i.e. [batch_size, in_height, in_width, depth] , is_training : Bool, training or testing num_units : Integer or long, the number of output units in the layer. reuse : whether or not the layer and its variables should be reused. To be able to reuse the layer scope must be given. filter_size : a int or list/tuple of 2 positive integers specifying the spatial dimensions of of the filters. stride : a int or tuple/list of 2 positive integers specifying the stride at which to compute output. padding : one of \"VALID\" or \"SAME\" . activation : activation function, set to None to skip it and maintain a linear activation. batch_norm : normalization function to use. If batch_norm is True then google original implementation is used and if another function is provided then it is applied. default set to None for no normalizer function batch_norm_args : normalization function parameters. w_init : An initializer for the weights. w_regularizer : Optional regularizer for the weights. untie_biases : spatial dimensions wise baises b_init : An initializer for the biases. If None skip biases. trainable : If True also add variables to the graph collection GraphKeys.TRAINABLE_VARIABLES (see tf.Variable). name : Optional name or scope for variable_scope/name_scope. use_bias : Whether to add bias or not Returns The 4-D Tensor variable representing the result of the series of operations. e.g.: 4-D Tensor [batch, new_height, new_width, n_output]. Performs a pooling operation that results in a fixed size: tefla.core.special_layers.max_pool_2d_nxn_regions (inputs, output_size, mode='max') output_size x output_size. Used by spatial_pyramid_pool. Refer to appendix A in [1]. Args inputs : A 4D Tensor (B, H, W, C) output_size : The output size of the pooling operation. mode : The pooling mode {max, avg} Returns A list of tensors, for each output bin. The list contains output_size * output_size elements, where each elment is a Tensor (N, C). Performs spatial pyramid pooling (SPP) over the input tefla.core.special_layers.spatial_pyramid_pool (inputs, dimensions=[2, 1], mode='max', implementation='kaiming') It will turn a 2D input of arbitrary size into an output of fixed dimenson. Hence, the convlutional part of a DNN can be connected to a dense part with a fixed number of nodes even if the dimensions of the input image are unknown. The pooling is performed over :math: l pooling levels. Each pooling level :math: i will create :math: M_i output features. :math: M_i is given by :math: n_i * n_i , with :math: n_i as the number of pooling operations per dimension level :math: i . The length of the parameter dimensions is the level of the spatial pyramid. Args inputs : A 4D Tensor (B, H, W, C). dimensions : The list of :math: n_i 's that define the output dimension of each pooling level :math: i . The length of dimensions is the level of the spatial pyramid. mode : Pooling mode 'max' or 'avg'. implementation : The implementation to use, either 'kaiming' or 'fast'. kamming is the original implementation from the paper, and supports variable sizes of input vectors, which fast does not support. Returns A fixed length vector representing the inputs.","title":"Special Layers"},{"location":"core/special_layers/#spatial-transformer-layer","text":"tefla.core.special_layers.spatialtransformer (U, theta, batch_size=64, downsample_factor=1.0, num_transform=1, name='SpatialTransformer', **kwargs) Implements a spatial transformer layer as described in [1] . It's based on lasagne implementation in [2] , modified by Mrinal Haloi","title":"Spatial Transformer Layer"},{"location":"core/special_layers/#subsamples-the-input-along-the-spatial-dimensions","text":"tefla.core.special_layers.subsample (inputs, factor, name=None)","title":"Subsamples the input along the spatial dimensions"},{"location":"core/special_layers/#strided-2-d-convolution-with-same-padding","text":"tefla.core.special_layers.conv2d_same (inputs, num_outputs, kernel_size, stride, rate=1, name=None, **kwargs) When stride > 1, then we do explicit zero-padding, followed by conv2d with 'VALID' padding. Note that net = conv2d_same(inputs, num_outputs, 3, stride=stride) is equivalent to net = conv2d(inputs, num_outputs, 3, stride=1, padding='SAME') net = subsample(net, factor=stride) whereas net = conv2d(inputs, num_outputs, 3, stride=stride, padding='SAME') is different when the input's height or width is even, which is why we add the current function. For more details, see ResnetUtilsTest.testConv2DSameEven().","title":"Strided 2-D convolution with 'SAME' padding"},{"location":"core/special_layers/#bottleneck-residual-unit-variant-with-bn-before-convolutions","text":"tefla.core.special_layers.bottleneck_v1 (inputs, depth, depth_bottleneck, stride, rate=1, name=None, **kwargs) This is the full preactivation residual unit variant proposed in [2]. See Fig. 1(b) of [2] for its definition. Note that we use here the bottleneck variant which has an extra bottleneck layer. When putting together two consecutive ResNet blocks that use this unit, one should use stride = 2 in the last unit of the first block.","title":"Bottleneck residual unit variant with BN before convolutions"},{"location":"core/special_layers/#bottleneck-residual-unit-variant-with-bn-before-convolutions_1","text":"tefla.core.special_layers.bottleneck_v2 (inputs, depth, depth_bottleneck, stride, rate=1, name=None, **kwargs) This is the full preactivation residual unit variant proposed in [2]. See Fig. 1(b) of [2] for its definition. Note that we use here the bottleneck variant which has an extra bottleneck layer. When putting together two consecutive ResNet blocks that use this unit, one should use stride = 2 in the last unit of the first block.","title":"Bottleneck residual unit variant with BN before convolutions"},{"location":"core/special_layers/#densecrf-over-unnormalised-predictions","text":"tefla.core.special_layers.dense_crf (probs, img=None, n_classes=21, n_iters=10, sxy_gaussian= (1, 1), compat_gaussian=4, kernel_gaussian= , normalisation_gaussian= , sxy_bilateral= (49, 49), compat_bilateral=2, srgb_bilateral= (13, 13, 13), kernel_bilateral= , normalisation_bilateral= ) More details on the arguments at https://github.com/lucasb-eyer/pydensecrf.","title":"DenseCRF over unnormalised predictions"},{"location":"core/special_layers/#resnext-block","text":"tefla.core.special_layers.resnext_block (inputs, nb_blocks, out_channels, is_training, reuse, cardinality, downsample=False, downsample_strides=2, activation= , batch_norm=None, batch_norm_args=None, name='ResNeXtBlock', **kwargs) resnext paper https://arxiv.org/pdf/1611.05431.pdf","title":"ResNeXt Block"},{"location":"core/special_layers/#embedding","text":"tefla.core.special_layers.embedding (inputs, vocab_dim, embedding_dim, reuse, validate_indices=False, w_init= , trainable=True, normalize=False, vocab_freqs=None, name='Embedding') Embedding layer for a sequence of integer ids or floats.","title":"Embedding"},{"location":"core/special_layers/#gated-unit-for-language-modelling","text":"tefla.core.special_layers.gated_layer (inputs, layer, num_units, is_training, reuse, name='gated_layer', **kwargs)","title":"Gated unit for language modelling"},{"location":"core/special_layers/#returns-glimpses-at-the-locations","text":"tefla.core.special_layers.glimpseSensor (img, normLoc, minRadius=4, depth=1, sensorBandwidth=12)","title":"Returns glimpses at the locations"},{"location":"core/special_layers/#adds-a-pva-block-layer","text":"tefla.core.special_layers.pva_block_v1 (x, num_units, name='pva_block_v1', **kwargs) convolution followed by crelu and scaling","title":"Adds a PVA block layer"},{"location":"core/special_layers/#adds-a-pva-block-v2-layer","text":"tefla.core.special_layers.pva_block_v2 (x, num_units, name='pva_block_v2', **kwargs) first batch normalization followed by crelu and scaling, convolution is applied after scalling","title":"Adds a PVA block v2 layer"},{"location":"core/special_layers/#performs-a-pooling-operation-that-results-in-a-fixed-size","text":"tefla.core.special_layers.max_pool_2d_nxn_regions (inputs, output_size, mode='max') output_size x output_size. Used by spatial_pyramid_pool. Refer to appendix A in [1].","title":"Performs a pooling operation that results in a fixed size:"},{"location":"core/special_layers/#performs-spatial-pyramid-pooling-spp-over-the-input","text":"tefla.core.special_layers.spatial_pyramid_pool (inputs, dimensions=[2, 1], mode='max', implementation='kaiming') It will turn a 2D input of arbitrary size into an output of fixed dimenson. Hence, the convlutional part of a DNN can be connected to a dense part with a fixed number of nodes even if the dimensions of the input image are unknown. The pooling is performed over :math: l pooling levels. Each pooling level :math: i will create :math: M_i output features. :math: M_i is given by :math: n_i * n_i , with :math: n_i as the number of pooling operations per dimension level :math: i . The length of the parameter dimensions is the level of the spatial pyramid.","title":"Performs spatial pyramid pooling (SPP) over the input"},{"location":"core/summary/","text":"Add summary to a tensor, scalar summary if the tensor is 1D, else scalar and histogram summary tefla.core.summary.summary_metric (tensor, name=None, collections=None) Args tensor : a tensor to add summary name : name of the tensor collections : training or validation collections Add summary to a tensor, scalar summary if the tensor is 1D, else scalar and histogram summary tefla.core.summary.summary_activation (tensor, name=None, collections=None) Args tensor : a tensor to add summary name : name of the tensor collections : training or validation collections creates the summar writter for training and validation tefla.core.summary.create_summary_writer (summary_dir, sess) Args summary_dir : the directory to write summary sess : the session to sun the ops Returns training and vaidation summary writter Add summary as per the ops mentioned tefla.core.summary.summary_param (op, tensor, ndims, name, collections=None) Args op : name of the summary op; e.g. 'stddev' available ops: ['scalar', 'histogram', 'sparsity', 'mean', 'rms', 'stddev', 'norm', 'max', 'min'] tensor : the tensor to add summary ndims : dimension of the tensor name : name of the op collections : training or validation collections Add summary to all trainable tensors tefla.core.summary.summary_trainable_params (summary_types, collections=None) Args summary_type : a list of all sumary types to add e.g.: ['scalar', 'histogram', 'sparsity', 'mean', 'rms', 'stddev', 'norm', 'max', 'min'] collections : training or validation collections Add summary to all gradient tensors tefla.core.summary.summary_gradients (grad_vars, summary_types, collections=None) Args grads_vars : grads and vars list summary_type : a list of all sumary types to add e.g.: ['scalar', 'histogram', 'sparsity', 'mean', 'rms', 'stddev', 'norm', 'max', 'min'] collections : training or validation collections Add image summary to a image tensor tefla.core.summary.summary_image (tensor, name=None, max_images=10, collections=None) Args tensor : a tensor to add summary name : name of the tensor max_images : num of images to add summary collections : training or validation collections","title":"Summaries"},{"location":"core/summary/#add-summary-to-a-tensor-scalar-summary-if-the-tensor-is-1d-else-scalar-and-histogram-summary","text":"tefla.core.summary.summary_metric (tensor, name=None, collections=None)","title":"Add summary to a tensor, scalar summary if the tensor is 1D, else scalar and histogram summary"},{"location":"core/summary/#add-summary-to-a-tensor-scalar-summary-if-the-tensor-is-1d-else-scalar-and-histogram-summary_1","text":"tefla.core.summary.summary_activation (tensor, name=None, collections=None)","title":"Add summary to a tensor, scalar summary if the tensor is 1D, else  scalar and histogram summary"},{"location":"core/summary/#creates-the-summar-writter-for-training-and-validation","text":"tefla.core.summary.create_summary_writer (summary_dir, sess)","title":"creates the summar writter for training and validation"},{"location":"core/summary/#add-summary-as-per-the-ops-mentioned","text":"tefla.core.summary.summary_param (op, tensor, ndims, name, collections=None)","title":"Add summary as per the ops mentioned"},{"location":"core/summary/#add-summary-to-all-trainable-tensors","text":"tefla.core.summary.summary_trainable_params (summary_types, collections=None)","title":"Add summary to all trainable tensors"},{"location":"core/summary/#add-summary-to-all-gradient-tensors","text":"tefla.core.summary.summary_gradients (grad_vars, summary_types, collections=None)","title":"Add summary to all gradient tensors"},{"location":"core/summary/#add-image-summary-to-a-image-tensor","text":"tefla.core.summary.summary_image (tensor, name=None, max_images=10, collections=None)","title":"Add image summary to a image tensor"},{"location":"core/training/","text":"Supervised Trainer class tefla.core.training.SupervisedTrainer (model, cnf, training_iterator= , validation_iterator= , start_epoch=1, resume_lr=0.01, classification=True, clip_norm=True, n_iters_per_epoch=1094, num_classes=5, gpu_memory_fraction=0.94, is_summary=False, loss_type='softmax_cross_entropy') Args model : model definition cnf : dict, training configs training_iterator : iterator to use for training data access, processing and augmentations validation_iterator : iterator to use for validation data access, processing and augmentations start_epoch : int, training start epoch; for resuming training provide the last epoch number to resume training from, its a required parameter for training data balancing resume_lr : float, learning rate to use for new training classification : bool, classificattion or regression clip_norm : bool, to clip gradient using gradient norm, stabilizes the training n_iters_per_epoch : int, number of iteratiosn for each epoch; e.g: total_training_samples/batch_size gpu_memory_fraction : amount of gpu memory to use is_summary : bool, to write summary or not Methods fit (data_set, weights_from=None, start_epoch=1, summary_every=10, weights_dir='weights', verbose=0) Args data_set : dataset instance to use to access data for training/validation weights_from : str, if not None, initializes model from exisiting weights start_epoch : int, epoch number to start training from e.g. for retarining set the epoch number you want to resume training from summary_every : int, epoch interval to write summary; higher value means lower frequency of summary writing verbose : log level Clips the gradients by the given value tefla.core.training._clip_grad_norms (gradients_to_variables, max_norm=10) Args gradients_to_variables : A list of gradient to variable pairs (tuples). max_norm : the maximum norm value. Returns A list of clipped gradient to variable pairs. Clips the gradients by the given value tefla.core.training.clip_grad_global_norms (tvars, loss, opt, global_norm=1, gate_gradients=1, gradient_noise_scale=4.0, GATE_GRAPH=2, grad_loss=None, agre_method=None, col_grad_ops=False) Args tvars : trainable variables used for gradint updates loss : total loss of the network opt : optimizer global_norm : the maximum global norm Returns A list of clipped gradient to variable pairs. Multiply specified gradients tefla.core.training.multiply_gradients (grads_and_vars, gradient_multipliers) Args grads_and_vars : A list of gradient to variable pairs (tuples). gradient_multipliers : A map from either Variables or Variable op names to the coefficient by which the associated gradient should be scaled. Returns The updated list of gradient to variable pairs. Adds scaled noise from a 0-mean normal distribution to gradients tefla.core.training.add_scaled_noise_to_gradients (grads_and_vars, gradient_noise_scale=10.0) Args grads_and_vars : list of gradient and variables gardient_noise_scale : value of noise factor Returns noise added gradients","title":"Learner"},{"location":"core/training/#supervised-trainer-class","text":"tefla.core.training.SupervisedTrainer (model, cnf, training_iterator= , validation_iterator= , start_epoch=1, resume_lr=0.01, classification=True, clip_norm=True, n_iters_per_epoch=1094, num_classes=5, gpu_memory_fraction=0.94, is_summary=False, loss_type='softmax_cross_entropy')","title":"Supervised Trainer class"},{"location":"core/training/#clips-the-gradients-by-the-given-value","text":"tefla.core.training._clip_grad_norms (gradients_to_variables, max_norm=10)","title":"Clips the gradients by the given value"},{"location":"core/training/#clips-the-gradients-by-the-given-value_1","text":"tefla.core.training.clip_grad_global_norms (tvars, loss, opt, global_norm=1, gate_gradients=1, gradient_noise_scale=4.0, GATE_GRAPH=2, grad_loss=None, agre_method=None, col_grad_ops=False)","title":"Clips the gradients by the given value"},{"location":"core/training/#multiply-specified-gradients","text":"tefla.core.training.multiply_gradients (grads_and_vars, gradient_multipliers)","title":"Multiply specified gradients"},{"location":"core/training/#adds-scaled-noise-from-a-0-mean-normal-distribution-to-gradients","text":"tefla.core.training.add_scaled_noise_to_gradients (grads_and_vars, gradient_noise_scale=10.0)","title":"Adds scaled noise from a 0-mean normal distribution to gradients"},{"location":"da/data/","text":"Warp an image according to a given coordinate transformation tefla.da.data.fast_warp (img, tf, output_shape, mode='constant', mode_cval=0, order=0) This wrapper function is faster than skimage.transform.warp Args img : ndarray , input image tf : For 2-D images, you can directly pass a transformation object e.g. skimage.transform.SimilarityTransform, or its inverse. output_shape : tuple, (rows, cols) mode : mode for transformation available modes: { constant , edge , symmetric , reflect , wrap } mode_cval : float, Used in conjunction with mode constant , the value outside the image boundaries order : int, The order of interpolation. The order has to be in the range 0-5: 0: Nearest-neighbor 1: Bi-linear (default) 2: Bi-quadratic 3: Bi-cubic 4: Bi-quartic 5: Bi-quintic Returns warped, double ndarray Transform input image contrast tefla.da.data.contrast_transform (img, contrast_min=0.8, contrast_max=1.2) Transform the input image contrast by a factor returned by a unifrom distribution with contarst_min and contarst_max as params Args img : ndarray , input image contrast_min : float, minimum contrast for transformation contrast_max : float, maximum contrast for transformation Returns ndarray , contrast enhanced image Transform input image brightness tefla.da.data.brightness_transform (img, brightness_min=0.93, brightness_max=1.4) Transform the input image brightness by a factor returned by a unifrom distribution with brightness_min and brightness_max as params Args img : ndarray , input image brightness_min : float, minimum contrast for transformation brightness_max : float, maximum contrast for transformation Returns ndarray , brightness transformed image Rescale Transform tefla.da.data.build_rescale_transform_slow (downscale_factor, image_shape, target_shape) This mimics the skimage.transform.resize function. The resulting image is centered. Args downscale_factor : float, >1 image_shape : tuple(rows, cols), input image shape target_shape : tuple(rows, cols), output image shape Returns rescaled centered image transform instance Rescale Transform tefla.da.data.build_rescale_transform_fast (downscale_factor, image_shape, target_shape) estimating the correct rescaling transform is slow, so just use the downscale_factor to define a transform directly. This probably isn't 100% correct, but it shouldn't matter much in practice. The resulting image is centered. Args downscale_factor : float, >1 image_shape : tuple(rows, cols), input image shape target_shape : tuple(rows, cols), output image shape Returns rescaled and centering transform instance Image cetering transform tefla.da.data.build_centering_transform (image_shape, target_shape) Args image_shape : tuple(rows, cols), input image shape target_shape : tuple(rows, cols), output image shape Returns a centering transform instance Center Unceter transform tefla.da.data.build_center_uncenter_transforms (image_shape) These are used to ensure that zooming and rotation happens around the center of the image. Use these transforms to center and uncenter the image around such a transform. Args image_shape : tuple(rows, cols), input image shape Returns a center and an uncenter transform instance Augmentation transform tefla.da.data.build_augmentation_transform (zoom= (1.0, 1.0), rotation=0, shear=0, translation= (0, 0), flip=False) It performs zooming, rotation, shear, translation and flip operation Affine Transformation on the input image Args zoom : a tuple(zoom_rows, zoom_cols) rotation : float, Rotation angle in counter-clockwise direction as radians. shear : float, shear angle in counter-clockwise direction as radians translation : tuple(trans_rows, trans_cols) flip : bool, flip an image Returns augment tranform instance Random perturbation tefla.da.data.random_perturbation_transform (zoom_range, rotation_range, shear_range, translation_range, do_flip=True, allow_stretch=False, rng= ) It perturbs the image randomly Args zoom_range : a tuple(min_zoom, max_zoom) e.g.: (1/1.15, 1.15) rotation_range : a tuple(min_angle, max_angle) e.g.: (0. 360) shear_range : a tuple(min_shear, max_shear) e.g.: (0, 15) translation_range : a tuple(min_shift, max_shift) e.g.: (-15, 15) do_flip : bool, flip an image allow_stretch : bool, stretch an image rng : an instance Returns augment transform instance crop an image tefla.da.data.definite_crop (img, bbox) Args img : ndarray , input image bbox : list, with crop co-ordinates and width and height e.g.: [x, y, width, height] Returns returns cropped image Perturb image tefla.da.data.perturb (img, augmentation_params, target_shape, rng= , mode='constant', mode_cval=0) It perturbs an image with augmentation transform Args img : a ndarray , input image augmentation_paras : a dict, with augmentation name as keys and values as params target_shape : a tuple(rows, cols), output image shape rng : an instance for random number generation mode : mode for transformation available modes: { constant , edge , symmetric , reflect , wrap } mode_cval : float, Used in conjunction with mode constant , the value outside the image boundaries Returns a ndarray of transformed image Perturb image rescaled tefla.da.data.perturb_rescaled (img, scale, augmentation_params, target_shape= (224, 224), rng= , mode='constant', mode_cval=0) It perturbs an image with augmentation transform Args img : a ndarray , input image scale : float, >1, downscaling factor. augmentation_paras : a dict, with augmentation name as keys and values as params target_shape : a tuple(rows, cols), output image shape rng : an instance for random number generation mode : mode for transformation available modes: { constant , edge , symmetric , reflect , wrap } mode_cval : float, Used in conjunction with mode constant , the value outside the image boundaries Returns a ndarray of transformed image Perturb image Determinastic tefla.da.data.perturb_fixed (img, tform_augment, target_shape= (50, 50), mode='constant', mode_cval=0) It perturbs an image with augmentation transform with determinastic params used for validation/testing data Args img : a ndarray , input image augmentation_paras : a dict, with augmentation name as keys and values as params target_shape : a tuple(rows, cols), output image shape mode : mode for transformation available modes: { constant , edge , symmetric , reflect , wrap } mode_cval : float, Used in conjunction with mode constant , the value outside the image boundaries Returns a ndarray of transformed image Load augmented image with output shape (w, h) tefla.da.data.load_augment (fname, preprocessor, w, h, is_training, aug_params={'zoom_range':(1.0, 1.0), 'translation_range':(0, 0), 'shear_range':(0, 0), 'do_flip': False, 'allow_stretch': False, 'rotation_range':(0, 0)}, transform=None, bbox=None, fill_mode='constant', fill_mode_cval=0, standardizer=None, save_to_dir=None) Default arguments return non augmented image of shape (w, h). To apply a fixed transform (color augmentation) specify transform (color_vec). To generate a random augmentation specify aug_params and sigma. Args fname : string, image filename preprocessor : real-time image processing/crop w : int, width of target image h : int, height of target image is_training : bool, if True then training else validation aug_params : a dict, augmentation params transform : transform instance bbox : object bounding box fll_mode : mode for transformation available modes: { constant , edge , symmetric , reflect , wrap } fill_mode_cval : float, Used in conjunction with mode constant , the value outside the image boundaries standardizer : image standardizer, zero mean, unit variance image e.g.: samplewise standardized each image based on its own value save_to_dir : a string, path to save image, save output image to a dir Returns augmented image Open Image tefla.da.data.image_no_preprocessing (fname) Args fname : Image filename Returns PIL formatted image Load batch of images tefla.da.data.load_images (imgs, preprocessor= ) Args imgs : a list of image filenames preprocessor : image processing function Returns a ndarray with a batch of images Load image tefla.da.data.load_image (img, preprocessor= ) Args img : a image filename preprocessor : image processing function Returns a processed image Save image tefla.da.data.save_image (x, fname) Args x : input array fname : filename of the output image Data balancing utility tefla.da.data.balance_per_class_indices (y, weights) Args y : class labels weights : sampling weights per class Returns balanced batch as per weights","title":"Data Augmentation"},{"location":"da/data/#warp-an-image-according-to-a-given-coordinate-transformation","text":"tefla.da.data.fast_warp (img, tf, output_shape, mode='constant', mode_cval=0, order=0) This wrapper function is faster than skimage.transform.warp","title":"Warp an image according to a given coordinate transformation"},{"location":"da/data/#transform-input-image-contrast","text":"tefla.da.data.contrast_transform (img, contrast_min=0.8, contrast_max=1.2) Transform the input image contrast by a factor returned by a unifrom distribution with contarst_min and contarst_max as params","title":"Transform input image contrast"},{"location":"da/data/#transform-input-image-brightness","text":"tefla.da.data.brightness_transform (img, brightness_min=0.93, brightness_max=1.4) Transform the input image brightness by a factor returned by a unifrom distribution with brightness_min and brightness_max as params","title":"Transform input image brightness"},{"location":"da/data/#rescale-transform","text":"tefla.da.data.build_rescale_transform_slow (downscale_factor, image_shape, target_shape) This mimics the skimage.transform.resize function. The resulting image is centered.","title":"Rescale Transform"},{"location":"da/data/#rescale-transform_1","text":"tefla.da.data.build_rescale_transform_fast (downscale_factor, image_shape, target_shape) estimating the correct rescaling transform is slow, so just use the downscale_factor to define a transform directly. This probably isn't 100% correct, but it shouldn't matter much in practice. The resulting image is centered.","title":"Rescale Transform"},{"location":"da/data/#image-cetering-transform","text":"tefla.da.data.build_centering_transform (image_shape, target_shape)","title":"Image cetering transform"},{"location":"da/data/#center-unceter-transform","text":"tefla.da.data.build_center_uncenter_transforms (image_shape) These are used to ensure that zooming and rotation happens around the center of the image. Use these transforms to center and uncenter the image around such a transform.","title":"Center Unceter transform"},{"location":"da/data/#augmentation-transform","text":"tefla.da.data.build_augmentation_transform (zoom= (1.0, 1.0), rotation=0, shear=0, translation= (0, 0), flip=False) It performs zooming, rotation, shear, translation and flip operation Affine Transformation on the input image","title":"Augmentation transform"},{"location":"da/data/#random-perturbation","text":"tefla.da.data.random_perturbation_transform (zoom_range, rotation_range, shear_range, translation_range, do_flip=True, allow_stretch=False, rng= ) It perturbs the image randomly","title":"Random perturbation"},{"location":"da/data/#crop-an-image","text":"tefla.da.data.definite_crop (img, bbox)","title":"crop an image"},{"location":"da/data/#perturb-image","text":"tefla.da.data.perturb (img, augmentation_params, target_shape, rng= , mode='constant', mode_cval=0) It perturbs an image with augmentation transform","title":"Perturb image"},{"location":"da/data/#perturb-image-rescaled","text":"tefla.da.data.perturb_rescaled (img, scale, augmentation_params, target_shape= (224, 224), rng= , mode='constant', mode_cval=0) It perturbs an image with augmentation transform","title":"Perturb image rescaled"},{"location":"da/data/#perturb-image-determinastic","text":"tefla.da.data.perturb_fixed (img, tform_augment, target_shape= (50, 50), mode='constant', mode_cval=0) It perturbs an image with augmentation transform with determinastic params used for validation/testing data","title":"Perturb image Determinastic"},{"location":"da/data/#load-augmented-image-with-output-shape-w-h","text":"tefla.da.data.load_augment (fname, preprocessor, w, h, is_training, aug_params={'zoom_range':(1.0, 1.0), 'translation_range':(0, 0), 'shear_range':(0, 0), 'do_flip': False, 'allow_stretch': False, 'rotation_range':(0, 0)}, transform=None, bbox=None, fill_mode='constant', fill_mode_cval=0, standardizer=None, save_to_dir=None) Default arguments return non augmented image of shape (w, h). To apply a fixed transform (color augmentation) specify transform (color_vec). To generate a random augmentation specify aug_params and sigma.","title":"Load augmented image with output shape (w, h)"},{"location":"da/data/#open-image","text":"tefla.da.data.image_no_preprocessing (fname)","title":"Open Image"},{"location":"da/data/#load-batch-of-images","text":"tefla.da.data.load_images (imgs, preprocessor= )","title":"Load batch of images"},{"location":"da/data/#load-image","text":"tefla.da.data.load_image (img, preprocessor= )","title":"Load image"},{"location":"da/data/#save-image","text":"tefla.da.data.save_image (x, fname)","title":"Save image"},{"location":"da/data/#data-balancing-utility","text":"tefla.da.data.balance_per_class_indices (y, weights)","title":"Data balancing utility"},{"location":"da/standardizer/","text":"Samplewise Standardizer tefla.da.standardizer.SamplewiseStandardizer (clip, channel_wise=False) Args clip : max/min allowed value in the output image e.g.: 6 channel_wise : perform standarization separately accross channels Samplewise Standardizer tefla.da.standardizer.SamplewiseStandardizerTF (clip, channel_wise=False) Args clip : max/min allowed value in the output image e.g.: 6 channel_wise : perform standarization separately accross channels Aggregate Standardizer tefla.da.standardizer.AggregateStandardizer (mean, std, u, ev, sigma=0.0, color_vec=None) Creates a standardizer based on whole training dataset Args mean : 1-D array, aggregate mean array e.g.: mean is calculated for each color channel, R, G, B std : 1-D array, aggregate standard deviation array e.g.: std is calculated for each color channel, R, G, B u : 2-D array, eigenvector for the color channel variation ev : 1-D array, eigenvalues sigma : float, noise factor color_vec : an optional color vector Methods augment_color (img, sigma=0.0, color_vec=None) Args img : input image sigma : a float, noise factor color_vec : an optional color vec Aggregate Standardizer tefla.da.standardizer.AggregateStandardizerTF (mean, std, u, ev, sigma=0.0, color_vec=None) Creates a standardizer based on whole training dataset Args mean : 1-D array, aggregate mean array e.g.: mean is calculated for each color channel, R, G, B std : 1-D array, aggregate standard deviation array e.g.: std is calculated for each color channel, R, G, B u : 2-D array, eigenvector for the color channel variation ev : 1-D array, eigenvalues sigma : float, noise factor color_vec : an optional color vector Methods augment_color (img, sigma=0.0, color_vec=None) Args img : input image sigma : a float, noise factor color_vec : an optional color vec","title":"Standardizer"},{"location":"da/standardizer/#samplewise-standardizer","text":"tefla.da.standardizer.SamplewiseStandardizer (clip, channel_wise=False)","title":"Samplewise Standardizer"},{"location":"da/standardizer/#samplewise-standardizer_1","text":"tefla.da.standardizer.SamplewiseStandardizerTF (clip, channel_wise=False)","title":"Samplewise Standardizer"},{"location":"da/standardizer/#aggregate-standardizer","text":"tefla.da.standardizer.AggregateStandardizer (mean, std, u, ev, sigma=0.0, color_vec=None) Creates a standardizer based on whole training dataset","title":"Aggregate Standardizer"},{"location":"da/standardizer/#aggregate-standardizer_1","text":"tefla.da.standardizer.AggregateStandardizerTF (mean, std, u, ev, sigma=0.0, color_vec=None) Creates a standardizer based on whole training dataset","title":"Aggregate Standardizer"},{"location":"dataset/base/","text":"A simple class for handling data sets, tefla.dataset.base.Dataset (name, decoder, data_dir=None, num_classes=10, num_examples_per_epoch=1, batch_size=1, items_to_descriptions=None, **kwargs) Args name : a string, Name of the class instance decoder : object instance, tfrecords object decoding and image encoding and decoding data_dir : a string, path to the data folder num_classes : num of classes of the dataset num_examples_per_epoch : total number of examples per epoch items_to_description : a string descriving the items of the dataset Methods data_files (self) Returns python list of all (sharded) data set files.","title":"Dataset"},{"location":"dataset/base/#a-simple-class-for-handling-data-sets","text":"tefla.dataset.base.Dataset (name, decoder, data_dir=None, num_classes=10, num_examples_per_epoch=1, batch_size=1, items_to_descriptions=None, **kwargs)","title":"A simple class for handling data sets,"},{"location":"dataset/dataflow/","text":"Dataflow handling class tefla.dataset.dataflow.Dataflow (dataset, num_readers=1, shuffle=True, num_epochs=None, min_queue_examples=1024, capacity=2048) Args dataset : an instance of the dataset class num_readers : num of readers to read the dataset shuffle : a bool, shuffle the dataset num_epochs : total number of epoch for training or validation min_queue_examples : minimum number of items after dequeue capacity : total queue capacity Methods batch_inputs (batch_size, train, tfrecords_image_size, crop_size, im_size=None, bbox=None, image_preprocessing=None, num_preprocess_threads=16) Args dataset : instance of Dataset class specifying the dataset. See dataset.py for details. batch_size : integer train : boolean crop_size : training time image size. a int or tuple tfrecords_image_size : a list with original image size used to encode image in tfrecords e.g.: [width, height, channel] image_processing : a function to process image num_preprocess_threads : integer, total number of preprocessing threads Returns images: 4-D float Tensor of a batch of images labels: 1-D integer Tensor of [batch_size]. get (items, image_size, resize_size=None) Args items : a list, with items to get from the dataset e.g.: ['image', 'label'] image_size : a list with original image size e.g.: [width, height, channel] resize_size : if image resize required, provide a list of width and height e.g.: [width, height] get_batch (batch_size, target_probs, image_size, resize_size=None, crop_size=[32, 32, 3], image_preprocessing=None, num_preprocess_threads=32, init_probs=None, enqueue_many=True, queue_capacity=2048, threads_per_queue=4, name='balancing_op', data_balancing=True) Stochastically creates batches based on per-class probabilities. This method discards examples. Internally, it creates one queue to amortize the cost of disk reads, and one queue to hold the properly-proportioned batch. Args batch_size : a int, batch_size target_probs : probabilities of class samples to be present in the batch image_size : a list with original image size e.g.: [width, height, channel] resize_size : if image resize required, provide a list of width and height e.g.: [width, height] init_probs : initial probs of data sample in the first batch enqueue_many : bool, if true, interpret input tensors as having a batch dimension. queue_capacity : Capacity of the large queue that holds input examples. threads_per_queue : Number of threads for the large queue that holds input examples and for the final queue with the proper class proportions. name : a optional scope/name of the op prefetch (tensor_dict, capacity) Creates a FIFO queue to asynchronously enqueue tensor_dicts and returns a dequeue op that evaluates to a tensor_dict. This function is useful in prefetching preprocessed tensors so that the data is readily available for consumers. Args tensor_dict : a dictionary of tensors to prefetch. capacity : the size of the prefetch queue. Returns a FIFO prefetcher queue","title":"Dataflow"},{"location":"dataset/dataflow/#dataflow-handling-class","text":"tefla.dataset.dataflow.Dataflow (dataset, num_readers=1, shuffle=True, num_epochs=None, min_queue_examples=1024, capacity=2048)","title":"Dataflow handling class"},{"location":"dataset/decoder/","text":"A Decoder class to decode examples tefla.dataset.decoder.Decoder (feature_keys) Args feature_keys : a dict, with features name and data types e.g.: features_keys = {'image/encoded/image': tf.FixedLenFeature((), tf.string, default_value=''),'image/format': tf.FixedLenFeature((), tf.string, default_value='jpg'),'image/class/label': tf.FixedLenFeature([], tf.int64, - default_value=tf.zeros([], dtype=tf.int64)), } Methods decode (example_serialized, image_size, resize_size=None) Args example_serialized : scalar Tensor tf.string containing a serialized Example protocol buffer. Returns : image_buffer: Tensor tf.string containing the contents of a JPEG file. label: Tensor tf.int32 containing the label. text: Tensor tf.string containing the human-readable label. Returns image_buffer: Tensor tf.string containing the contents of a JPEG file. label: Tensor tf.int32 containing the label. text: Tensor tf.string containing the human-readable label. distort_image (image, distort_op, height, width, thread_id=0, scope=None) Args image : 3-D float Tensor of image height : integer width : integer thread_id : integer indicating the preprocessing thread. scope : Optional scope for name_scope. Returns 3-D float Tensor of distorted image used for training. Returns 3-D float Tensor of distorted image used for training. eval_image (image, height, width, scope=None) Args image : 3-D float Tensor height : integer width : integer scope : Optional scope for name_scope. Returns 3-D float Tensor of prepared image. Returns 3-D float Tensor of prepared image. parse_example_proto (example_serialized, is_bbox=False) The output of the build_image_data.py image preprocessing script is a dataset containing serialized Example protocol buffers. Each Example proto contains the following fields: image/height: 462 image/width: 581 image/colorspace: 'RGB' image/channels: 3 image/class/label: 615 image/class/synset: 'n03623198' image/class/text: 'knee pad' image/object/bbox/xmin: 0.1 image/object/bbox/xma x 0.9 image/object/bbox/ymin: 0.2 image/object/bbox/yma x 0.6 image/object/bbox/label: 615 image/format: 'JPEG' image/filename: 'ILSVRC2012_val_00041207.JPEG' image/encoded: Args example_serialized : scalar Tensor tf.string containing a serialized Example protocol buffer. Returns image_buffer: Tensor tf.string containing the contents of a JPEG file. label: Tensor tf.int32 containing the label. bbo x 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords] where each coordinate is [0, 1) and the coordinates are arranged as [ymin, xmin, ymax, xmax]. text: Tensor tf.string containing the human-readable label.","title":"Decoder"},{"location":"dataset/decoder/#a-decoder-class-to-decode-examples","text":"tefla.dataset.decoder.Decoder (feature_keys)","title":"A Decoder class to decode examples"},{"location":"dataset/image_to_tfrecords/","text":"","title":"TfRecords"},{"location":"dataset/reader/","text":"TFrecords reader class tefla.dataset.reader.Reader (dataset, reader_kwargs=None, shuffle=True, num_readers=16, capacity=1, num_epochs=None) Args dataset : an instance of the dataset class reader_kwargs : extra arguments to be passed to the TFRecordReader shuffle : whether to shuffle the dataset num_readers :a int, num of readers to launch capacity : a int, capacity of the queue used num_epochs : a int, num of epochs for training or validation Methods parallel_reader (min_queue_examples=1024) Primarily used for Training ops Args min_queue_examples : min number of queue examples after dequeue single_reader (num_epochs=1, shuffle=False, capacity=1) Data will be read using single TFRecordReader, primarily used for validation Args num_epochs : number of epoch shuffle : shuffle the dataset. False for validation capacity : queue capacity Returns a single item from the tfrecord files","title":"Reader"},{"location":"dataset/reader/#tfrecords-reader-class","text":"tefla.dataset.reader.Reader (dataset, reader_kwargs=None, shuffle=True, num_readers=16, capacity=1, num_epochs=None)","title":"TFrecords reader class"},{"location":"utils/util/","text":"Device chooser for variables tefla.utils.util.VariableDeviceChooser (num_parameter_servers=0, ps_device='/job:ps', placement='CPU:0') When using a parameter server it will assign them in a round-robin fashion. When not using a parameter server it allows GPU:0 placement otherwise CPU:0. Initialize VariableDeviceChooser. Args num_parameter_servers : number of parameter servers. ps_device : string representing the parameter server device. placement : string representing the placement of the variable either CPU:0 or GPU:0. When using parameter servers forced to CPU:0. Valid types for loss, variables and gradients tefla.utils.util.valid_dtypes () Subclasses should override to allow other float types. Returns Valid types for loss, variables and gradients. Asserts tensors are all valid types (see _valid_dtypes ) tefla.utils.util.assert_valid_dtypes (tensors) Args tensors : Tensors to check. Raises ValueError : If any tensor is not a valid type. Returns value if value_or_tensor_or_var has a constant value tefla.utils.util.constant_value (value_or_tensor_or_var, dtype=None) Args value_or_tensor_or_var : A value, a Tensor or a Variable . dtype : Optional tf.dtype , if set it would check it has the right dtype. Returns The constant value or None if it not constant. Return either fn1() or fn2() based on the boolean value of pred tefla.utils.util.static_cond (pred, fn1, fn2) Same signature as control_flow_ops.cond() but requires pred to be a bool. Args pred : A value determining whether to return the result of fn1 or fn2 . fn1 : The callable to be performed if pred is true. fn2 : The callable to be performed if pred is false. Returns Tensors returned by the call to either fn1 or fn2 . Return either fn1() or fn2() based on the boolean predicate/value pred tefla.utils.util.smart_cond (pred, fn1, fn2, name=None) If pred is bool or has a constant value it would use static_cond , otherwise it would use tf.cond . Args pred : A scalar determining whether to return the result of fn1 or fn2 . fn1 : The callable to be performed if pred is true. fn2 : The callable to be performed if pred is false. name : Optional name prefix when using tf.cond Returns Tensors returned by the call to either fn1 or fn2 . Returns Tensors returned by the call to either fn1 or fn2 . Transform numeric labels into onehot_labels tefla.utils.util.one_hot (labels, num_classes, name='one_hot') Args labels : [batch_size] target labels. num_classes : total number of classes. scope : Optional scope for op_scope. Returns one hot encoding of the labels. Returns one hot encoding of the labels. Returns a true if its input is a collections.Sequence (except strings) tefla.utils.util.is_sequence (seq) Args seq : an input sequence. Returns True if the sequence is a not a string and is a collections.Sequence. Returns a flat sequence from a given nested structure tefla.utils.util.flatten_sq (nest_sq) If nest is not a sequence, this returns a single-element list: [nest] . Args nest : an arbitrarily nested structure or a scalar object. Note, numpy arrays are considered scalars. Returns A Python list, the flattened version of the input. Returns the last dimension of shape while checking it has min_rank tefla.utils.util.last_dimension (shape, min_rank=1) Args shape : A TensorShape . min_rank : Integer, minimum rank of shape. Returns The value of the last dimension. Load Graph from frozen weights and model tefla.utils.util.load_frozen_graph (frozen_graph) Args frozen_graph : binary pb file Returns loaded graph Normalize a input layer tefla.utils.util.normalize (input_layer) Args inmput_layer : input layer tp normalize Returns normalized layer DeNormalize a input layer tefla.utils.util.denormalize (input_layer) Args input_layer : input layer to de normalize Returns denormalized layer Computes the squared pairwise Euclidean distances between x and y tefla.utils.util.compute_pairwise_distances (x, y) Args x : a tensor of shape [num_x_samples, num_features] y : a tensor of shape [num_y_samples, num_features] Returns a distance matrix of dimensions [num_x_samples, num_y_samples]. Computes a Guassian Radial Basis Kernel between the samples of x and y tefla.utils.util.gaussian_kernel_matrix (x, y, sigmas) We create a sum of multiple gaussian kernels each having a width sigma_i. Args x : a tensor of shape [num_samples, num_features] y : a tensor of shape [num_samples, num_features] sigmas : a tensor of floats which denote the widths of each of the gaussians in the kernel. Returns A tensor of shape [num_samples{x}, num_samples{y}] with the RBF kernel. compute the length of a sequence. 0 are masked tefla.utils.util.retrieve_seq_length (data) Args data : input sequence Returns a int , length of the sequence Advanced Indexing for Sequences tefla.utils.util.advanced_indexing (inp, index) Args inp : input sequence index : input index for indexing Returns a indexed sequence pad_sequences tefla.utils.util.pad_sequences (sequences, maxlen=None, dtype='int32', padding='post', truncating='post', value=0.0) Pad each sequence to the same length: the length of the longest sequence. If maxlen is provided, any sequence longer than maxlen is truncated to maxlen. Truncation happens off either the beginning or the end (default) of the sequence. Supports pre-padding and post-padding (default). Args sequences : list of lists where each element is a sequence. maxlen : a int , maximum length. dtype : type to cast the resulting sequence. padding : 'pre' or 'post', pad either before or after each sequence. truncating : 'pre' or 'post', remove values from sequences larger than maxlen either in the beginning or in the end of the sequence value : float , value to pad the sequences to the desired value. Returns x numpy array with dimensions (number_of_sequences, maxlen) Creates a dictionary char:integer for each unique character tefla.utils.util.chars_to_dictionary (string) Args string : a string input Returns dictionary of chars string_to_semi_redundant_sequences tefla.utils.util.string_to_semi_redundant_sequences (string, seq_maxlen=25, redun_step=3, char_idx=None) Vectorize a string and returns parsed sequences and targets, along with the associated dictionary. Args string : str . Lower-case text from input text file. seq_maxlen : int . Maximum length of a sequence. Default: 25. redun_step : int . Redundancy step. Default: 3. char_idx : 'dict'. A dictionary to convert chars to positions. Will be automatically generated if None Returns A tuple: (inputs, targets, dictionary) Vectorize Text file tefla.utils.util.textfile_to_semi_redundant_sequences (path, seq_maxlen=25, redun_step=3, to_lower_case=False, pre_defined_char_idx=None) textfile_to_semi_redundant_sequences. Vectorize a string from a textfile and returns parsed sequences and targets, along with the associated dictionary. Args path : str . path of the input text file. seq_maxlen : int . Maximum length of a sequence. Default: 25. redun_step : int . Redundancy step. Default: 3. to_lower_case : a bool , if true, convert to lowercase pre_defined_char_idx : 'dict'. A dictionary to convert chars to positions. Will be automatically generated if None Returns A tuple: (inputs, targets, dictionary) Computes log probabilities using numerically stable trick tefla.utils.util.logits_to_log_prob (logits) This uses two numerical stability tricks: 1) softmax(x) = softmax(x - c) where c is a constant applied to all arguments. If we set c = max(x) then the softmax is more numerically stable. 2) log softmax(x) is not numerically stable, but we can stabilize it by using the identity log softmax(x) = x - log sum exp(x) Args logits : Tensor of arbitrary shape whose last dimension contains logits. Returns A tensor of the same shape as the input, but with corresponding log probabilities. Get the name of the op that created a tensor tefla.utils.util.GetTensorOpName (x) Useful for naming related tensors, as ':' in name field of op is not permitted Args x the input tensor. Returns the name of the op. Returns the union of two lists tefla.utils.util.ListUnion (list_1, list_2) Python sets can have a non-deterministic iteration order. In some contexts, this could lead to TensorFlow producing two different programs when the same Python script is run twice. In these contexts we use lists instead of sets. This function is not designed to be especially fast and should only be used with small lists. Args list_1: A list list_2: Another list Returns A new list containing one copy of each unique element of list_1 and list_2. Uniqueness is determined by \"x in union\" logic; e.g. two ` string of that value appearing in the union. Maps xs to consumers tefla.utils.util.Interface (ys, xs) Returns a dict mapping each element of xs to any of its consumers that are indirectly consumed by ys. Args ys: The outputs xs: The inputs Returns out: Dict mapping each member x of xs to a list of all Tensors that are direct consumers of x and are eventually consumed by a member of ys . Clip an array of tensors by L2 norm tefla.utils.util.BatchClipByL2norm (t, upper_bound, name=None) Shrink each dimension-0 slice of tensor (for matrix it is each row) such that the l2 norm is at most upper_bound. Here we clip each row as it corresponds to each example in the batch. Args t: the input tensor. upper_bound: the upperbound of the L2 norm. name: optional name. Returns the clipped tensor. Add i.i.d. Gaussian noise (0, sigma^2) to every entry of t tefla.utils.util.AddGaussianNoise (t, sigma, name=None) Args t: the input tensor. sigma: the stddev of the Gaussian noise. name: optional name. Returns the noisy tensor.","title":"Utils"},{"location":"utils/util/#device-chooser-for-variables","text":"tefla.utils.util.VariableDeviceChooser (num_parameter_servers=0, ps_device='/job:ps', placement='CPU:0') When using a parameter server it will assign them in a round-robin fashion. When not using a parameter server it allows GPU:0 placement otherwise CPU:0. Initialize VariableDeviceChooser.","title":"Device chooser for variables"},{"location":"utils/util/#valid-types-for-loss-variables-and-gradients","text":"tefla.utils.util.valid_dtypes () Subclasses should override to allow other float types.","title":"Valid types for loss, variables and gradients"},{"location":"utils/util/#asserts-tensors-are-all-valid-types-see-_valid_dtypes","text":"tefla.utils.util.assert_valid_dtypes (tensors)","title":"Asserts tensors are all valid types (see _valid_dtypes)"},{"location":"utils/util/#returns-value-if-value_or_tensor_or_var-has-a-constant-value","text":"tefla.utils.util.constant_value (value_or_tensor_or_var, dtype=None)","title":"Returns value if value_or_tensor_or_var has a constant value"},{"location":"utils/util/#return-either-fn1-or-fn2-based-on-the-boolean-value-of-pred","text":"tefla.utils.util.static_cond (pred, fn1, fn2) Same signature as control_flow_ops.cond() but requires pred to be a bool.","title":"Return either fn1() or fn2() based on the boolean value of pred"},{"location":"utils/util/#return-either-fn1-or-fn2-based-on-the-boolean-predicatevalue-pred","text":"tefla.utils.util.smart_cond (pred, fn1, fn2, name=None) If pred is bool or has a constant value it would use static_cond , otherwise it would use tf.cond .","title":"Return either fn1() or fn2() based on the boolean predicate/value pred"},{"location":"utils/util/#transform-numeric-labels-into-onehot_labels","text":"tefla.utils.util.one_hot (labels, num_classes, name='one_hot')","title":"Transform numeric labels into onehot_labels"},{"location":"utils/util/#returns-a-true-if-its-input-is-a-collectionssequence-except-strings","text":"tefla.utils.util.is_sequence (seq)","title":"Returns a true if its input is a collections.Sequence (except strings)"},{"location":"utils/util/#returns-a-flat-sequence-from-a-given-nested-structure","text":"tefla.utils.util.flatten_sq (nest_sq) If nest is not a sequence, this returns a single-element list: [nest] .","title":"Returns a flat sequence from a given nested structure"},{"location":"utils/util/#returns-the-last-dimension-of-shape-while-checking-it-has-min_rank","text":"tefla.utils.util.last_dimension (shape, min_rank=1)","title":"Returns the last dimension of shape while checking it has min_rank"},{"location":"utils/util/#load-graph-from-frozen-weights-and-model","text":"tefla.utils.util.load_frozen_graph (frozen_graph)","title":"Load Graph from frozen weights and model"},{"location":"utils/util/#normalize-a-input-layer","text":"tefla.utils.util.normalize (input_layer)","title":"Normalize a input layer"},{"location":"utils/util/#denormalize-a-input-layer","text":"tefla.utils.util.denormalize (input_layer)","title":"DeNormalize a input layer"},{"location":"utils/util/#computes-the-squared-pairwise-euclidean-distances-between-x-and-y","text":"tefla.utils.util.compute_pairwise_distances (x, y)","title":"Computes the squared pairwise Euclidean distances between x and y"},{"location":"utils/util/#computes-a-guassian-radial-basis-kernel-between-the-samples-of-x-and-y","text":"tefla.utils.util.gaussian_kernel_matrix (x, y, sigmas) We create a sum of multiple gaussian kernels each having a width sigma_i.","title":"Computes a Guassian Radial Basis Kernel between the samples of x and y"},{"location":"utils/util/#compute-the-length-of-a-sequence-0-are-masked","text":"tefla.utils.util.retrieve_seq_length (data)","title":"compute the length of a sequence. 0 are masked"},{"location":"utils/util/#advanced-indexing-for-sequences","text":"tefla.utils.util.advanced_indexing (inp, index)","title":"Advanced Indexing for Sequences"},{"location":"utils/util/#pad_sequences","text":"tefla.utils.util.pad_sequences (sequences, maxlen=None, dtype='int32', padding='post', truncating='post', value=0.0) Pad each sequence to the same length: the length of the longest sequence. If maxlen is provided, any sequence longer than maxlen is truncated to maxlen. Truncation happens off either the beginning or the end (default) of the sequence. Supports pre-padding and post-padding (default).","title":"pad_sequences"},{"location":"utils/util/#creates-a-dictionary-charinteger-for-each-unique-character","text":"tefla.utils.util.chars_to_dictionary (string)","title":"Creates a dictionary char:integer for each unique character"},{"location":"utils/util/#string_to_semi_redundant_sequences","text":"tefla.utils.util.string_to_semi_redundant_sequences (string, seq_maxlen=25, redun_step=3, char_idx=None) Vectorize a string and returns parsed sequences and targets, along with the associated dictionary.","title":"string_to_semi_redundant_sequences"},{"location":"utils/util/#vectorize-text-file","text":"tefla.utils.util.textfile_to_semi_redundant_sequences (path, seq_maxlen=25, redun_step=3, to_lower_case=False, pre_defined_char_idx=None) textfile_to_semi_redundant_sequences. Vectorize a string from a textfile and returns parsed sequences and targets, along with the associated dictionary.","title":"Vectorize Text file"},{"location":"utils/util/#computes-log-probabilities-using-numerically-stable-trick","text":"tefla.utils.util.logits_to_log_prob (logits) This uses two numerical stability tricks: 1) softmax(x) = softmax(x - c) where c is a constant applied to all arguments. If we set c = max(x) then the softmax is more numerically stable. 2) log softmax(x) is not numerically stable, but we can stabilize it by using the identity log softmax(x) = x - log sum exp(x)","title":"Computes log probabilities using numerically stable trick"},{"location":"utils/util/#get-the-name-of-the-op-that-created-a-tensor","text":"tefla.utils.util.GetTensorOpName (x) Useful for naming related tensors, as ':' in name field of op is not permitted","title":"Get the name of the op that created a tensor"},{"location":"utils/util/#returns-the-union-of-two-lists","text":"tefla.utils.util.ListUnion (list_1, list_2) Python sets can have a non-deterministic iteration order. In some contexts, this could lead to TensorFlow producing two different programs when the same Python script is run twice. In these contexts we use lists instead of sets. This function is not designed to be especially fast and should only be used with small lists.","title":"Returns the union of two lists"},{"location":"utils/util/#maps-xs-to-consumers","text":"tefla.utils.util.Interface (ys, xs) Returns a dict mapping each element of xs to any of its consumers that are indirectly consumed by ys.","title":"Maps xs to consumers"},{"location":"utils/util/#clip-an-array-of-tensors-by-l2-norm","text":"tefla.utils.util.BatchClipByL2norm (t, upper_bound, name=None) Shrink each dimension-0 slice of tensor (for matrix it is each row) such that the l2 norm is at most upper_bound. Here we clip each row as it corresponds to each example in the batch.","title":"Clip an array of tensors by L2 norm"},{"location":"utils/util/#add-iid-gaussian-noise-0-sigma2-to-every-entry-of-t","text":"tefla.utils.util.AddGaussianNoise (t, sigma, name=None)","title":"Add i.i.d. Gaussian noise (0, sigma^2) to every entry of t"}]}