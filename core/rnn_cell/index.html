<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="Mrinal(Ishant) Haloi">
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>RNN - Tefla</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "RNN";
    var mkdocs_page_input_path = "core/rnn_cell.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js" defer></script>
  <script src="../../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> Tefla</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../..">Index</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../installation/">Installation</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Layers</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../layers/">Layers</a>
                </li>
                <li class=" current">
                    
    <a class="current" href="./">RNN</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#the-most-basic-rnn-cell">The most basic RNN cell</a></li>
    

    <li class="toctree-l3"><a href="#lstm-unit">LSTM unit</a></li>
    

    <li class="toctree-l3"><a href="#basic-attention-cell">Basic attention cell</a></li>
    

    <li class="toctree-l3"><a href="#gated-recurrent-unit-cell-cf-httparxivorgabs14061078">Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078)</a></li>
    

    <li class="toctree-l3"><a href="#rnn-cell-composed-sequentially-of-multiple-simple-cells">RNN cell composed sequentially of multiple simple cells</a></li>
    

    <li class="toctree-l3"><a href="#operator-adding-dropout-to-inputs-and-outputs-of-the-given-cell">Operator adding dropout to inputs and outputs of the given cell</a></li>
    

    <li class="toctree-l3"><a href="#convolution">convolution:</a></li>
    

    <li class="toctree-l3"><a href="#adds-a-fully-connected-layer">Adds a fully connected layer</a></li>
    

    <li class="toctree-l3"><a href="#adds-a-layer-normalization-layer-from-httpsarxivorgabs160706450">Adds a Layer Normalization layer from https://arxiv.org/abs/1607.06450</a></li>
    

    <li class="toctree-l3"><a href="#lstm">LSTM</a></li>
    

    <li class="toctree-l3"><a href="#gru">GRU</a></li>
    

    <li class="toctree-l3"><a href="#bidirectional-rnn">Bidirectional RNN</a></li>
    

    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../special_layers/">Special Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../layer_arg_ops/">Layer Args</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Learner</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../training/">Learner</a>
                </li>
                <li class="">
                    
    <a class="" href="../learning/">Learner Multi GPU</a>
                </li>
                <li class="">
                    
    <a class="" href="../learning_v2/">Learner Multi GPU V2</a>
                </li>
                <li class="">
                    
    <a class="" href="../learning_ss/">Learner Semi Supervised</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Predictor</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../prediction/">Predictor</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Built-in Ops</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../lr_policy/">Lr_Policy</a>
                </li>
                <li class="">
                    
    <a class="" href="../metrics/">Metrics</a>
                </li>
                <li class="">
                    
    <a class="" href="../initializers/">Initializations</a>
                </li>
                <li class="">
                    
    <a class="" href="../losses/">Losses</a>
                </li>
                <li class="">
                    
    <a class="" href="../summary/">Summaries</a>
                </li>
                <li class="">
                    
    <a class="" href="../logger/">Logger</a>
                </li>
                <li class="">
                    
    <a class="" href="../iter_ops/">Iter Ops</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Data Management</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../da/data/">Data Augmentation</a>
                </li>
                <li class="">
                    
    <a class="" href="../../da/standardizer/">Standardizer</a>
                </li>
                <li class="">
                    
    <a class="" href="../../dataset/image_to_tfrecords/">TfRecords</a>
                </li>
                <li class="">
                    
    <a class="" href="../../dataset/base/">Dataset</a>
                </li>
                <li class="">
                    
    <a class="" href="../../dataset/dataflow/">Dataflow</a>
                </li>
                <li class="">
                    
    <a class="" href="../../dataset/decoder/">Decoder</a>
                </li>
                <li class="">
                    
    <a class="" href="../../dataset/reader/">Reader</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Utils</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../utils/util/">Utils</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../license/">License</a>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">Tefla</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>Layers &raquo;</li>
        
      
    
    <li>RNN</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/n3011/tefla/edit/master/docs/core/rnn_cell.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="the-most-basic-rnn-cell">The most basic RNN cell</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/openagi/tefla/blob/master/tefla/core/rnn_cell.py#L15 target="_blank"><b>tefla.core.rnn_cell.BasicRNNCell</b></a></span>  (num_units,  reuse,  trainable=True,  w_init=<function  _initializer  at  0x7f8534d0ef50>,  use_bias=False,  input_size=None,  activation=<function  tanh  at  0x7f85478f7938>,  layer_norm=None,  layer_norm_args=None)</span></p>
<h3>Args</h3>

<ul>
<li><strong>num_units</strong>: int, The number of units in the LSTM cell.</li>
<li><strong>reuse</strong>: whether or not the layer and its variables should be reused. To be
able to reuse the layer scope must be given.</li>
<li><strong>input_size</strong>: Deprecated and unused.</li>
<li><strong>activation</strong>: Activation function of the states.</li>
<li><strong>layer_norm</strong>: If <code>True</code>, layer normalization will be applied.</li>
<li><strong>layer_norm_args</strong>: optional dict, layer_norm arguments</li>
<li><strong>trainable</strong>: If <code>True</code> also add variables to the graph collection
<code>GraphKeys.TRAINABLE_VARIABLES</code> (see tf.Variable).</li>
</ul>
<h2>Methods</h2>

<p><span class="hr_large"></span> </p>
<p><span class="extra_h2"><span style="color:black;"><a href=https://github.com/openagi/tefla/blob/master/tefla/#L353 target="_blank"><b>add_variable</b></a></span>  (name,  shape,  dtype=None,  initializer=None,  regularizer=None,  trainable=True)</span></p>
<p>Arguments:
  name: variable name.
  shape: variable shape.
  dtype: The type of the variable. Defaults to <code>self.dtype</code>.
  initializer: initializer instance (callable).
  regularizer: regularizer instance (callable).
  trainable: whether the variable should be part of the layer's
"trainable_variables" (e.g. variables, biases)
or "non_trainable_variables" (e.g. BatchNorm mean, stddev).</p>
<h5>Returns</h5>

<p>The created variable.</p>
<p><span class="hr_large"></span> </p>
<p><span class="extra_h2"><span style="color:black;"><a href=https://github.com/openagi/tefla/blob/master/tefla/#L490 target="_blank"><b>apply</b></a></span>  (inputs,  <em>args,  </em>*kwargs)</span></p>
<p>This simply wraps <code>self.__call__</code>.</p>
<p>Arguments:
  inputs: Input tensor(s).
  <em>args: additional positional arguments to be passed to <code>self.call</code>.
  </em>*kwargs: additional keyword arguments to be passed to <code>self.call</code>.</p>
<h5>Returns</h5>

<p>Output tensor(s).</p>
<p><span class="hr_large"></span> </p>
<p><span class="extra_h2"><span style="color:black;"><a href=https://github.com/openagi/tefla/blob/master/tefla/#L308 target="_blank"><b>call</b></a></span>  (inputs,  **kwargs)</span></p>
<p>Arguments:
  inputs: input tensor(s).
 **kwargs: additional keyword arguments.</p>
<h5>Returns</h5>

<p>Output tensor(s).</p>
<p><span class="hr_large"></span> </p>
<p><span class="extra_h2"><span style="color:black;"><a href=https://github.com/openagi/tefla/blob/master/tefla/#L280 target="_blank"><b>get_losses_for</b></a></span>  (inputs)</span></p>
<p>Arguments:
  inputs: Input tensor or list/tuple of input tensors.
Must match the <code>inputs</code> argument passed to the <code>__call__</code>
method at the time the losses were created.
If you pass <code>inputs=None</code>, unconditional losses are returned,
such as weight regularization losses.</p>
<h5>Returns</h5>

<p>List of loss tensors of the layer that depend on <code>inputs</code>.</p>
<p><span class="hr_large"></span> </p>
<p><span class="extra_h2"><span style="color:black;"><a href=https://github.com/openagi/tefla/blob/master/tefla/#L216 target="_blank"><b>get_updates_for</b></a></span>  (inputs)</span></p>
<p>Arguments:
  inputs: Input tensor or list/tuple of input tensors.
Must match the <code>inputs</code> argument passed to the <code>__call__</code> method
at the time the updates were created.
If you pass <code>inputs=None</code>, unconditional updates are returned.</p>
<h5>Returns</h5>

<p>List of update ops of the layer that depend on <code>inputs</code>.</p>
<p><span class="hr_large"></span> </p>
<p><span class="extra_h2"><span style="color:black;"><a href=https://github.com/openagi/tefla/blob/master/tefla/#L212 target="_blank"><b>zero_state</b></a></span>  (batch_size,  dtype)</span></p>
<h5>Args</h5>

<p>batch_size: int, float, or unit Tensor representing the batch size.
  dtype: the data type to use for the state.</p>
<h5>Returns</h5>

<p>If <code>state_size</code> is an int or TensorShape, then the return value is a
  <code>N-D</code> tensor of shape <code>[batch_size x state_size]</code> filled with zeros.</p>
<hr />
<h1 id="lstm-unit">LSTM unit</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/openagi/tefla/blob/master/tefla/core/rnn_cell.py#L61 target="_blank"><b>tefla.core.rnn_cell.LSTMCell</b></a></span>  (num_units,  reuse,  trainable=True,  w_init=<function  _initializer  at  0x7f8534cad230>,  forget_bias=1.0,  use_bias=False,  input_size=None,  activation=<function  tanh  at  0x7f85478f7938>,  inner_activation=<function  sigmoid  at  0x7f85478f7848>,  keep_prob=1.0,  dropout_seed=None,  cell_clip=None,  layer_norm=None,  layer_norm_args=None)</span></p>
<p>This class adds layer normalization and recurrent dropout to a
basic LSTM unit. Layer normalization implementation is based on:
https://arxiv.org/abs/1607.06450.
"Layer Normalization" Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton
and is applied before the internal nonlinearities.
Recurrent dropout is base on:
https://arxiv.org/abs/1603.05118
"Recurrent Dropout without Memory Loss"
Stanislau Semeniuta, Aliaksei Severyn, Erhardt Barth.</p>
<h3>Args</h3>

<ul>
<li><strong>num_units</strong>: int, The number of units in the LSTM cell.</li>
<li><strong>reuse</strong>: whether or not the layer and its variables should be reused. To be
able to reuse the layer scope must be given.</li>
<li><strong>forget_bias</strong>: float, The bias added to forget gates (see above).</li>
<li><strong>input_size</strong>: Deprecated and unused.</li>
<li><strong>activation</strong>: Activation function of the states.</li>
<li><strong>inner_activation</strong>: Activation function of the inner states.</li>
<li><strong>layer_norm</strong>: If <code>True</code>, layer normalization will be applied.</li>
<li><strong>layer_norm_args</strong>: optional dict, layer_norm arguments</li>
<li><strong>cell_clip</strong>: (optional) A float value, if provided the cell state is clipped
by this value prior to the cell output activation.</li>
<li><strong>keep_prob</strong>: unit Tensor or float between 0 and 1 representing the
recurrent dropout probability value. If float and 1.0, no dropout will
be applied.</li>
<li><strong>dropout_seed</strong>: (optional) integer, the randomness seed.</li>
<li><strong>trainable</strong>: If <code>True</code> also add variables to the graph collection
<code>GraphKeys.TRAINABLE_VARIABLES</code> (see tf.Variable).</li>
</ul>
<hr />
<h1 id="basic-attention-cell">Basic attention cell</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/openagi/tefla/blob/master/tefla/core/rnn_cell.py#L164 target="_blank"><b>tefla.core.rnn_cell.AttentionCell</b></a></span>  (cell,  attn_length,  reuse,  w_init=<function  _initializer  at  0x7f8534cad488>,  use_bias=False,  trainable=True,  attn_size=None,  attn_vec_size=None,  input_size=None,  layer_norm=None,  layer_norm_args=None)</span></p>
<p>Implementation based on https://arxiv.org/abs/1409.0473.
Create a cell with attention.</p>
<h3>Args</h3>

<ul>
<li><strong>cell</strong>: an RNNCell, an attention is added to it.
e.g.: a LSTMCell</li>
<li><strong>attn_length</strong>: integer, the size of an attention window.</li>
<li><strong>reuse</strong>: whether or not the layer and its variables should be reused. To be
able to reuse the layer scope must be given.</li>
<li><strong>attn_size</strong>: integer, the size of an attention vector. Equal to
cell.output_size by default.</li>
<li><strong>attn_vec_size</strong>: integer, the number of convolutional features calculated
on attention state and a size of the hidden layer built from
base cell state. Equal attn_size to by default.</li>
<li><strong>input_size</strong>: integer, the size of a hidden linear layer,</li>
<li><strong>layer_norm</strong>: If <code>True</code>, layer normalization will be applied.</li>
<li><strong>layer_norm_args</strong>: optional dict, layer_norm arguments
built from inputs and attention. Derived from the input tensor by default.</li>
<li><strong>trainable</strong>: If <code>True</code> also add variables to the graph collection
<code>GraphKeys.TRAINABLE_VARIABLES</code> (see tf.Variable).</li>
</ul>
<hr />
<h1 id="gated-recurrent-unit-cell-cf-httparxivorgabs14061078">Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078)</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/openagi/tefla/blob/master/tefla/core/rnn_cell.py#L257 target="_blank"><b>tefla.core.rnn_cell.GRUCell</b></a></span>  (num_units,  reuse,  w_init=<function  _initializer  at  0x7f8534cad758>,  use_bias=False,  trainable=True,  input_size=None,  activation=<function  tanh  at  0x7f85478f7938>,  inner_activation=<function  sigmoid  at  0x7f85478f7848>,  b_init=1.0,  keep_prob=1.0,  layer_norm=None,  layer_norm_args=None)</span></p>
<h3>Args</h3>

<ul>
<li><strong>num_units</strong>: int, The number of units in the LSTM cell.</li>
<li><strong>reuse</strong>: whether or not the layer and its variables should be reused. To be
able to reuse the layer scope must be given.</li>
<li><strong>input_size</strong>: Deprecated and unused.</li>
<li><strong>activation</strong>: Activation function of the states.</li>
<li><strong>inner_activation</strong>: Activation function of the inner states.</li>
<li><strong>layer_norm</strong>: If <code>True</code>, layer normalization will be applied.</li>
<li><strong>layer_norm_args</strong>: optional dict, layer_norm arguments</li>
<li><strong>trainable</strong>: If <code>True</code> also add variables to the graph collection
<code>GraphKeys.TRAINABLE_VARIABLES</code> (see tf.Variable).</li>
</ul>
<hr />
<h1 id="rnn-cell-composed-sequentially-of-multiple-simple-cells">RNN cell composed sequentially of multiple simple cells</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/openagi/tefla/blob/master/tefla/core/rnn_cell.py#L322 target="_blank"><b>tefla.core.rnn_cell.MultiRNNCell</b></a></span>  (cells,  state_is_tuple=True)</span></p>
<p>Create a RNN cell composed sequentially of a number of RNNCells.
<h3>Args</h3></p>
<ul>
<li><strong>cells</strong>: list of RNNCells that will be composed in this order.</li>
</ul>
<hr />
<h1 id="operator-adding-dropout-to-inputs-and-outputs-of-the-given-cell">Operator adding dropout to inputs and outputs of the given cell</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/openagi/tefla/blob/master/tefla/core/rnn_cell.py#L1034 target="_blank"><b>tefla.core.rnn_cell.DropoutWrapper</b></a></span>  (cell,  is_training,  input_keep_prob=1.0,  output_keep_prob=1.0,  seed=None)</span></p>
<p>Create a cell with added input and/or output dropout.
Dropout is never used on the state.</p>
<h3>Args</h3>

<ul>
<li><strong>cell</strong>: an RNNCell, a projection to output_size is added to it.</li>
<li><strong>is_training</strong>: a bool, training if true else validation/testing</li>
<li><strong>input_keep_prob</strong>: unit Tensor or float between 0 and 1, input keep
probability; if it is float and 1, no input dropout will be added.</li>
<li><strong>output_keep_prob</strong>: unit Tensor or float between 0 and 1, output keep
probability; if it is float and 1, no output dropout will be added.</li>
<li><strong>seed</strong>: (optional) integer, the randomness seed.</li>
</ul>
<hr />
<h1 id="convolution">convolution:</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/openagi/tefla/blob/master/tefla/core/rnn_cell.py#L799 target="_blank"><b>tefla.core.rnn_cell._conv</b></a></span>  (args,  filter_size,  num_features,  bias,  reuse,  w_init=None,  b_init=0.0,  scope='_conv')</span></p>
<h3>Args</h3>

<ul>
<li><strong>args</strong>: a Tensor or a list of Tensors of dimension 3D, 4D or 5D</li>
<li>batch x n, Tensors.</li>
<li><strong>filter_size</strong>: int tuple of filter height and width.</li>
<li><strong>reuse</strong>: None/True, whether to reuse variables</li>
<li><strong>w_init</strong>: weights initializer object</li>
<li><strong>b_init</strong>: a <code>int</code>, bias initializer value</li>
<li><strong>num_features</strong>: int, number of features.</li>
<li><strong>bias_start</strong>: starting value to initialize the bias; 0 by default.</li>
</ul>
<h3>Returns</h3>

<p>A 3D, 4D, or 5D Tensor with shape [batch ... num_features]</p>
<hr />
<h1 id="adds-a-fully-connected-layer">Adds a fully connected layer</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/openagi/tefla/blob/master/tefla/core/rnn_cell.py#L1087 target="_blank"><b>tefla.core.rnn_cell._linear</b></a></span>  (x,  n_output,  reuse,  trainable=True,  w_init=<function  _initializer  at  0x7f8534caf500>,  b_init=0.0,  w_regularizer=<function  l2_loss  at  0x7f85479d6ed8>,  name='fc',  layer_norm=None,  layer_norm_args=None,  activation=None,  outputs_collections=None,  use_bias=True)</span></p>
<p><code>fully_connected</code> creates a variable called <code>weights</code>, representing a fully
connected weight matrix, which is multiplied by the <code>x</code> to produce a
<code>Tensor</code> of hidden units. If a <code>layer_norm</code> is provided (such as
<code>layer_norm</code>), it is then applied. Otherwise, if <code>layer_norm</code> is
None and a <code>b_init</code> and <code>use_bias</code> is provided then a <code>biases</code> variable would be
created and added the hidden units. Finally, if <code>activation</code> is not <code>None</code>,
it is applied to the hidden units as well.
Note: that if <code>x</code> have a rank greater than 2, then <code>x</code> is flattened
prior to the initial matrix multiply by <code>weights</code>.</p>
<h3>Args</h3>

<ul>
<li><strong>x</strong>: A <code>Tensor</code> of with at least rank 2 and value for the last dimension,
i.e. <code>[batch_size, depth]</code>, <code>[None, None, None, channels]</code>.</li>
<li><strong>n_output</strong>: Integer or long, the number of output units in the layer.</li>
<li><strong>reuse</strong>: whether or not the layer and its variables should be reused. To be
able to reuse the layer scope must be given.</li>
<li><strong>activation</strong>: activation function, set to None to skip it and maintain
a linear activation.</li>
<li><strong>layer_norm</strong>: normalization function to use. If
 -<code>batch_norm</code> is <code>True</code> then google original implementation is used and
if another function is provided then it is applied.
default set to None for no normalizer function</li>
<li><strong>layer_norm_args</strong>: normalization function parameters.</li>
<li><strong>w_init</strong>: An initializer for the weights.</li>
<li><strong>w_regularizer</strong>: Optional regularizer for the weights.</li>
<li><strong>b_init</strong>: An initializer for the biases. If None skip biases.</li>
<li><strong>outputs_collections</strong>: The collections to which the outputs are added.</li>
<li><strong>trainable</strong>: If <code>True</code> also add variables to the graph collection
<code>GraphKeys.TRAINABLE_VARIABLES</code> (see tf.Variable).</li>
<li><strong>name</strong>: Optional name or scope for variable_scope/name_scope.</li>
<li><strong>use_bias</strong>: Whether to add bias or not</li>
</ul>
<h3>Returns</h3>

<p>The 2-D <code>Tensor</code> variable representing the result of the series of operations.
e.g: 2-D <code>Tensor</code> [batch, n_output].</p>
<hr />
<h1 id="adds-a-layer-normalization-layer-from-httpsarxivorgabs160706450">Adds a Layer Normalization layer from https://arxiv.org/abs/1607.06450</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/openagi/tefla/blob/master/tefla/core/rnn_cell.py#L1185 target="_blank"><b>tefla.core.rnn_cell.layer_norm</b></a></span>  (x,  reuse,  center=True,  scale=True,  trainable=True,  epsilon=1e-12,  name='bn',  outputs_collections=None)</span>
"Layer Normalization" Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton
Can be used as a normalizer function for conv2d and fully_connected.</p>
<h3>Args</h3>

<ul>
<li><strong>x</strong>: a tensor with 2 or more dimensions, where the first dimension has
<code>batch_size</code>. The normalization is over all but the last dimension if
<code>data_format</code> is <code>NHWC</code> and the second dimension if <code>data_format</code> is
<code>NCHW</code>.</li>
<li><strong>center</strong>: If True, subtract <code>beta</code>. If False, <code>beta</code> is ignored.</li>
<li><strong>scale</strong>: If True, multiply by <code>gamma</code>. If False, <code>gamma</code> is
not used. When the next layer is linear (also e.g. <code>nn.relu</code>), this can be
disabled since the scaling can be done by the next layer.</li>
<li><strong>epsilon</strong>: small float added to variance to avoid dividing by zero.</li>
<li><strong>reuse</strong>: whether or not the layer and its variables should be reused. To be
able to reuse the layer scope must be given.</li>
<li><strong>outputs_collections</strong>: collections to add the outputs.</li>
<li><strong>trainable</strong>: If <code>True</code> also add variables to the graph collection
<code>GraphKeys.TRAINABLE_VARIABLES</code> (see <code>tf.Variable</code>).</li>
<li><strong>name</strong>: Optional scope/name for <code>variable_scope</code>.</li>
</ul>
<h3>Returns</h3>

<p>A <code>Tensor</code> representing the output of the operation.</p>
<hr />
<h1 id="lstm">LSTM</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/openagi/tefla/blob/master/tefla/core/rnn_cell.py#L1322 target="_blank"><b>tefla.core.rnn_cell.lstm</b></a></span>  (inputs,  n_units,  reuse,  is_training,  activation=<function  tanh  at  0x7f85478f7938>,  inner_activation=<function  sigmoid  at  0x7f85478f7848>,  dropout=None,  use_bias=True,  w_init=<function  _initializer  at  0x7f8534cafde8>,  forget_bias=1.0,  return_seq=False,  return_state=False,  initial_state=None,  dynamic=True,  trainable=True,  scope='lstm')</span>
Long Short Term Memory Recurrent Layer.</p>
<h3>Args</h3>

<ul>
<li><strong>inputs</strong>: <code>Tensor</code>. Inputs 3-D Tensor [samples, timesteps, input_dim].</li>
<li><strong>n_units</strong>: <code>int</code>, number of units for this layer.</li>
<li><strong>reuse</strong>: <code>bool</code>. If True and 'scope' is provided, this layer variables
will be reused (shared).</li>
<li><strong>is_training</strong>: <code>bool</code>, training if True</li>
<li><strong>activation</strong>: <code>function</code> (returning a <code>Tensor</code>).</li>
<li><strong>inner_activation</strong>: <code>function</code> (returning a <code>Tensor</code>).</li>
<li><strong>dropout</strong>: <code>tuple</code> of <code>float</code>: (input_keep_prob, output_keep_prob). The
input and output keep probability.</li>
<li><strong>use_bias</strong>: <code>bool</code>. If True, a bias is used.</li>
<li><strong>w_init</strong>: <code>function</code> (returning a <code>Tensor</code>). Weights initialization.</li>
<li><strong>forget_bias</strong>: <code>float</code>. Bias of the forget gate. Default: 1.0.</li>
<li><strong>return_seq</strong>: <code>bool</code>. If True, returns the full sequence instead of
last sequence output only.</li>
<li><strong>return_state</strong>: <code>bool</code>. If True, returns a tuple with output and
states: (output, states).</li>
<li><strong>initial_state</strong>: <code>Tensor</code>. An initial state for the RNN.  This must be
a tensor of appropriate type and shape [batch_size x cell.state_size].</li>
<li><strong>dynamic</strong>: <code>bool</code>. If True, dynamic computation is performed. It will not
compute RNN steps above the sequence length. Note that because TF
requires to feed sequences of same length, 0 is used as a mask.
So a sequence padded with 0 at the end must be provided. When
computation is performed, it will stop when it meets a step with
a value of 0.</li>
<li><strong>trainable</strong>: <code>bool</code>. If True, weights will be trainable.</li>
<li><strong>scope</strong>: <code>str</code>. Define this layer scope (optional). A scope can be
used to share variables between layers. Note that scope will
override name.</li>
</ul>
<h3>Returns</h3>

<p>if <code>return_seq</code>: 3-D Tensor [samples, timesteps, output dim].
else: 2-D Tensor [samples, output dim].</p>
<hr />
<h1 id="gru">GRU</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/openagi/tefla/blob/master/tefla/core/rnn_cell.py#L1375 target="_blank"><b>tefla.core.rnn_cell.gru</b></a></span>  (inputs,  n_units,  reuse,  is_training,  activation=<function  tanh  at  0x7f85478f7938>,  inner_activation=<function  sigmoid  at  0x7f85478f7848>,  dropout=None,  use_bias=True,  w_init=<function  _initializer  at  0x7f8534cafed8>,  forget_bias=1.0,  return_seq=False,  return_state=False,  initial_state=None,  dynamic=True,  trainable=True,  scope='gru')</span>
Gated Recurrent Layer.</p>
<h3>Args</h3>

<ul>
<li><strong>inputs</strong>: <code>Tensor</code>. Inputs 3-D Tensor [samples, timesteps, input_dim].</li>
<li><strong>n_units</strong>: <code>int</code>, number of units for this layer.</li>
<li><strong>reuse</strong>: <code>bool</code>. If True and 'scope' is provided, this layer variables
will be reused (shared).</li>
<li><strong>is_training</strong>: <code>bool</code>, training if True</li>
<li><strong>activation</strong>: <code>function</code> (returning a <code>Tensor</code>).</li>
<li><strong>inner_activation</strong>: <code>function</code> (returning a <code>Tensor</code>).</li>
<li><strong>dropout</strong>: <code>tuple</code> of <code>float</code>: (input_keep_prob, output_keep_prob). The
input and output keep probability.</li>
<li><strong>use_bias</strong>: <code>bool</code>. If True, a bias is used.</li>
<li><strong>w_init</strong>: <code>function</code> (returning a <code>Tensor</code>). Weights initialization.</li>
<li><strong>forget_bias</strong>: <code>float</code>. Bias of the forget gate. Default: 1.0.</li>
<li><strong>return_seq</strong>: <code>bool</code>. If True, returns the full sequence instead of
last sequence output only.</li>
<li><strong>return_state</strong>: <code>bool</code>. If True, returns a tuple with output and
states: (output, states).</li>
<li><strong>initial_state</strong>: <code>Tensor</code>. An initial state for the RNN.  This must be
a tensor of appropriate type and shape [batch_size x cell.state_size].</li>
<li><strong>dynamic</strong>: <code>bool</code>. If True, dynamic computation is performed. It will not
compute RNN steps above the sequence length. Note that because TF
requires to feed sequences of same length, 0 is used as a mask.
So a sequence padded with 0 at the end must be provided. When
computation is performed, it will stop when it meets a step with
a value of 0.</li>
<li><strong>trainable</strong>: <code>bool</code>. If True, weights will be trainable.</li>
<li><strong>scope</strong>: <code>str</code>. Define this layer scope (optional). A scope can be
used to share variables between layers. Note that scope will
override name.</li>
</ul>
<h3>Returns</h3>

<p>if <code>return_seq</code>: 3-D Tensor [samples, timesteps, output dim].
else: 2-D Tensor [samples, output dim].</p>
<hr />
<h1 id="bidirectional-rnn">Bidirectional RNN</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/openagi/tefla/blob/master/tefla/core/rnn_cell.py#L1428 target="_blank"><b>tefla.core.rnn_cell.bidirectional_rnn</b></a></span>  (inputs,  rnncell_fw,  rnncell_bw,  reuse,  is_training,  dropout_fw=None,  dropout_bw=None,  return_seq=False,  return_states=False,  initial_state_fw=None,  initial_state_bw=None,  dynamic=False,  scope='BiRNN',  outputs_collections=None)</span>
Build a bidirectional recurrent neural network, it requires 2 RNN Cells
to process sequence in forward and backward order. Any RNN Cell can be
used i.e. SimpleRNN, LSTM, GRU... with its own parameters. But the two
cells number of units must match.</p>
<h3>Args</h3>

<ul>
<li><strong>inputs</strong>: <code>Tensor</code>. The 3D inputs Tensor [samples, timesteps, input_dim].</li>
<li><strong>rnncell_fw</strong>: <code>RNNCell</code>. The RNN Cell to use for foward computation.</li>
<li><strong>rnncell_bw</strong>: <code>RNNCell</code>. The RNN Cell to use for backward computation.</li>
<li><strong>reuse</strong>: <code>bool</code>. If True and 'scope' is provided, this layer variables
will be reused (shared).</li>
<li><strong>is_training</strong>: <code>bool</code>, training if True</li>
<li><strong>dropout_fw</strong>: <code>tuple</code> of <code>float</code>: (input_keep_prob, output_keep_prob). the
input and output keep probability.</li>
<li><strong>dropout_bw</strong>: <code>tuple</code> of <code>float</code>: (input_keep_prob, output_keep_prob). the
input and output keep probability.</li>
<li><strong>return_seq</strong>: <code>bool</code>. If True, returns the full sequence instead of
last sequence output only.</li>
<li><strong>return_states</strong>: <code>bool</code>. If True, returns a tuple with output and
states: (output, states).</li>
<li><strong>initial_state_fw</strong>: <code>Tensor</code>. An initial state for the forward RNN.
This must be a tensor of appropriate type and shape [batch_size
x cell.state_size].</li>
<li><strong>initial_state_bw</strong>: <code>Tensor</code>. An initial state for the backward RNN.
This must be a tensor of appropriate type and shape [batch_size
x cell.state_size].</li>
<li><strong>dynamic</strong>: <code>bool</code>. If True, dynamic computation is performed. It will not
compute RNN steps above the sequence length. Note that because TF
requires to feed sequences of same length, 0 is used as a mask.
So a sequence padded with 0 at the end must be provided. When
computation is performed, it will stop when it meets a step with
a value of 0.</li>
<li><strong>scope</strong>: <code>str</code>. Define this layer scope (optional). A scope can be
used to share variables between layers. Note that scope will
override name.</li>
</ul>
<h3>Returns</h3>

<p>if <code>return_seq</code>: 3-D Tensor [samples, timesteps, output dim].
else: 2-D Tensor Layer [samples, output dim].</p>
<hr />
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../special_layers/" class="btn btn-neutral float-right" title="Special Layers">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../layers/" class="btn btn-neutral" title="Layers"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/n3011/tefla/" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../layers/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../special_layers/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme.js" defer></script>
      <script src="../../search/main.js" defer></script>

</body>
</html>
