<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="Mrinal(Ishant) Haloi">
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>Losses - Tefla</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Losses";
    var mkdocs_page_input_path = "core/losses.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js" defer></script>
  <script src="../../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> Tefla</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../..">Index</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../installation/">Installation</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Layers</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../layers/">Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../rnn_cell/">RNN</a>
                </li>
                <li class="">
                    
    <a class="" href="../special_layers/">Special Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../layer_arg_ops/">Layer Args</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Learner</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../training/">Learner</a>
                </li>
                <li class="">
                    
    <a class="" href="../learning/">Learner Multi GPU</a>
                </li>
                <li class="">
                    
    <a class="" href="../learning_v2/">Learner Multi GPU V2</a>
                </li>
                <li class="">
                    
    <a class="" href="../learning_ss/">Learner Semi Supervised</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Predictor</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../prediction/">Predictor</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Built-in Ops</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../lr_policy/">Lr_Policy</a>
                </li>
                <li class="">
                    
    <a class="" href="../metrics/">Metrics</a>
                </li>
                <li class="">
                    
    <a class="" href="../initializers/">Initializations</a>
                </li>
                <li class=" current">
                    
    <a class="current" href="./">Losses</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#define-a-log-loss">Define a log loss</a></li>
    

    <li class="toctree-l3"><a href="#define-a-log-loss_1">Define a log loss</a></li>
    

    <li class="toctree-l3"><a href="#define-a-kappa-loss-its-a-continuous-differentiable-approximation-of-discrete-kappa-loss">Define a kappa loss, Its a continuous differentiable approximation of discrete kappa loss</a></li>
    

    <li class="toctree-l3"><a href="#define-a-joint-kappa-and-log-loss-kappa-is-a-continuous-differentiable-approximation-of-discrete-kappa-loss">Define a joint kappa and log loss, Kappa is a continuous differentiable approximation of discrete kappa loss</a></li>
    

    <li class="toctree-l3"><a href="#define-a-joint-kappa-and-log-loss-log-loss-is-clipped-by-a-defined-min-value-kappa-is-a-continuous-differentiable-approximation-of-discrete-kappa-loss">Define a joint kappa and log loss; log loss is clipped by a defined min value; Kappa is a continuous differentiable approximation of discrete kappa loss</a></li>
    

    <li class="toctree-l3"><a href="#define-a-cross-entropy-loss-with-label-smoothing">Define a cross entropy loss with label smoothing</a></li>
    

    <li class="toctree-l3"><a href="#define-a-l2loss-useful-for-regularize-ie-weight-decay">Define a L2Loss, useful for regularize, i.e. weight decay</a></li>
    

    <li class="toctree-l3"><a href="#returns-a-function-that-can-be-used-to-apply-l1-regularization-to-weights">Returns a function that can be used to apply L1 regularization to weights</a></li>
    

    <li class="toctree-l3"><a href="#returns-a-function-that-can-be-used-to-apply-l2-regularization-to-weights">Returns a function that can be used to apply L2 regularization to weights</a></li>
    

    <li class="toctree-l3"><a href="#log-likelihood-for-mixture-of-discretized-logistics-assumes-the-data-has-been-rescaled-to-11-interval">log-likelihood for mixture of discretized logistics, assumes the data has been rescaled to [-1,1] interval</a></li>
    

    <li class="toctree-l3"><a href="#pull-away-loss-calculation">Pull Away loss calculation</a></li>
    

    <li class="toctree-l3"><a href="#calculate-the-loss-from-the-logits-and-the-labels">Calculate the loss from the logits and the labels</a></li>
    

    <li class="toctree-l3"><a href="#calculate-the-triplet-loss-according-to-the-facenet-paper">Calculate the triplet loss according to the FaceNet paper</a></li>
    

    <li class="toctree-l3"><a href="#decov-loss-as-described-in-httpsarxivorgpdf151106068pdf">Decov loss as described in https://arxiv.org/pdf/1511.06068.pdf</a></li>
    

    <li class="toctree-l3"><a href="#center-loss-based-on-the-paper-a-discriminative-feature-learning-approach-for-deep-face-recognition">Center loss based on the paper "A Discriminative Feature Learning Approach for Deep Face Recognition"</a></li>
    

    <li class="toctree-l3"><a href="#adds-a-similarity-loss-term-the-correlation-between-two-representations">Adds a similarity loss term, the correlation between two representations</a></li>
    

    <li class="toctree-l3"><a href="#computes-the-maximum-mean-discrepancy-mmd-of-two-samples-x-and-y">Computes the Maximum Mean Discrepancy (MMD) of two samples: x and y</a></li>
    

    <li class="toctree-l3"><a href="#adds-a-similarity-loss-term-the-mmd-between-two-representations">Adds a similarity loss term, the MMD between two representations</a></li>
    

    <li class="toctree-l3"><a href="#adds-the-domain-adversarial-dann-loss">Adds the domain adversarial (DANN) loss</a></li>
    

    <li class="toctree-l3"><a href="#adds-the-difference-loss-between-the-private-and-shared-representations">Adds the difference loss between the private and shared representations</a></li>
    

    <li class="toctree-l3"><a href="#a-helper-function-to-compute-the-error-between-quaternions">A helper function to compute the error between quaternions</a></li>
    

    <li class="toctree-l3"><a href="#a-helper-function-to-compute-the-mean-error-between-batches-of-quaternions">A helper function to compute the mean error between batches of quaternions</a></li>
    

    <li class="toctree-l3"><a href="#adds-noise-to-embeddings-and-recomputes-classification-loss">Adds noise to embeddings and recomputes classification loss</a></li>
    

    <li class="toctree-l3"><a href="#adds-gradient-to-embedding-and-recomputes-classification-loss">Adds gradient to embedding and recomputes classification loss</a></li>
    

    <li class="toctree-l3"><a href="#virtual-adversarial-loss">Virtual adversarial loss</a></li>
    

    <li class="toctree-l3"><a href="#adds-noise-to-embeddings-and-recomputes-classification-loss-fir-bidirectional-rnn-models">Adds noise to embeddings and recomputes classification loss fir bidirectional rnn models</a></li>
    

    <li class="toctree-l3"><a href="#adds-gradient-to-embeddings-and-recomputes-classification-loss-for-bidirectional-rnn-models">Adds gradient to embeddings and recomputes classification loss for bidirectional rnn models</a></li>
    

    <li class="toctree-l3"><a href="#virtual-adversarial-loss-for-bidirectional-models">Virtual adversarial loss for bidirectional models</a></li>
    

    <li class="toctree-l3"><a href="#generate-a-mask-for-the-eos-token-10-on-eos-00-otherwise">Generate a mask for the EOS token (1.0 on EOS, 0.0 otherwise)</a></li>
    

    <li class="toctree-l3"><a href="#returns-weighted-kl-divergence-between-distributions-q-and-p">Returns weighted KL divergence between distributions q and p</a></li>
    

    <li class="toctree-l3"><a href="#calculates-the-per-example-cross-entropy-loss-for-a-sequence-of-logits-and">Calculates the per-example cross-entropy loss for a sequence of logits and</a></li>
    

    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../summary/">Summaries</a>
                </li>
                <li class="">
                    
    <a class="" href="../logger/">Logger</a>
                </li>
                <li class="">
                    
    <a class="" href="../iter_ops/">Iter Ops</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Data Management</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../da/data/">Data Augmentation</a>
                </li>
                <li class="">
                    
    <a class="" href="../../da/standardizer/">Standardizer</a>
                </li>
                <li class="">
                    
    <a class="" href="../../dataset/image_to_tfrecords/">TfRecords</a>
                </li>
                <li class="">
                    
    <a class="" href="../../dataset/base/">Dataset</a>
                </li>
                <li class="">
                    
    <a class="" href="../../dataset/dataflow/">Dataflow</a>
                </li>
                <li class="">
                    
    <a class="" href="../../dataset/decoder/">Decoder</a>
                </li>
                <li class="">
                    
    <a class="" href="../../dataset/reader/">Reader</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Utils</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../utils/util/">Utils</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../license/">License</a>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">Tefla</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>Built-in Ops &raquo;</li>
        
      
    
    <li>Losses</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/n3011/tefla/edit/master/docs/core/losses.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="define-a-log-loss">Define a log loss</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/losses.py#L16 target="_blank"><b>tefla.core.losses.log_loss_custom</b></a></span>  (predictions,  labels,  eps=1e-07,  name='log')</span></p>
<h3>Args</h3>

<ul>
<li><strong>predictions</strong>: 2D tensor or array, [batch_size, num_classes] predictions of the network .</li>
<li><strong>labels</strong>: 2D or array tensor, [batch_size, num_classes]  ground truth labels or target labels.</li>
<li><strong>eps</strong>: a constant to set upper or lower limit for labels, smoothening factor</li>
<li><strong>name</strong>: Optional scope/name for op_scope.</li>
</ul>
<h3>Returns</h3>

<p>A tensor with the log loss.</p>
<hr />
<h1 id="define-a-log-loss_1">Define a log loss</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/losses.py#L37 target="_blank"><b>tefla.core.losses.log_loss_tf</b></a></span>  (predictions,  labels,  eps=1e-07,  weights=1.0,  name='log_loss')</span></p>
<h3>Args</h3>

<ul>
<li><strong>predictions</strong>: 2D tensor or array, [batch_size, num_classes] predictions of the network .</li>
<li><strong>labels</strong>: 2D or array tensor, [batch_size, num_classes]  ground truth labels or target labels.</li>
<li><strong>eps</strong>: a constant to set upper or lower limit for labels, smoothening factor</li>
<li><strong>name</strong>: Optional scope/name for op_scope.</li>
</ul>
<h3>Returns</h3>

<p>A tensor with the log loss.</p>
<hr />
<h1 id="define-a-kappa-loss-its-a-continuous-differentiable-approximation-of-discrete-kappa-loss">Define a kappa loss, Its a continuous differentiable approximation of discrete kappa loss</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/losses.py#L58 target="_blank"><b>tefla.core.losses.kappa_loss</b></a></span>  (predictions,  labels,  y_pow=1,  eps=1e-15,  num_ratings=5,  batch_size=32,  name='kappa')</span></p>
<h3>Args</h3>

<ul>
<li><strong>predictions</strong>: 2D tensor or array, [batch_size, num_classes] predictions of the network .</li>
<li><strong>labels</strong>: 2D tensor or array,[batch_size, num_classes]  ground truth labels or target labels.</li>
<li><strong>y_pow</strong>: int, to whcih the labels should be raised; useful if model diverge. e.g. y_pow=2</li>
<li><strong>num_ratings</strong>: numbers of rater to used, typically num_classes of the model</li>
<li><strong>batch_size</strong>: batch_size of the training or validation ops</li>
<li><strong>eps</strong>: a float, prevents divide by zero</li>
<li><strong>name</strong>: Optional scope/name for op_scope.</li>
</ul>
<h3>Returns</h3>

<p>A tensor with the kappa loss.</p>
<hr />
<h1 id="define-a-joint-kappa-and-log-loss-kappa-is-a-continuous-differentiable-approximation-of-discrete-kappa-loss">Define a joint kappa and log loss, Kappa is a continuous differentiable approximation of discrete kappa loss</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/losses.py#L103 target="_blank"><b>tefla.core.losses.kappa_log_loss</b></a></span>  (predictions,  labels,  label_smoothing=0.0,  y_pow=1,  batch_size=32,  log_scale=0.5,  num_classes=5,  log_offset=0.5,  name='kappa_log')</span></p>
<h3>Args</h3>

<ul>
<li><strong>predictions</strong>: 2D tensor or array, [batch_size, num_classes] predictions of the network .</li>
<li><strong>labels</strong>: 2D tensor or array,[batch_size, num_classes]  ground truth labels or target labels.</li>
<li><strong>label_smoothing</strong>: a float, used to smooth the labels for better generalization if greater than 0 then smooth the labels.</li>
<li><strong>y_pow</strong>: int, to whcih the labels should be raised; useful if model diverge. e.g. y_pow=2</li>
<li><strong>num_ratings</strong>: numbers of rater to used, typically num_classes of the model</li>
<li><strong>batch_size</strong>: batch_size of the training or validation ops</li>
<li><strong>log_scale</strong>: a float, used to multiply the clipped log loss, e.g: 0.5</li>
<li><strong>log_offset</strong>:a float minimum log loss offset to substract from original log loss; e.g. 0.50</li>
<li><strong>name</strong>: Optional scope/name for op_scope.</li>
</ul>
<h3>Returns</h3>

<p>A tensor with the kappa log loss.</p>
<hr />
<h1 id="define-a-joint-kappa-and-log-loss-log-loss-is-clipped-by-a-defined-min-value-kappa-is-a-continuous-differentiable-approximation-of-discrete-kappa-loss">Define a joint kappa and log loss; log loss is clipped by a defined min value; Kappa is a continuous differentiable approximation of discrete kappa loss</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/losses.py#L134 target="_blank"><b>tefla.core.losses.kappa_log_loss_clipped</b></a></span>  (predictions,  labels,  label_smoothing=0.0,  y_pow=1,  batch_size=32,  log_scale=0.5,  log_cutoff=0.8,  num_classes=5,  name='kappa_log_clipped')</span></p>
<h3>Args</h3>

<ul>
<li><strong>predictions</strong>: 2D tensor or array, [batch_size, num_classes] predictions of the network .</li>
<li><strong>labels</strong>: 2D tensor or array,[batch_size, num_classes]  ground truth labels or target labels.</li>
<li><strong>label_smoothing</strong>: a float, used to smooth the labels for better generalization if greater than 0 then smooth the labels.</li>
<li><strong>y_pow</strong>: int, to whcih the labels should be raised; useful if model diverge. e.g. y_pow=2</li>
<li><strong>num_ratings</strong>: numbers of rater to used, typically num_classes of the model</li>
<li><strong>batch_size</strong>: batch_size of the training or validation ops</li>
<li><strong>log_scale</strong>: a float, used to multiply the clipped log loss, e.g: 0.5</li>
<li><strong>log_cutoff</strong>:a float, minimum log loss value; e.g. 0.50</li>
<li><strong>name</strong>: Optional scope/name for op_scope.</li>
</ul>
<h3>Returns</h3>

<p>A tensor with the clipped kappa log loss.</p>
<hr />
<h1 id="define-a-cross-entropy-loss-with-label-smoothing">Define a cross entropy loss with label smoothing</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/losses.py#L165 target="_blank"><b>tefla.core.losses.cross_entropy_loss</b></a></span>  (logits,  labels,  label_smoothing=0.0,  weight=1.0,  name='cross_entropy_loss')</span></p>
<h3>Args</h3>

<ul>
<li><strong>predictions</strong>: 2D tensor or array, [batch_size, num_classes] predictions of the network .</li>
<li><strong>labels</strong>: 2D tensor or array,[batch_size, num_classes]  ground truth labels or target labels.</li>
<li><strong>label_smoothing</strong>: a float, used to smooth the labels for better generalizationif greater than 0 then smooth the labels.</li>
<li><strong>weight</strong>: scale the loss by this factor.</li>
<li><strong>name</strong>: Optional scope/name for op_scope.</li>
</ul>
<h3>Returns</h3>

<p>A tensor with the cross entropy loss.</p>
<hr />
<h1 id="define-a-l2loss-useful-for-regularize-ie-weight-decay">Define a L2Loss, useful for regularize, i.e. weight decay</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/losses.py#L195 target="_blank"><b>tefla.core.losses.l1_l2_regularizer</b></a></span>  (var,  weight_l1=1.0,  weight_l2=1.0,  name='l1_l2_regularizer')</span></p>
<h3>Args</h3>

<ul>
<li><strong>var</strong>: tensor to regularize.</li>
<li><strong>weight_l1</strong>: an optional weight to modulate the l1 loss.</li>
<li><strong>weight_l2</strong>: an optional weight to modulate the l2 loss.</li>
<li><strong>name</strong>: Optional scope/name for op_scope.</li>
</ul>
<h3>Returns</h3>

<p>the l1+L2 loss op.</p>
<hr />
<h1 id="returns-a-function-that-can-be-used-to-apply-l1-regularization-to-weights">Returns a function that can be used to apply L1 regularization to weights</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/losses.py#L218 target="_blank"><b>tefla.core.losses.l1_regularizer</b></a></span>  (scale,  name='l1_regularizer')</span>
L1 regularization encourages sparsity.</p>
<h3>Args</h3>

<p>scale: A scalar multiplier <code>Tensor</code>. 0.0 disables the regularizer.
  name: An optional name/scope name.</p>
<h3>Returns</h3>

<p>A function with signature <code>l1(weights)</code> that apply L1 regularization.</p>
<hr />
<h1 id="returns-a-function-that-can-be-used-to-apply-l2-regularization-to-weights">Returns a function that can be used to apply L2 regularization to weights</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/losses.py#L255 target="_blank"><b>tefla.core.losses.l2_regularizer</b></a></span>  (scale,  name='l2_regularizer')</span>
Small values of L2 can help prevent overfitting the training data.</p>
<h3>Args</h3>

<p>scale: A scalar multiplier <code>Tensor</code>. 0.0 disables the regularizer.
  name: An optional name/scope name.</p>
<h3>Returns</h3>

<p>A function with signature <code>l2(weights)</code> that applies L2 regularization.</p>
<hr />
<h1 id="log-likelihood-for-mixture-of-discretized-logistics-assumes-the-data-has-been-rescaled-to-11-interval">log-likelihood for mixture of discretized logistics, assumes the data has been rescaled to [-1,1] interval</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/losses.py#L289 target="_blank"><b>tefla.core.losses.discretized_mix_logistic_loss</b></a></span>  (inputs,  predictions,  sum_all=True,  name='disretized_mix_logistic_loss')</span></p>
<h3>Args</h3>

<ul>
<li><strong>predictions</strong>: 4D tensor or array, [batch_size, width, height, out_channels] predictions of the network .</li>
<li><strong>inputs</strong>: 4D tensor or array, [batch_size, width, height, num_classes] ground truth labels or target labels.</li>
<li><strong>name</strong>: Optional scope/name for op_scope.</li>
</ul>
<h3>Returns</h3>

<p>A tensor with the discretized mix logistic loss.</p>
<hr />
<h1 id="pull-away-loss-calculation">Pull Away loss calculation</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/losses.py#L351 target="_blank"><b>tefla.core.losses.pullaway_loss</b></a></span>  (embeddings,  name='pullaway_loss')</span></p>
<h3>Args</h3>

<ul>
<li><strong>embeddings</strong>: The embeddings to be orthogonalized for varied faces. Shape [batch_size, embeddings_dim]</li>
</ul>
<hr />
<h1 id="calculate-the-loss-from-the-logits-and-the-labels">Calculate the loss from the logits and the labels</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/losses.py#L385 target="_blank"><b>tefla.core.losses.segment_loss</b></a></span>  (logits,  labels,  num_classes,  head=None)</span></p>
<h3>Args</h3>

<p>logits: tensor, float - [batch_size * width * height, num_classes].
 -   Use vgg_fcn.up as logits.
  labels: Labels tensor, int32 - [batch_size * width * height, num_classes].
 -   The ground truth of your data.
  head: numpy array - [num_classes]
 -   Weighting the loss of each class
 -   Optional: Prioritize some classes</p>
<h3>Returns</h3>

<p>loss: Loss tensor of type float.</p>
<hr />
<h1 id="calculate-the-triplet-loss-according-to-the-facenet-paper">Calculate the triplet loss according to the FaceNet paper</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/losses.py#L420 target="_blank"><b>tefla.core.losses.triplet_loss</b></a></span>  (anchor,  positive,  negative,  alpha=0.2,  name='triplet_loss')</span></p>
<h3>Args</h3>

<p>anchor: 2-D <code>tensor</code> [batch_size, embedding_size], the embeddings for the anchor images.
  positive: 2-D <code>tensor</code> [batch_size, embedding_size], the embeddings for the positive images.
  negative: 2-D <code>tensor</code> [batch_size, embedding_size], the embeddings for the negative images.
  alpha: positive to negative triplet distance margin</p>
<h3>Returns</h3>

<p>the triplet loss.</p>
<hr />
<h1 id="decov-loss-as-described-in-httpsarxivorgpdf151106068pdf">Decov loss as described in https://arxiv.org/pdf/1511.06068.pdf</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/losses.py#L440 target="_blank"><b>tefla.core.losses.decov_loss</b></a></span>  (xs,  name='decov_loss')</span>
'Reducing Overfitting In Deep Networks by Decorrelating Representation'</p>
<h3>Args</h3>

<ul>
<li><strong>xs</strong>: 4-D <code>tensor</code> [batch_size, height, width, channels], input</li>
</ul>
<h3>Returns</h3>

<p>a <code>float</code> decov loss</p>
<hr />
<h1 id="center-loss-based-on-the-paper-a-discriminative-feature-learning-approach-for-deep-face-recognition">Center loss based on the paper "A Discriminative Feature Learning Approach for Deep Face Recognition"</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/losses.py#L461 target="_blank"><b>tefla.core.losses.center_loss</b></a></span>  (features,  label,  alpha,  num_classes,  name='center_loss')</span>
   (http://ydwen.github.io/papers/WenECCV16.pdf)</p>
<h3>Args</h3>

<ul>
<li><strong>features</strong>: 2-D <code>tensor</code> [batch_size, feature_length], input features</li>
<li><strong>label</strong>: 1-D <code>tensor</code> [batch_size], input label</li>
<li><strong>alpha</strong>: center loss parameter</li>
<li><strong>num_classes</strong>: a <code>int</code> numof classes for training</li>
</ul>
<h3>Returns</h3>

<p>a <code>float</code>, center loss</p>
<hr />
<h1 id="adds-a-similarity-loss-term-the-correlation-between-two-representations">Adds a similarity loss term, the correlation between two representations</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/losses.py#L486 target="_blank"><b>tefla.core.losses.correlation_loss</b></a></span>  (source_samples,  target_samples,  weight,  name='corr_loss')</span></p>
<h3>Args</h3>

<ul>
<li><strong>source_samples</strong>: a tensor of shape [num_samples, num_features]</li>
<li><strong>target_samples</strong>: a tensor of shape [num_samples, num_features]</li>
<li><strong>weight</strong>: a scalar weight for the loss.</li>
<li><strong>scope</strong>: optional name scope for summary tags.</li>
</ul>
<h3>Returns</h3>

<p>a scalar tensor representing the correlation loss value.</p>
<hr />
<h1 id="computes-the-maximum-mean-discrepancy-mmd-of-two-samples-x-and-y">Computes the Maximum Mean Discrepancy (MMD) of two samples: x and y</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/losses.py#L516 target="_blank"><b>tefla.core.losses.maximum_mean_discrepancy</b></a></span>  (x,  y,  kernel=<function  gaussian_kernel_matrix  at  0x7f853bfa0cf8>,  name='maximum_mean_discrepancy')</span></p>
<p>Maximum Mean Discrepancy (MMD) is a distance-measure between the samples of
the distributions of x and y. Here we use the kernel two sample estimate
using the empirical mean of the two distributions.</p>
<p>MMD^2(P, Q) = || \E{\phi(x)} - \E{\phi(y)} ||^2= \E{ K(x, x) } + \E{ K(y, y) } - 2 \E{ K(x, y) },</p>
<p>where K = &lt;\phi(x), \phi(y)&gt;,
  is the desired kernel function, in this case a radial basis kernel.</p>
<h3>Args</h3>

<ul>
<li><strong>x</strong>: a tensor of shape [num_samples, num_features]</li>
<li><strong>y</strong>: a tensor of shape [num_samples, num_features]</li>
<li><strong>kernel</strong>: a function which computes the kernel in MMD. Defaults to theGaussianKernelMatrix.</li>
</ul>
<h3>Returns</h3>

<p>a scalar denoting the squared maximum mean discrepancy loss.</p>
<hr />
<h1 id="adds-a-similarity-loss-term-the-mmd-between-two-representations">Adds a similarity loss term, the MMD between two representations</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/losses.py#L549 target="_blank"><b>tefla.core.losses.mmd_loss</b></a></span>  (source_samples,  target_samples,  weight,  name='mmd_loss')</span></p>
<p>This Maximum Mean Discrepancy (MMD) loss is calculated with a number of
different Gaussian kernels.</p>
<h3>Args</h3>

<p>source_samples: a tensor of shape [num_samples, num_features].
  target_samples: a tensor of shape [num_samples, num_features].
  weight: the weight of the MMD loss.
  scope: optional name scope for summary tags.</p>
<h3>Returns</h3>

<p>a scalar tensor representing the MMD loss value.</p>
<hr />
<h1 id="adds-the-domain-adversarial-dann-loss">Adds the domain adversarial (DANN) loss</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/losses.py#L582 target="_blank"><b>tefla.core.losses.dann_loss</b></a></span>  (source_samples,  target_samples,  weight,  name='dann_loss')</span></p>
<h3>Args</h3>

<p>source_samples: a tensor of shape [num_samples, num_features].
  target_samples: a tensor of shape [num_samples, num_features].
  weight: the weight of the loss.
  scope: optional name scope for summary tags.</p>
<h3>Returns</h3>

<p>a scalar tensor representing the correlation loss value.</p>
<hr />
<h1 id="adds-the-difference-loss-between-the-private-and-shared-representations">Adds the difference loss between the private and shared representations</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/losses.py#L624 target="_blank"><b>tefla.core.losses.difference_loss</b></a></span>  (private_samples,  shared_samples,  weight=1.0,  name='difference_loss')</span></p>
<h3>Args</h3>

<p>private_samples: a tensor of shape [num_samples, num_features].
  shared_samples: a tensor of shape [num_samples, num_features].
  weight: the weight of the incoherence loss.
  name: the name of the tf summary.</p>
<hr />
<h1 id="a-helper-function-to-compute-the-error-between-quaternions">A helper function to compute the error between quaternions</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/losses.py#L652 target="_blank"><b>tefla.core.losses.log_quaternion_loss_batch</b></a></span>  (predictions,  labels,  name='log_quaternion_batch_loss')</span></p>
<h3>Args</h3>

<p>predictions: A Tensor of size [batch_size, 4].
  labels: A Tensor of size [batch_size, 4].
  params: A dictionary of parameters. Expecting 'use_logging', 'batch_size'.</p>
<h3>Returns</h3>

<p>A Tensor of size [batch_size], denoting the error between the quaternions.</p>
<hr />
<h1 id="a-helper-function-to-compute-the-mean-error-between-batches-of-quaternions">A helper function to compute the mean error between batches of quaternions</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/losses.py#L678 target="_blank"><b>tefla.core.losses.log_quaternion_loss</b></a></span>  (predictions,  labels,  batch_size,  name='log_quaternion_loss')</span></p>
<p>The caller is expected to add the loss to the graph.</p>
<h3>Args</h3>

<p>predictions: A Tensor of size [batch_size, 4].
  labels: A Tensor of size [batch_size, 4].
  params: A dictionary of parameters. Expecting 'use_logging', 'batch_size'.</p>
<h3>Returns</h3>

<p>A Tensor of size 1, denoting the mean error between batches of quaternions.</p>
<hr />
<h1 id="adds-noise-to-embeddings-and-recomputes-classification-loss">Adds noise to embeddings and recomputes classification loss</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/losses.py#L699 target="_blank"><b>tefla.core.losses.random_perturbation_loss</b></a></span>  (embedded,  length,  loss_fn,  perturb_norm_length=0.1)</span></p>
<h3>Args</h3>

<ul>
<li><strong>embedded</strong>: 3-D float <code>Tensor</code>, [batch_size, num_timesteps, embedding_dim]</li>
<li><strong>length</strong>: a <code>int</code>, length of the mask</li>
<li><strong>loss_fn</strong>: a callable, that returns loss</li>
<li><strong>perturb_norm_length</strong>: a <code>float</code>, Norm length of adversarial perturbation to be optimized with validatio</li>
</ul>
<h3>Returns</h3>

<p>perturbation loss</p>
<hr />
<h1 id="adds-gradient-to-embedding-and-recomputes-classification-loss">Adds gradient to embedding and recomputes classification loss</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/losses.py#L716 target="_blank"><b>tefla.core.losses.adversarial_loss</b></a></span>  (embedded,  loss,  loss_fn,  perturb_norm_length=0.1)</span></p>
<h3>Args</h3>

<ul>
<li><strong>embedded</strong>: 3-D float <code>Tensor</code>, [batch_size, num_timesteps, embedding_dim]</li>
<li><strong>loss</strong>: <code>float</code>, loss</li>
<li><strong>loss_fn</strong>: a callable, that returns loss</li>
<li><strong>perturb_norm_length</strong>: a <code>float</code>, Norm length of adversarial perturbation to be optimized with validatio</li>
</ul>
<h3>Returns</h3>

<p>adversial loss</p>
<hr />
<h1 id="virtual-adversarial-loss">Virtual adversarial loss</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/losses.py#L737 target="_blank"><b>tefla.core.losses.virtual_adversarial_loss</b></a></span>  (logits,  embedded,  labels,  length,  logits_from_embedding_fn,  num_classes,  num_power_iteration=1,  small_constant_for_finite_diff=0.001,  perturb_norm_length=0.1)</span>
Computes virtual adversarial perturbation by finite difference method and
power iteration, adds it to the embedding, and computes the KL divergence
between the new logits and the original logits.</p>
<h3>Args</h3>

<ul>
<li><strong>logits</strong>: 2-D float <code>Tensor</code>, [num_timesteps*batch_size, m], where m=1 if
num_classes=2, otherwise m=num_classes.</li>
<li><strong>embedded</strong>: 3-D float <code>Tensor</code>, [batch_size, num_timesteps, embedding_dim].</li>
<li><strong>labels</strong>: 1-D <code>Tensor</code>, input labels</li>
<li><strong>length</strong>: a <code>int</code>, input length</li>
<li><strong>logits_from_embedding_fn</strong>: callable that takes embeddings and returns
classifier logits.</li>
<li><strong>num_classes</strong>: num_classes for training</li>
<li><strong>vocab_size</strong>: a <code>int</code>, vocabular size of the problem</li>
<li><strong>num_power_iteration</strong>: a <code>int</code>, the number of power iteration</li>
<li><strong>small_constant_for_finite_diff</strong>: a <code>float</code>, Small constant for finite difference method</li>
<li><strong>perturb_norm_length</strong>: a <code>float</code>, Norm length of adversarial perturbation to be optimized with validatio</li>
</ul>
<h3>Returns</h3>

<p>a <code>float</code> <code>scalar</code>, KL divergence.</p>
<hr />
<h1 id="adds-noise-to-embeddings-and-recomputes-classification-loss-fir-bidirectional-rnn-models">Adds noise to embeddings and recomputes classification loss fir bidirectional rnn models</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/losses.py#L781 target="_blank"><b>tefla.core.losses.random_perturbation_loss_brnn</b></a></span>  (embedded,  length,  loss_fn,  perturb_norm_length=0.1)</span></p>
<h3>Args</h3>

<ul>
<li><strong>embedded</strong>: 3-D float <code>Tensor</code>, [batch_size, num_timesteps, embedding_dim]</li>
<li><strong>length</strong>: a <code>int</code>, length of the mask</li>
<li><strong>loss_fn</strong>: a callable, that returns loss</li>
<li><strong>perturb_norm_length</strong>: a <code>float</code>, Norm length of adversarial perturbation to be optimized with validatio</li>
</ul>
<h3>Returns</h3>

<p>perturbation loss</p>
<hr />
<h1 id="adds-gradient-to-embeddings-and-recomputes-classification-loss-for-bidirectional-rnn-models">Adds gradient to embeddings and recomputes classification loss for bidirectional rnn models</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/losses.py#L799 target="_blank"><b>tefla.core.losses.adversarial_loss_brnn</b></a></span>  (embedded,  loss,  loss_fn,  perurb_norm_length=0.1)</span></p>
<h3>Args</h3>

<ul>
<li><strong>embedded</strong>: 3-D float <code>Tensor</code>, [batch_size, num_timesteps, embedding_dim]</li>
<li><strong>loss</strong>: <code>float</code>, loss</li>
<li><strong>loss_fn</strong>: a callable, that returns loss</li>
<li><strong>perturb_norm_length</strong>: a <code>float</code>, Norm length of adversarial perturbation to be optimized with validatio</li>
</ul>
<h3>Returns</h3>

<p>adversial loss</p>
<hr />
<h1 id="virtual-adversarial-loss-for-bidirectional-models">Virtual adversarial loss for bidirectional models</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/losses.py#L818 target="_blank"><b>tefla.core.losses.virtual_adversarial_loss_brnn</b></a></span>  (logits,  embedded,  labels,  length,  logits_from_embedding_fn,  vocab_size,  num_classes,  num_power_iteration=1,  small_constant_for_finite_diff=0.001,  perturb_norm_length=0.1)</span>
Computes virtual adversarial perturbation by finite difference method and
power iteration, adds it to the embedding, and computes the KL divergence
between the new logits and the original logits.</p>
<h3>Args</h3>

<ul>
<li><strong>logits</strong>: 2-D float <code>Tensor</code>, [num_timesteps*batch_size, m], where m=1 if
num_classes=2, otherwise m=num_classes.</li>
<li><strong>embedded</strong>: 3-D float <code>Tensor</code>, [batch_size, num_timesteps, embedding_dim].</li>
<li><strong>labels</strong>: 1-D <code>Tensor</code>, input labels</li>
<li><strong>length</strong>: a <code>int</code>, input length</li>
<li><strong>logits_from_embedding_fn</strong>: callable that takes embeddings and returns
classifier logits.</li>
<li><strong>num_classes</strong>: num_classes for training</li>
<li><strong>vocab_size</strong>: a <code>int</code>, vocabular size of the problem</li>
<li><strong>num_power_iteration</strong>: a <code>int</code>, the number of power iteration</li>
<li><strong>small_constant_for_finite_diff</strong>: a <code>float</code>, Small constant for finite difference method</li>
<li><strong>perturb_norm_length</strong>: a <code>float</code>, Norm length of adversarial perturbation to be optimized with validatio</li>
</ul>
<h3>Returns</h3>

<p>a <code>float</code> <code>scalar</code>, KL divergence.</p>
<hr />
<h1 id="generate-a-mask-for-the-eos-token-10-on-eos-00-otherwise">Generate a mask for the EOS token (1.0 on EOS, 0.0 otherwise)</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/losses.py#L879 target="_blank"><b>tefla.core.losses._end_of_seq_mask</b></a></span>  (tokens,  vocab_size)</span></p>
<h3>Args</h3>

<ul>
<li><strong>tokens</strong>: 1-D integer <code>Tensor</code> [num_timesteps*batch_size]. Each element is an
id from the vocab.</li>
<li><strong>vocab_size</strong>: a <code>int</code>, vocabular size of the problem</li>
</ul>
<h3>Returns</h3>

<p>Float 1-D <code>Tensor</code> same shape as tokens, whose values are 1.0 on the end of
sequence and 0.0 on the others.</p>
<hr />
<h1 id="returns-weighted-kl-divergence-between-distributions-q-and-p">Returns weighted KL divergence between distributions q and p</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/losses.py#L895 target="_blank"><b>tefla.core.losses._kl_divergence_with_logits</b></a></span>  (q_logits,  p_logits,  weights,  num_classes)</span></p>
<h3>Args</h3>

<ul>
<li><strong>q_logits</strong>: logits for 1st argument of KL divergence shape
  [num_timesteps * batch_size, num_classes] if num_classes &gt; 2, and
  [num_timesteps * batch_size] if num_classes == 2.</li>
<li><strong>p_logits</strong>: logits for 2nd argument of KL divergence with same shape q_logits.</li>
<li><strong>weights</strong>: 1-D <code>float</code> tensor with shape [num_timesteps * batch_size].
 Elements should be 1.0 only on end of sequences</li>
<li><strong>num_classes</strong>: a <code>int</code>, number of training classes</li>
</ul>
<h3>Returns</h3>

<p>a <code>float</code> <code>scalar</code>, KL divergence.</p>
<hr />
<h1 id="calculates-the-per-example-cross-entropy-loss-for-a-sequence-of-logits-and">Calculates the per-example cross-entropy loss for a sequence of logits and</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/losses.py#L931 target="_blank"><b>tefla.core.losses.cross_entropy_sequence_loss</b></a></span>  (logits,  targets,  sequence_length)</span>
masks out all losses passed the sequence length.</p>
<h3>Args</h3>

<ul>
<li><strong>logits</strong>: Logits of shape <code>[T, B, vocab_size]</code></li>
<li><strong>targets</strong>: Target classes of shape <code>[T, B]</code></li>
<li><strong>sequence_length</strong>: An int32 tensor of shape <code>[B]</code> corresponding
 -to the length of each input</li>
</ul>
<h3>Returns</h3>

<p>A tensor of shape [T, B] that contains the loss per example, per time step.</p>
<hr />
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../summary/" class="btn btn-neutral float-right" title="Summaries">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../initializers/" class="btn btn-neutral" title="Initializations"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/n3011/tefla/" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../initializers/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../summary/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme.js" defer></script>
      <script src="../../search/main.js" defer></script>

</body>
</html>
